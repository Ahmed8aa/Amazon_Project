{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43227,"status":"ok","timestamp":1728498380613,"user":{"displayName":"Ahmed Abdelrhman","userId":"02369919969702643030"},"user_tz":-180},"id":"eNjF_3I6HBnI","outputId":"a4a547e5-5f61-458f-c2a7-a1eaf0af4d8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting selenium\n","  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n","Collecting trio~=0.17 (from selenium)\n","  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n","Collecting trio-websocket~=0.9 (from selenium)\n","  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n","Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n","Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n","Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n","Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n","Collecting outcome (from trio~=0.17->selenium)\n","  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n","Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n","  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n","Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio-0.26.2-py3-none-any.whl (475 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n","Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n","Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n","Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.25.0 trio-0.26.2 trio-websocket-0.11.1 wsproto-1.2.0\n","Collecting webdriver_manager\n","  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (2.32.3)\n","Collecting python-dotenv (from webdriver_manager)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (24.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2024.8.30)\n","Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n","Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Installing collected packages: python-dotenv, webdriver_manager\n","Successfully installed python-dotenv-1.0.1 webdriver_manager-4.0.2\n","Collecting easyocr\n","  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.4.1+cu121)\n","Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.1+cu121)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.10.0.84)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.13.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.26.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from easyocr) (10.4.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.24.0)\n","Collecting python-bidi (from easyocr)\n","  Downloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0.2)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.6)\n","Collecting pyclipper (from easyocr)\n","  Downloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n","Collecting ninja (from easyocr)\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2024.6.1)\n","Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.35.1)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.9.20)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (24.1)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->easyocr) (1.3.0)\n","Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (908 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-bidi, pyclipper, ninja, easyocr\n","Successfully installed easyocr-1.7.2 ninja-1.11.1.1 pyclipper-1.3.0.post5 python-bidi-0.6.0\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.10.8)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.13.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp) (4.12.2)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp) (3.10)\n","Collecting azure-storage-blob\n","  Downloading azure_storage_blob-12.23.1-py3-none-any.whl.metadata (26 kB)\n","Collecting azure-core>=1.30.0 (from azure-storage-blob)\n","  Downloading azure_core-1.31.0-py3-none-any.whl.metadata (39 kB)\n","Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (43.0.1)\n","Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (4.12.2)\n","Collecting isodate>=0.6.1 (from azure-storage-blob)\n","  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.30.0->azure-storage-blob) (2.32.3)\n","Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.30.0->azure-storage-blob) (1.16.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.22)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2024.8.30)\n","Downloading azure_storage_blob-12.23.1-py3-none-any.whl (405 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.6/405.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading azure_core-1.31.0-py3-none-any.whl (197 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.4/197.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n","Installing collected packages: isodate, azure-core, azure-storage-blob\n","Successfully installed azure-core-1.31.0 azure-storage-blob-12.23.1 isodate-0.7.2\n"]}],"source":["!pip install selenium\n","!pip install webdriver_manager\n","!pip install easyocr\n","!pip install aiohttp #asyncio\n","# !pip install nest_asyncio\n","!pip install azure-storage-blob\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xt3HyH6GHMEq"},"outputs":[],"source":["import aiohttp\n","import asyncio\n","from urllib.parse import urljoin\n","import time\n","import random\n","import io\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from selenium import webdriver\n","from selenium.webdriver.chrome.service import Service\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","from bs4 import BeautifulSoup\n","from tqdm import tqdm\n","from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementNotInteractableException, ElementClickInterceptedException\n","# from google.colab import files\n","from datetime import datetime\n","import nest_asyncio\n","import easyocr\n","import re\n","\n","class AmazonScraper:\n","    def __init__(self, search_term, num_pages, country=\"Egypt\"):\n","        self.search_term = search_term\n","        self.num_pages = num_pages\n","        self.country = country\n","        self.driver = self.web_driver()\n","        self.product_data = []\n","        self.user_agents = [\n","            'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0',\n","          'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36',\n","          'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36 Edg/93.0.961.47',\n","\n","          # Less common browsers\n","          'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36 OPR/79.0.4143.50',\n","          'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36 Vivaldi/4.1',\n","\n","          # Older browser versions\n","          'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0',\n","          'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/601.7.7 (KHTML, like Gecko) Version/9.1.2 Safari/601.7.7',\n","        ]\n","        self.current_user_agent_index = 0\n","        self.base_url = 'https://www.amazon.com'\n","\n","    def get_next_user_agent(self):\n","        user_agent = self.user_agents[self.current_user_agent_index]\n","        self.current_user_agent_index = (self.current_user_agent_index + 1) % len(self.user_agents)\n","        return user_agent\n","\n","    def update_user_agent(self):\n","        new_user_agent = self.get_next_user_agent()\n","        self.driver.execute_cdp_cmd('Network.setUserAgentOverride', {\"userAgent\": new_user_agent})\n","        print(f\"Updated user agent to: {new_user_agent}\")\n","\n","    def web_driver(self):\n","        options = webdriver.ChromeOptions()\n","        options.add_argument(\"--verbose\")\n","        options.add_argument('--no-sandbox')\n","        options.add_argument('--headless')\n","        options.add_argument('--disable-gpu')\n","        options.add_argument(\"--window-size=1920,1200\")\n","        options.add_argument('--disable-dev-shm-usage')\n","        driver = webdriver.Chrome(options=options)\n","        driver.set_window_size(1920, 1080)\n","        return driver\n","\n","    def access_amazon(self):\n","        print(\"Accessing Amazon...\")\n","        max_attempts = 3\n","        for attempt in range(max_attempts):\n","            try:\n","                self.driver.get('https://www.amazon.com')\n","                self.handle_captcha()\n","                return\n","            except Exception as e:\n","                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n","                if attempt < max_attempts - 1:\n","                    print(\"Retrying...\")\n","                    time.sleep(random.uniform(2, 5))\n","                else:\n","                    print(\"Max attempts reached. Unable to access Amazon.\")\n","                    raise\n","\n","    def handle_captcha(self):\n","        try:\n","            # Check for known keywords that indicate a CAPTCHA is present\n","            page_source = self.driver.page_source\n","            captcha_keywords = [\"Enter the characters\", \"اكتب الأحرف\", \"请键入字符\"]  # Add more as needed\n","\n","            # Check if any of the keywords appear in the page source\n","            if any(keyword in page_source for keyword in captcha_keywords):\n","                print(\"CAPTCHA detected. Taking screenshot...\")\n","                self.take_captcha_screenshot()\n","\n","                # Wait for the CAPTCHA input field to appear\n","                captcha_input = WebDriverWait(self.driver, 5).until(\n","                    EC.presence_of_element_located((By.ID, 'captchacharacters'))\n","                )\n","\n","                time.sleep(10)  # Allow time for user to interact or for CAPTCHA to load\n","                captcha_code = self.extract_captcha_code('/content/after_login.png')\n","\n","                if captcha_code:\n","                    print(f\"Extracted CAPTCHA code: {captcha_code}\")\n","                    captcha_input.send_keys(captcha_code)\n","                    continue_button = self.driver.find_element(By.CSS_SELECTOR, 'button.a-button-text')\n","                    continue_button.click()\n","                else:\n","                    print(\"No CAPTCHA code detected.\")\n","                time.sleep(3)\n","            else:\n","                print(\"No CAPTCHA detected.\")\n","        except TimeoutException:\n","            print(\"No CAPTCHA detected.\")\n","        except Exception as e:\n","            print(f\"An error occurred while handling CAPTCHA: {e}\")\n","\n","    def take_captcha_screenshot(self):\n","        screenshot = self.driver.get_screenshot_as_png()\n","        image = Image.open(io.BytesIO(screenshot))\n","        image.save('/content/after_login.png')\n","        plt.figure(figsize=(12, 8))\n","        plt.imshow(image)\n","        plt.axis('off')\n","        plt.show()\n","\n","    def extract_captcha_code(self, image_path):\n","        # Initialize EasyOCR reader\n","        reader = easyocr.Reader(['en'])\n","        results = reader.readtext(image_path)\n","        captcha_text = ' '.join(result[1] for result in results if result[1].isalnum())\n","\n","        # Simple heuristic: return the first alphanumeric code of appropriate length\n","        captcha_code = re.findall(r'[A-Z0-9]{6}', captcha_text)\n","        return captcha_code[0] if captcha_code else None\n","\n","    def change_amazon_location(self):\n","        print(f\"Attempting to change location to {self.country}\")\n","        max_attempts = 3\n","        for attempt in range(max_attempts):\n","            try:\n","                self.driver.get('https://www.amazon.com')\n","                time.sleep(5)\n","                location_button = WebDriverWait(self.driver, 10).until(\n","                    EC.element_to_be_clickable((By.ID, \"glow-ingress-line2\"))\n","                )\n","                location_button.click()\n","                time.sleep(2)\n","                try:\n","                    deliver_to = WebDriverWait(self.driver, 5).until(\n","                        EC.element_to_be_clickable((By.ID, \"GLUXCountryListDropdown\"))\n","                    )\n","                    deliver_to.click()\n","                    time.sleep(2)\n","                except:\n","                    print(\"'Deliver to' dropdown not found, continuing...\")\n","                country_option = WebDriverWait(self.driver, 10).until(\n","                    EC.element_to_be_clickable((By.XPATH, f\"//a[contains(text(), '{self.country}')]\"))\n","                )\n","                country_option.click()\n","                time.sleep(2)\n","                done_button = WebDriverWait(self.driver, 10).until(\n","                    EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'a-button-text') and contains(text(), 'Done')]\"))\n","                )\n","                done_button.click()\n","                time.sleep(5)\n","                location_text = WebDriverWait(self.driver, 10).until(\n","                    EC.presence_of_element_located((By.ID, \"glow-ingress-line2\"))\n","                ).text\n","                if self.country.lower() in location_text.lower():\n","                    print(f\"Successfully changed location to {self.country}\")\n","                    return\n","                else:\n","                    print(f\"Location change unsuccessful. Current location: {location_text}\")\n","            except Exception as e:\n","                print(f\"Attempt {attempt + 1} to change location failed.\")\n","                if attempt < max_attempts - 1:\n","                    print(\"Retrying...\")\n","                    time.sleep(random.uniform(2, 5))\n","                else:\n","                    print(f\"Failed to change location to {self.country} after {max_attempts} attempts.\")\n","        print(\"Location change process completed, but may not have been successful.\")\n","\n","    def scroll_with_pauses(self):\n","        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n","        while True:\n","            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","            time.sleep(random.uniform(1, 2))\n","            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n","            if new_height == last_height:\n","                break\n","            last_height = new_height\n","\n","    def extract_product_links_and_prices(self):\n","        print(\"Extracting product links, prices, ratings, and image URLs...\")\n","        self.driver.get(f\"https://www.amazon.com/s?k={self.search_term.replace(' ', '+')}\")\n","\n","        while \"Something went wrong\" in self.driver.page_source:\n","            print(f\"Error page detected for URL. Navigating to Amazon homepage.\")\n","            self.driver.get('https://www.amazon.com')\n","            time.sleep(2)\n","            self.driver.get(f\"https://www.amazon.com/?k={self.search_term.replace(' ', '+')}\")\n","            time.sleep(2)\n","\n","        page_num = 1\n","        while page_num <= self.num_pages:\n","            try:\n","                WebDriverWait(self.driver, 10).until(\n","                    EC.presence_of_element_located((By.CSS_SELECTOR, \"div.s-result-item\"))\n","                )\n","                self.scroll_with_pauses()\n","                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n","                products = soup.find_all('div', {'data-component-type': 's-search-result'})\n","\n","                for product in products:\n","                    try:\n","                        link_tag = product.find('a', class_='a-link-normal')\n","                        link = urljoin(self.base_url, link_tag['href']) if link_tag else None\n","\n","                        price_span = product.select_one('span.a-price-whole')\n","                        if not price_span:\n","                            price_span = product.select_one('div[data-cy=\"secondary-offer-recipe\"] span.a-color-base')\n","                        price = price_span.get_text(strip=True) if price_span else \"No Price\"\n","\n","                        rating_span = product.select_one('span.a-icon-alt')\n","                        rating = rating_span.get_text(strip=True) if rating_span else \"No Rating\"\n","\n","                        img_tag = product.find('img', class_='s-image')\n","                        image_url = img_tag['src'] if img_tag else \"No Image\"\n","\n","                        if link:\n","                            self.product_data.append({\n","                                \"link\": link,\n","                                \"price\": price,\n","                                \"currency\":'USD',\n","                                \"rating\": rating,\n","                                \"image_url\": image_url\n","                            })\n","\n","                    except Exception as e:\n","                        print(f\"Error processing product: {str(e)}\")\n","\n","                print(f\"Extracted {len(self.product_data)} products so far...\")\n","\n","                next_button = self.driver.find_elements(By.CSS_SELECTOR, \"a.s-pagination-next\")\n","                if next_button and page_num < self.num_pages:\n","                    self.driver.execute_script(\"arguments[0].click();\", next_button[0])\n","                    time.sleep(random.uniform(3, 5))\n","                    page_num += 1\n","                else:\n","                    print(f\"Reached the last page or hit the page limit. Stopping at page {page_num}\")\n","                    break\n","\n","            except Exception as e:\n","                print(f\"Error on page {page_num}: {str(e)}\")\n","                print(\"Continuing with the data collected so far...\")\n","                break\n","\n","        print(f\"Total products extracted: {len(self.product_data)}\")\n","\n","    async def fetch_page(self, session, url):\n","        async with session.get(url, timeout=50) as response:\n","            return await response.text()\n","\n","    async def extract_product_details_async(self, session, url, extended_sleep=False):\n","        max_attempts = 3\n","        for attempt in range(max_attempts):\n","            try:\n","                await asyncio.sleep(random.uniform(2, 5) + extended_sleep)\n","                html = await self.fetch_page(session, url)\n","\n","                soup = BeautifulSoup(html, 'lxml')\n","\n","                # Extract product title\n","                product_title_element = soup.find('span', {'id': 'productTitle'})\n","                product_title = product_title_element.get_text(strip=True) if product_title_element else 'No title found'\n","\n","                # Initialize details dictionary\n","                details = {\n","                    \"Product Title\": product_title,\n","                    \"Category\": \"Mobile Phones\",\n","                    \"Site\": \"Amazon\"\n","                }\n","\n","                # Extract product details from the table\n","                table = soup.find('table', class_='a-normal a-spacing-micro')\n","                if table:\n","                    rows = table.find_all('tr')\n","                    for row in rows:\n","                        key_element = row.find('td', class_='a-span3')\n","                        value_element = row.find('td', class_='a-span9')\n","\n","                        if key_element and value_element:\n","                            key = key_element.get_text(strip=True)\n","                            value = value_element.get_text(strip=True)\n","                            details[key] = value\n","                else:\n","                    rows = soup.select('#productDetails_detailBullets_sections1 tr')\n","                    for row in rows:\n","                        key_element = row.find('th')\n","                        value_element = row.find('td')\n","\n","                        if key_element and value_element:\n","                            key = key_element.get_text(strip=True)\n","                            value = value_element.get_text(strip=True)\n","                            details[key] = value\n","\n","                # Now extract reviews\n","                all_reviews = []\n","                if \"No customer reviews\" in html:\n","                    print(f\"No customer reviews found for URL {url}\")\n","                    reviews_info = {\n","\n","                        \"All Reviews\": []\n","                    }\n","                else:\n","                    reviews = soup.find_all('div', {'data-hook': 'review'})\n","                    for review in reviews:\n","                        try:\n","                            reviewer_name = review.find('span', {'class': 'a-profile-name'})\n","                            rating = review.find('i', {'data-hook': 'review-star-rating'})\n","                            review_title = review.find('a', {'data-hook': 'review-title'})\n","                            review_date = review.find('span', {'data-hook': 'review-date'})\n","                            review_body = review.find('span', {'data-hook': 'review-body'})\n","\n","                            review_dict = {\n","                                \"Reviewer Name\": reviewer_name.text.strip() if reviewer_name else \"N/A\",\n","                                \"Rating\": rating.text.strip() if rating else \"N/A\",\n","                                \"Title\": review_title.find_all('span')[2].text.strip() if review_title and len(review_title.find_all('span')) > 2 else \"N/A\",\n","                                \"Date\": review_date.text.strip() if review_date else \"N/A\",\n","                                \"Review Body\": review_body.text.strip() if review_body else \"N/A\"\n","                            }\n","\n","                            all_reviews.append(review_dict)\n","                        except Exception as e:\n","                            print(f\"Error extracting review details: {e}\")\n","\n","\n","                    reviews_info = {\n","\n","                        \"All Reviews\": all_reviews\n","                    }\n","\n","                # Combine product details and reviews\n","                details.update(reviews_info)\n","\n","                return details\n","            except Exception as e:\n","                print(f\"Attempt {attempt + 1} failed for URL {url}.\")\n","                if attempt < max_attempts - 1:\n","                    print(\"Retrying...\")\n","                    await asyncio.sleep(random.uniform(2, 5))\n","                else:\n","                    print(f\"Failed to extract product details for {url} after {max_attempts} attempts.\")\n","                    return {\"Product Title\": \"Error\", \"Category\": \"Error\"}\n","\n","\n","    async def run_async_product_details(self):\n","        try:\n","            self.access_amazon()\n","            self.extract_product_links_and_prices()\n","\n","            async with aiohttp.ClientSession() as session:\n","                tasks = []\n","                for idx, product in enumerate(self.product_data, start=1):\n","                    if idx % 50 == 0:\n","                        self.update_user_agent()\n","                    task = asyncio.ensure_future(self.process_product_details(session, product, idx))\n","                    tasks.append(task)\n","\n","                self.product_data = await asyncio.gather(*tasks)\n","\n","            df = pd.DataFrame(self.product_data)\n","\n","            # Get today's date\n","            today_date = datetime.now().date()\n","\n","            # Add a column with today's date\n","            df['Today'] = today_date\n","\n","            # print(\"Saving product details to CSV file...\")\n","            # df.to_csv('amazon_product_details.csv', index=False)\n","            # print(\"Product details have been saved successfully.\")\n","\n","        finally:\n","            pass\n","\n","\n","\n","    def run_product_details(self):\n","        asyncio.get_event_loop().run_until_complete(self.run_async_product_details())\n","        self.driver.quit()\n","\n","    async def process_product_details(self, session, product, idx):\n","        extended_sleep = (idx // 250) * 1.5\n","\n","        details = await self.extract_product_details_async(session, product[\"link\"], extended_sleep)\n","        product.update(details)\n","\n","        return product\n","\n","\n","\n","    async def process_product_reviews(self, session, product_url, idx):\n","        extended_sleep = (idx // 250) * 1.5\n","\n","        reviews = await self.extract_reviews_async(session, product_url, extended_sleep)\n","        return reviews\n","\n","def main():\n","    search_term = 'mobile phone'\n","    num_pages =20\n","    country = \"Egypt\"\n","\n","    nest_asyncio.apply()\n","    %time\n","\n","\n","    scraper = AmazonScraper(search_term, num_pages, country)\n","    scraper.run_product_details()\n","    return scraper.product_data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"pNk9oXoWHsTB","executionInfo":{"status":"ok","timestamp":1728498734862,"user_tz":-180,"elapsed":342559,"user":{"displayName":"Ahmed Abdelrhman","userId":"02369919969702643030"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1lXScF8mtx3SzHUmep4Lz-LKIqlBQvT7Q"},"outputId":"dca99c7b-2065-48c0-c11f-73f42944cec8"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["def validate_df(df):\n","    # Check if df is a list or DataFrame and validate accordingly\n","    if isinstance(df, list) and df:  # Check if it's a non-empty list\n","        return True\n","    elif hasattr(df, 'empty'):  # Check if it's a DataFrame\n","        return not df.empty\n","    return False\n","\n","# Loop until a valid df_f is returned\n","df_f = None\n","max_attempts = 5  # Set a limit to avoid infinite loops\n","attempt = 0\n","\n","while attempt < max_attempts:\n","    df_f = main()\n","    if validate_df(df_f):\n","        print(\"DataFrame successfully processed.\")\n","        break\n","    else:\n","        print(f\"Attempt {attempt + 1}: DataFrame is not valid. Reprocessing...\")\n","        attempt += 1\n","\n","if df_f is None or not validate_df(df_f):\n","    print(\"Failed to generate a valid DataFrame after multiple attempts.\")\n","else:\n","    print(\"Final DataFrame/List:\", df_f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWxj6QkPIbJS"},"outputs":[],"source":["df=pd.DataFrame(df_f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t3BZDoG5MguB"},"outputs":[],"source":["# Get today's date\n","today_date = datetime.now().date()\n","\n","# Add a column with today's date\n","df['date'] = today_date"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-JcXnOhr0HbJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728498735251,"user_tz":-180,"elapsed":409,"user":{"displayName":"Ahmed Abdelrhman","userId":"02369919969702643030"}},"outputId":"8fa2a42b-fda2-4488-b042-d52e4f3b6132"},"outputs":[{"output_type":"stream","name":"stdout","text":["Container already exists: The specified container already exists.\n","RequestId:5f9f557e-101e-005b-5e79-1ac0a2000000\n","Time:2024-10-09T18:32:20.7438143Z\n","ErrorCode:ContainerAlreadyExists\n","Content: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>ContainerAlreadyExists</Code><Message>The specified container already exists.\n","RequestId:5f9f557e-101e-005b-5e79-1ac0a2000000\n","Time:2024-10-09T18:32:20.7438143Z</Message></Error>\n","CSV file raw/amazon_us_raw2024-10-09.csv uploaded successfully.\n"]}],"source":["from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n","import os\n","\n","# Azure Storage connection string\n","connect_str = \"DefaultEndpointsProtocol=https;AccountName=ynwa;AccountKey=aTLtGZuymmaAirlsL1/39g3dDjaQvBFu3lHzQaUrY0o6LDdGduAW8Wbk8qI7fuilCKS8chuiklq7+AStBN3UdA==;EndpointSuffix=core.windows.net\"\n","\n","# Initialize BlobServiceClient\n","blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n","\n","# Name of the container and blob\n","container_name = \"huayra\"  # Replace with your container name\n","folder_name = \"raw\"\n","\n","# Name of the blob (file) with today's date\n","blob_name = f\"{folder_name}/amazon_us_raw{today_date}.csv\"  # Example: scraped_data_2024-10-07.csv\n","\n","# Create a container if it doesn't exist\n","container_client = blob_service_client.get_container_client(container_name)\n","try:\n","    container_client.create_container()\n","except Exception as e:\n","    print(f\"Container already exists: {e}\")\n","\n","\n","# Save the CSV locally (you can skip this if you already have the file path)\n","csv_file_path = f\"scraped_data_{today_date}.csv\"\n","df.to_csv(csv_file_path, index=False)\n","\n","# Upload the CSV file to Blob Storage with today's date in the name\n","with open(csv_file_path, \"rb\") as data:\n","    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n","    blob_client.upload_blob(data, overwrite=True)\n","    print(f\"CSV file {blob_name} uploaded successfully.\")\n"]},{"cell_type":"markdown","metadata":{"id":"xDK8AZuTybmM"},"source":["### Transformation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K5bUHvdXyIiE"},"outputs":[],"source":["df_new=df.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WrGjdvJyHbm"},"outputs":[],"source":["ordered_columns_original = [ 'link','Site','Category','rating', 'image_url',\n","    'Brand', 'Model Name', 'Product Title', 'price',\n","    'Operating System', 'Ram Memory Installed Size',\n","    'Memory Storage Capacity', 'Screen Size', 'Resolution', 'Refresh Rate',\n","    'CPU Speed', 'Connectivity Technology', 'CPU Model', 'Color',\n","    'Wireless Carrier', 'Cellular Technology','date',\n","     'All Reviews'\n","]\n","# Ensure all desired columns are present in the DataFrame\n","missing_columns = set(ordered_columns_original) - set(df_new.columns)\n","if missing_columns:\n","    print(f\"Warning: Missing columns: {missing_columns}\")\n","\n","df_new = df_new[ordered_columns_original]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3JhYaOKTyHYO"},"outputs":[],"source":["def format_column_names(column_names):\n","    return [name.replace(' ', '_').lower() for name in column_names]\n","\n","# Preprocess column names in the DataFrame\n","df_new.columns = format_column_names(df_new.columns)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgZlsskKyHVc"},"outputs":[],"source":["df_new.rename(columns={'operating_system':'os','ram_memory_installed_size':'ram',\n","                         'memory_storage_capacity':'storage','today':'date'}, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8-pqwPRAyHSk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728498735253,"user_tz":-180,"elapsed":24,"user":{"displayName":"Ahmed Abdelrhman","userId":"02369919969702643030"}},"outputId":"b5030d50-36bd-416b-80af-424873c5f241"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(284, 23)"]},"metadata":{},"execution_count":11}],"source":["#drop nulls in model name\n","df_new.dropna(subset=['model_name'], inplace=True)\n","df_new.reset_index(drop=True, inplace=True)\n","df_new.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ra1eJP5yHPX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728498735253,"user_tz":-180,"elapsed":20,"user":{"displayName":"Ahmed Abdelrhman","userId":"02369919969702643030"}},"outputId":"8bc556d7-c79c-419f-e925-824c32e92efd"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-12-f0591b57d383>:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df_new = df_new.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n"]}],"source":["# Convert all values to lowercase\n","df_new = df_new.applymap(lambda x: x.lower() if isinstance(x, str) else x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6KrjWknNyHMd"},"outputs":[],"source":["df_last=df_new.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xARRYfMPyHJl"},"outputs":[],"source":["# Function definitions with updated names\n","def format_samsung_model(model):\n","    model = model.lower()  # Convert to lowercase for consistency\n","\n","    # Extract special terms like FE, Ultra, Plus, 5G, 4G\n","    special_terms = re.findall(r'\\b(fe|ultra|plus)\\b', model, re.IGNORECASE)\n","    special_terms = list(dict.fromkeys(special_terms))  # Remove duplicates while maintaining order\n","    special_terms_str = ' '.join(special_terms).upper()\n","\n","    # Remove unwanted words and characters\n","    model = re.sub(r'\\b(samsung|galaxy)\\b', '', model).strip()\n","    model = re.sub(r'\\b(5g|4g)\\b', '', model).strip()  # Remove 5G/4G here to prevent duplication\n","\n","    # Extract core model name\n","    if 'note' in model:\n","        match = re.search(r'note\\s*(\\d*)\\s*(\\w*)', model)\n","        if match:\n","            note_num, note_suffix = match.groups()\n","            core_model = f\"Note {note_num} {note_suffix}\".strip().title()\n","    else:\n","        match = re.search(r'([a-z]+\\s*\\d+(?:\\s*[a-z]+)?)', model)\n","        if match:\n","            core_model = match.group(1).strip().title()\n","        else:\n","            core_model = model.strip().title()\n","\n","    # Remove special terms from core_model if they're already present\n","    for term in special_terms:\n","        core_model = re.sub(rf'\\b{term}\\b', '', core_model, flags=re.IGNORECASE).strip()\n","\n","    # Combine core model and special terms\n","    result = f'{core_model} {special_terms_str}'.strip()\n","\n","    return result.lower()\n","\n","def clean_motorola_model(model):\n","    if not isinstance(model, str):\n","        return model\n","\n","    # Remove \"moto\" and \"motorola\"\n","    model = re.sub(r'\\bmoto\\b|\\bmotorola\\b', '', model)\n","\n","    # Remove '+' characters\n","    model = re.sub(r'\\+', '', model)\n","\n","    # Remove parentheses but preserve the content\n","    model = re.sub(r'\\s*\\((\\d{4})\\)', r' \\1', model)  # Preserve year in parentheses\n","\n","    # Remove single quotes in years (e.g., '22)\n","    model = re.sub(r'\\'(\\d{2})', r'\\1', model)\n","\n","    # Replace multiple spaces with a single space\n","    model = re.sub(r'\\s+', ' ', model)\n","\n","    # Strip leading and trailing whitespace\n","    model = model.strip()\n","\n","    return model\n","def extract_google_model(model):\n","    # Ensure model is a string\n","    if not isinstance(model, str):\n","        return model  # Return the original value if it's not a string\n","\n","    # Convert to lowercase for consistency\n","    model = model.lower()\n","\n","    # Use regex to extract everything after \"pixel\"\n","    match = re.search(r'\\bpixel\\s+(.*)', model)\n","    if match:\n","        return match.group(1).strip()  # Return the portion after \"pixel\"\n","\n","    return model\n","\n","def clean_oneplus_model(model):\n","    if not isinstance(model, str):\n","        return model\n","\n","    # Remove \"oneplus\" and leading whitespace\n","    model = re.sub(r'\\boneplus\\b', '', model).strip()\n","\n","    # Remove parentheses but keep the content inside\n","    model = re.sub(r'\\s*\\(', ' ', model)\n","    model = re.sub(r'\\)', '', model)\n","\n","    # Remove any text associated with \"gb\"\n","    model = re.sub(r'\\s*\\d+gb\\b', '', model, flags=re.IGNORECASE).strip()\n","\n","    # Extract text up to and including \"5g\"\n","    match = re.search(r'(.*?\\b5g\\b)', model)\n","    if match:\n","        model = match.group(1).strip()\n","\n","    return model\n","\n","def remove_xiaomi_brand(model):\n","    return model.replace('xiaomi', '').strip()\n","\n","def extract_iphone_model(model):\n","    parts = model.split('iphone ', 1)\n","    if len(parts) > 1:\n","        return parts[1].split('\\t', 1)[0].strip()\n","    return model.strip()\n","\n","# Function to apply based on brand\n","def process_model(row):\n","    brand = row['brand'].lower()\n","    model_name = row['model_name']\n","\n","    if brand == 'samsung':\n","        return format_samsung_model(model_name)\n","    elif brand == 'motorola':\n","        return clean_motorola_model(model_name)\n","    elif brand == 'google':\n","        return extract_google_model(model_name)\n","    elif brand == 'oneplus':\n","        return clean_oneplus_model(model_name)\n","    elif brand == 'xiaomi':\n","        return remove_xiaomi_brand(model_name)\n","    elif brand == 'apple':\n","        return extract_iphone_model(model_name)\n","    else:\n","        return model_name\n","\n","\n","# Apply function to the DataFrame\n","df_last['model_name'] = df_last.apply(process_model, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clViBDjyyHHU"},"outputs":[],"source":["\n","# Define the function to extract and format OS versions\n","def extract_versions(text):\n","    if not isinstance(text, str):  # Ensure the input is a string\n","        return ''\n","\n","    text = text.lower()  # Convert to lowercase for consistency\n","\n","    results = set()  # Use a set to avoid duplicates\n","\n","    # Helper function to format version numbers\n","    def format_version(version):\n","        if '.' not in version:\n","            return f\"{version}.0\"  # Add '.0' if there is no decimal point\n","        return version\n","\n","    # Special case for 'google_android'\n","    if 'google_android' in text:\n","        text = 'android' + text.split('google_android')[1]\n","\n","    # Extract Android version numbers\n","    android_match = re.search(r'\\bandroid\\s+(\\d+(\\.\\d+)?)\\b', text)\n","    if android_match:\n","        version = format_version(android_match.group(1))\n","        results.add(f\"android {version}\")\n","\n","    # Extract iOS version numbers\n","    ios_match = re.search(r'\\bios\\s+(\\d+(\\.\\d+)?)\\b', text)\n","    if ios_match:\n","        version = format_version(ios_match.group(1))\n","        results.add(f\"ios {version}\")\n","\n","    # Extract any other OS version numbers\n","    os_match = re.search(r'\\b(\\w+os)\\s+(\\d+(\\.\\d+)?)\\b', text)\n","    if os_match:\n","        os_name = os_match.group(1).lower()\n","        version = format_version(os_match.group(2))\n","        results.add(f\"{os_name} {version}\")\n","\n","    # Return the results as a comma-separated string\n","    return ', '.join(results) if results else text\n","df_last['os'] = df_last['os'].apply(extract_versions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwOA2UQTyHE0"},"outputs":[],"source":["df_last['os']=df_last.os.map(lambda x: 'android' if 'oxygenos' in x else x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Smxt9Xcrylu7"},"outputs":[],"source":["# Define the function to extract RAM size\n","def extract_ram(text):\n","    if not isinstance(text, str):  # Ensure the input is a string\n","        return ''\n","\n","    # Use regex to find the RAM size\n","    match = re.search(r'(\\d+)\\s*gb', text, re.IGNORECASE)\n","    if match:\n","        return match.group(1)  # Return only the numeric part of the RAM size\n","\n","    return ''  # Return empty string if no match is found\n","df_last['ram']=df_last['ram'].apply(extract_ram)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrENiOdhylsc"},"outputs":[],"source":["# Function to extract storage size\n","def extract_storage(text):\n","    if not isinstance(text, str):  # Ensure the input is a string\n","        return ''\n","\n","    match = re.search(r'(\\d+)\\s*gb', text, re.IGNORECASE)\n","    if match:\n","        return match.group(1)  # Return only the numeric part of the storage size\n","\n","    return ''  # Return empty string if no match is found\n","\n","# Function to extract screen size\n","def extract_screen_size(text):\n","    if not isinstance(text, str):  # Ensure the input is a string\n","        return ''\n","\n","    match = re.search(r'(\\d+(\\.\\d+)?)\\s*inches?', text, re.IGNORECASE)\n","    if match:\n","        return match.group(1)  # Return the screen size\n","\n","    return ''  # Return empty string if no match is found\n","\n","# Function to extract resolution\n","def extract_resolution(text):\n","    if not isinstance(text, str):  # Ensure the input is a string\n","        return ''\n","\n","    # Use regex to find resolution in formats like '1920 x 1080', '1920x1080', '1280 x 720 pixels', etc.\n","    match = re.search(r'(\\d{3,4})\\s*x\\s*(\\d{3,4})\\b', text, re.IGNORECASE)\n","    if match:\n","        return f\"{match.group(1)} x {match.group(2)}\"  # Return resolution in 'width x height' format\n","\n","    # Handle cases where resolution might be written as '1920x1080' without spaces\n","    match_no_space = re.search(r'(\\d{3,4})\\s*x\\s*(\\d{3,4})', text, re.IGNORECASE)\n","    if match_no_space:\n","        return f\"{match_no_space.group(1)} x {match_no_space.group(2)}\"  # Return resolution in 'width x height' format\n","\n","    return ''  # Return empty string if no match is found\n","\n","# Function to extract refresh rate\n","def extract_refresh_rate(text):\n","    if not isinstance(text, str):  # Ensure the input is a string\n","        return ''\n","\n","    match = re.search(r'(\\d+)\\s*hz', text, re.IGNORECASE)\n","    if match:\n","        return match.group(1)  # Return the refresh rate\n","\n","    return ''  # Return empty string if no match is found\n","\n","# Function to extract CPU speed\n","def extract_cpu_speed(text):\n","    if not isinstance(text, str):  # Ensure the input is a string\n","        return ''\n","\n","    match = re.search(r'(\\d+(\\.\\d+)?)\\s*ghz', text, re.IGNORECASE)\n","    if match:\n","        return match.group(1)  # Return the CPU speed\n","\n","    return ''  # Return empty string if no match is found\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEJmZpsqylp8"},"outputs":[],"source":["df_last['storage']=df_last['storage'].apply(extract_storage)\n","df_last['screen_size']=df_last['screen_size'].apply(extract_screen_size)\n","df_last['resolution']=df_last['resolution'].apply(extract_resolution)\n","df_last['refresh_rate']=df_last['refresh_rate'].apply(extract_refresh_rate)\n","df_last['cpu_speed']=df_last['cpu_speed'].apply(extract_cpu_speed)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D7_Np1zNylnl"},"outputs":[],"source":["df_last.rename(columns={'ram':'ram_gb','screen_size':'screen_size_in','refresh_rate':'refresh_rate_hz','cpu_speed':'cpu_speed_ghz'},inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GNHcjh7JyllQ"},"outputs":[],"source":["df_last['wireless_carrier']=df_last['wireless_carrier'].map(lambda x: 'unlocked' if x=='unlocked for all carriers' or x==' unlocked' else x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uznrBIsfylir"},"outputs":[],"source":["# Define the mapping dictionary\n","carrier_mapping = {\n","    'unlocked': 'unlocked',\n","    'tracfone': 'tracfone',\n","    'verizon': 'verizon',\n","    'verizon wireless': 'verizon',\n","    't-mobile': 't-mobile',\n","    'simple mobile': 'simple mobile',\n","    '3': '3',\n","    'mvno': 'mvno',\n","    't-mobile, at&t': 't-mobile',\n","    'at&t': 'at&t',\n","    'straight talk': 'straight talk',\n","    'boost mobile': 'boost mobile',\n","    'vodafone': 'vodafone',\n","    't-mobile, unlocked': 't-mobile',\n","    't-mobile, unlocked, verizon, sprint': 't-mobile',\n","    't-mobile, unlocked, sprint': 't-mobile',\n","    'sprint': 'sprint',\n","    'total wireless': 'total wireless'\n","}\n","\n","# Handle NaN values separately\n","df_last['wireless_carrier'] = df_last['wireless_carrier'].fillna('')\n","\n","# Apply the mapping dictionary\n","df_last['wireless_carrier'] = df_last['wireless_carrier'].map(carrier_mapping).fillna(df_last['wireless_carrier'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZpHnpdBIylgd"},"outputs":[],"source":["import numpy as np\n","# Replace all zeroes with NaNs\n","df_last.replace(0, np.nan, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MJahX59KyleD"},"outputs":[],"source":["# Replace empty strings and spaces with NaNs\n","df_last.replace(r'^\\s*$', np.nan, regex=True, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVc38f9sylTT"},"outputs":[],"source":["# Convert `price_usd` to numeric\n","df_last['price'] = pd.to_numeric(df_last['price'], errors='coerce')\n","\n","# Convert `ram_gb`, `storage`, `screen_size_in`, `refresh_rate_hz`, `cpu_speed_ghz` to numeric\n","df_last['ram_gb'] = pd.to_numeric(df_last['ram_gb'], errors='coerce')\n","df_last['storage'] = pd.to_numeric(df_last['storage'], errors='coerce')\n","df_last['screen_size_in'] = pd.to_numeric(df_last['screen_size_in'], errors='coerce')\n","df_last['refresh_rate_hz'] = pd.to_numeric(df_last['refresh_rate_hz'], errors='coerce')\n","df_last['cpu_speed_ghz'] = pd.to_numeric(df_last['cpu_speed_ghz'], errors='coerce')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JftXT-K8yHCM"},"outputs":[],"source":["def extract_product_id(url):\n","    # Regular expression pattern to capture product ID after '/dp/'\n","    pattern = r'/dp/([A-Za-z0-9]+)'\n","    match = re.search(pattern, url)\n","\n","    if match:\n","        return match.group(1)\n","    else:\n","        return None\n","\n","def get_review_url( product_url):\n","        asin =extract_product_id(product_url)\n","        if asin:\n","            return f\"https://www.amazon.com/product-reviews/{asin}\"\n","        return None\n","def extract_rating(text):\n","    # Regular expression to match a floating point number at the start of the string\n","    pattern = r'(\\d+\\.\\d+)'\n","    match = re.search(pattern, text)\n","\n","    if match:\n","        return float(match.group(1))\n","    else:\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pu451_Gaznw0"},"outputs":[],"source":["df_last['ASIN']=df_last['link'].apply(extract_product_id)\n","df_last['review_url']=df_last['link'].apply(get_review_url)\n","df_last['rating']=df_last['rating'].apply(extract_rating)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xaTQyvQ5yG_u"},"outputs":[],"source":["df=df_last.copy()\n","# Function to extract and remove network type\n","def extract_network(text):\n","    # Check if text is a string, otherwise return it unchanged\n","    if isinstance(text, str):\n","        match = re.search(r'(5g|4g)', text)\n","        if match:\n","            network = match.group(0)\n","            # Remove 5G/4G from the original text\n","            text = re.sub(r'\\s?(5g|4g)\\s?', '', text).strip()\n","            return text, network\n","    return text, None\n","\n","# Initialize 'network' column as None\n","df['network'] = None\n","\n","# Apply the extraction logic to each column and update 'network' column\n","for col in ['model_name', 'product_title', 'cellular_technology']:\n","    # Apply the function to extract network and updated column text\n","    df[col], extracted_networks = zip(*df[col].apply(extract_network))\n","\n","    # Fill the 'network' column where it is None with the extracted network\n","    df['network'] = df['network'].combine_first(pd.Series(extracted_networks))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmJkwbaFyG88"},"outputs":[],"source":["# Fill any remaining None values in 'network' using 'cellular_technology'\n","df['network'] = df['network'].fillna(df['cellular_technology'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HO3KL4IGyG6T"},"outputs":[],"source":["df.drop(columns=['cellular_technology'],inplace=True)\n","df.dropna(subset=['price'],inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9txlLO_HyGut"},"outputs":[],"source":["df.date=df.date.astype('datetime64[ns]')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"se7C5DAiOH3Q","outputId":"87e187be-9f20-4acd-b1b7-8e66f462c06e","executionInfo":{"status":"ok","timestamp":1728498737577,"user_tz":-180,"elapsed":1499,"user":{"displayName":"Ahmed Abdelrhman","userId":"02369919969702643030"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Container already exists: The specified container already exists.\n","RequestId:3bd7b668-b01e-0042-3c79-1a4019000000\n","Time:2024-10-09T18:32:23.1205609Z\n","ErrorCode:ContainerAlreadyExists\n","Content: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>ContainerAlreadyExists</Code><Message>The specified container already exists.\n","RequestId:3bd7b668-b01e-0042-3c79-1a4019000000\n","Time:2024-10-09T18:32:23.1205609Z</Message></Error>\n","CSV file transformed/amazon_us_raw2024-10-09.csv uploaded successfully.\n"]}],"source":["from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n","import os\n","\n","# Azure Storage connection string\n","connect_str = \"DefaultEndpointsProtocol=https;AccountName=ynwa;AccountKey=aTLtGZuymmaAirlsL1/39g3dDjaQvBFu3lHzQaUrY0o6LDdGduAW8Wbk8qI7fuilCKS8chuiklq7+AStBN3UdA==;EndpointSuffix=core.windows.net\"\n","\n","# Initialize BlobServiceClient\n","blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n","\n","# Name of the container and blob\n","container_name = \"huayra\"  # Replace with your container name\n","folder_name = \"transformed\"\n","\n","# Name of the blob (file) with today's date\n","blob_name = f\"{folder_name}/amazon_us_raw{today_date}.csv\"  # Example: scraped_data_2024-10-07.csv\n","\n","# Create a container if it doesn't exist\n","container_client = blob_service_client.get_container_client(container_name)\n","try:\n","    container_client.create_container()\n","except Exception as e:\n","    print(f\"Container already exists: {e}\")\n","\n","\n","# Save the CSV locally (you can skip this if you already have the file path)\n","csv_file_path = f\"scraped_data_{today_date}.csv\"\n","df.to_csv(csv_file_path, index=False)\n","\n","# Upload the CSV file to Blob Storage with today's date in the name\n","with open(csv_file_path, \"rb\") as data:\n","    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n","    blob_client.upload_blob(data, overwrite=True)\n","    print(f\"CSV file {blob_name} uploaded successfully.\")\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO2y9rIek38JCvxiKE4ldoY"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}