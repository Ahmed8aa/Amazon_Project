{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmed8aa/az_sc_tr/blob/main/EG_SC%26TR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_1GAmX4RXK3",
        "outputId": "6d65f1bd-89e0-4686-9ada-471ba006b1ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.10.10)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting azure-storage-blob\n",
            "  Downloading azure_storage_blob-12.23.1-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Collecting azure-core>=1.30.0 (from azure-storage-blob)\n",
            "  Downloading azure_core-1.31.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (43.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (4.12.2)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.30.0->azure-storage-blob) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.30.0->azure-storage-blob) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp) (0.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2024.8.30)\n",
            "Downloading azure_storage_blob-12.23.1-py3-none-any.whl (405 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.6/405.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.31.0-py3-none-any.whl (197 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.4/197.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate, azure-core, azure-storage-blob\n",
            "Successfully installed azure-core-1.31.0 azure-storage-blob-12.23.1 isodate-0.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install aiohttp nest-asyncio pandas beautifulsoup4 azure-storage-blob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import aiohttp\n",
        "import asyncio\n",
        "from urllib.parse import urljoin\n",
        "import re\n",
        "from datetime import datetime\n",
        "import random\n",
        "import logging\n",
        "import nest_asyncio\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Apply the async fix for Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "class WebScraper:\n",
        "    def __init__(self):\n",
        "        # Define a list of user agents\n",
        "        self.user_agents = [\n",
        "            'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0',\n",
        "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36',\n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36 Edg/93.0.961.47',\n",
        "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36 OPR/79.0.4143.50',\n",
        "            'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36 Vivaldi/4.1',\n",
        "            'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0',\n",
        "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/601.7.7 (KHTML, like Gecko) Version/9.1.2 Safari/601.7.7',\n",
        "        ]\n",
        "        self.current_user_agent_index = 0  # Track current user agent index\n",
        "\n",
        "    def get_next_user_agent(self):\n",
        "        user_agent = self.user_agents[self.current_user_agent_index]\n",
        "        self.current_user_agent_index = (\n",
        "            self.current_user_agent_index + 1) % len(self.user_agents)\n",
        "        return user_agent\n",
        "\n",
        "    async def fetch_page(self, session, url, max_retries=1, initial_delay=2):\n",
        "        user_agent = self.get_next_user_agent()  # Store the updated user agent\n",
        "        headers = {\n",
        "            \"User-Agent\": user_agent,\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
        "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "            \"Connection\": \"keep-alive\",\n",
        "            \"Upgrade-Insecure-Requests\": \"1\"\n",
        "        }\n",
        "        captcha_indicators = [\n",
        "            \"captcha\", \"i am not a robot\", \"robot\",\n",
        "            \"prove you are human\", \"Enter the characters\"\n",
        "        ]\n",
        "        delay = initial_delay\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                async with session.get(url, headers=headers) as response:\n",
        "                    # First check the status code\n",
        "                    if response.status == 403:\n",
        "                        print(\"CAPTCHA detected via HTTP status 403.\")\n",
        "                        return None  # You can handle CAPTCHA solution here if needed\n",
        "                    html = await response.text()  # Directly fetch content\n",
        "\n",
        "                    if any(indicator in str(response.url).lower() for indicator in captcha_indicators):\n",
        "                        print(\"CAPTCHA detected!\")\n",
        "\n",
        "                    logging.info(f\"Successfully fetched page from {url[-4:]}\")\n",
        "                    return html  # Return the fetched HTML\n",
        "\n",
        "            except asyncio.TimeoutError:\n",
        "                logging.error(f\"Timeout error occurred while fetching {url}. Retrying...\")\n",
        "                await asyncio.sleep(delay)\n",
        "                delay *= 1.5  # Exponential backoff\n",
        "            except aiohttp.ClientError as e:\n",
        "                logging.error(f\"Error fetching {url}: {str(e)}\")\n",
        "                await asyncio.sleep(delay)\n",
        "                delay *= 1.5  # Exponential backoff\n",
        "\n",
        "        logging.error(f\"Max retries reached for {url}\")\n",
        "        return None\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = re.sub(r'[\\u200f\\u200e]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def remove_key_from_value(self, key, value):\n",
        "        key_cleaned = self.clean_text(key)\n",
        "        value_cleaned = self.clean_text(value)\n",
        "        if value_cleaned.startswith(key_cleaned):\n",
        "            return value_cleaned[len(key_cleaned):].strip(\" :\")\n",
        "        return value_cleaned\n",
        "\n",
        "    async def scrape_product_data(self, session, product_url, region):\n",
        "        html = await self.fetch_page(session, product_url)\n",
        "        if not html:\n",
        "            return None\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "        # Extracting Price\n",
        "        price_selector_1 = \"#corePriceDisplay_desktop_feature_div .a-price-whole\"\n",
        "        price_selector_2 = \"div.a-section.a-spacing-micro span.a-price.a-text-price.a-size-medium span.a-offscreen\"\n",
        "\n",
        "        price_element = soup.select_one(price_selector_1)\n",
        "        if price_element:\n",
        "            price = price_element.get_text(strip=True)\n",
        "        else:\n",
        "            price_element = soup.select_one(price_selector_2)\n",
        "            price = price_element.get_text(\n",
        "                strip=True) if price_element else np.nan\n",
        "\n",
        "        # Extract discount\n",
        "        discount = None\n",
        "\n",
        "        # Try various selectors\n",
        "        possible_selectors = [\n",
        "            \"span.a-color-price\",\n",
        "            \".savingsPercentage\"\n",
        "        ]\n",
        "\n",
        "        for selector in possible_selectors:\n",
        "            discount_elements = soup.select(selector)\n",
        "            for element in discount_elements:\n",
        "                discount_text = element.get_text(strip=True)\n",
        "\n",
        "                # Flexible regex to capture both negative and non-negative percentages\n",
        "                discount_match = re.search(r'(-?\\d+%)', discount_text)\n",
        "\n",
        "                if discount_match:\n",
        "                    discount = discount_match.group(1)\n",
        "                    break\n",
        "            if discount:\n",
        "                break\n",
        "\n",
        "        # Rating extraction logic\n",
        "        rate_element = soup.select_one(\"span.a-icon-alt\")\n",
        "        if rate_element and \"out of 5 stars\" in rate_element.text:\n",
        "            rate = rate_element.text.replace(\"out of 5 stars\", \"\").strip()\n",
        "        else:\n",
        "            rate = np.nan\n",
        "        site = f\"amazon_{region.lower()}\"\n",
        "        product_data = {\n",
        "            \"date_column\": datetime.today().strftime('%Y-%m-%d'),\n",
        "            \"product_url\": product_url,\n",
        "            \"site\": site,  # Changed from amazon_sa to amazon_us\n",
        "            \"category\": \"mobile phones\",\n",
        "            \"Title\": soup.select_one(\"#productTitle\").text.strip() if soup.select_one(\"#productTitle\") else np.nan,\n",
        "            \"Rate\": rate,\n",
        "            \"Price\": price,\n",
        "            \"Discount\": discount,\n",
        "            \"Image URL\": soup.select_one(\"#imgTagWrapperId img\")['src'] if soup.select_one(\"#imgTagWrapperId img\") else np.nan,\n",
        "            \"Description\": soup.select_one(\"#feature-bullets\").text.strip() if soup.select_one(\"#feature-bullets\") else np.nan\n",
        "        }\n",
        "\n",
        "        tables = {\n",
        "            'first_table': '.a-normal.a-spacing-micro',\n",
        "            'tech_specs': '#productDetails_techSpec_section_1',\n",
        "            'right_table': '#productDetails_detailBullets_sections1',\n",
        "            'new_table': 'ul.a-unordered-list.a-nostyle.a-vertical.a-spacing-none.detail-bullet-list'\n",
        "        }\n",
        "\n",
        "        for table_name, selector in tables.items():\n",
        "            table = soup.select_one(selector)\n",
        "            if table:\n",
        "                if table_name == 'new_table':\n",
        "                    items = table.find_all('li')\n",
        "                    for item in items:\n",
        "                        key_element = item.select_one('span.a-text-bold')\n",
        "                        value_element = item.find(\n",
        "                            'span', class_=lambda x: x != 'a-text-bold')\n",
        "                        if key_element and value_element:\n",
        "                            key = self.clean_text(\n",
        "                                key_element.text.strip().replace(':', ''))\n",
        "                            value = self.clean_text(value_element.text.strip())\n",
        "                            value = self.remove_key_from_value(key, value)\n",
        "                            product_data[key] = value\n",
        "                else:\n",
        "                    rows = table.find_all('tr')\n",
        "                    for row in rows:\n",
        "                        key_element = row.find(['th', 'td'])\n",
        "                        value_element = row.find_all(\n",
        "                            'td')[-1] if row.find_all('td') else None\n",
        "                        if key_element and value_element:\n",
        "                            key = self.clean_text(\n",
        "                                key_element.get_text(strip=True))\n",
        "                            value = self.clean_text(\n",
        "                                value_element.get_text(strip=True))\n",
        "                            product_data[key] = value\n",
        "\n",
        "        reviews = []\n",
        "        review_cards = soup.select(\"div[data-hook='review']\")\n",
        "        for review in review_cards[:5]:\n",
        "            reviewer_name = review.select_one(\n",
        "                \"span.a-profile-name\").text.strip()\n",
        "            review_rating = review.select_one(\n",
        "                \"i.a-icon-star span.a-icon-alt\").text.strip().replace(\"out of 5 stars\", \"\")\n",
        "            review_date = review.select_one(\"span.review-date\").text.strip()\n",
        "            review_text = review.select_one(\n",
        "                \"span[data-hook='review-body']\").text.strip()\n",
        "            reviews.append({\n",
        "                \"Reviewer\": reviewer_name,\n",
        "                \"Rating\": review_rating,\n",
        "                \"Date\": review_date,\n",
        "                \"Review\": review_text\n",
        "            })\n",
        "\n",
        "        product_data['reviews'] = reviews\n",
        "        return product_data\n",
        "\n",
        "    async def scrape_page_products(self, session, page_url, region):\n",
        "        html = await self.fetch_page(session, page_url)\n",
        "        if not html:\n",
        "            return [], None\n",
        "\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "        product_links = soup.find_all(\n",
        "            'a', class_='a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal')\n",
        "        if region == 'eg':\n",
        "            base_url = 'https://www.amazon.eg'\n",
        "        elif region == 'sa':\n",
        "            base_url = 'https://www.amazon.sa'\n",
        "        elif region == 'us':\n",
        "            base_url = 'https://www.amazon.com'\n",
        "        elif region == 'jp':\n",
        "            base_url = 'https://www.amazon.co.jp'\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported region: {region}\")\n",
        "\n",
        "        list_products_links = [urljoin(base_url, link.get('href'))\n",
        "                               for link in product_links if link.get('href')]\n",
        "\n",
        "        next_button = soup.select_one(\"a.s-pagination-next\")\n",
        "        next_page_url = urljoin(\n",
        "            base_url, next_button['href']) if next_button and next_button.get('href') else None\n",
        "\n",
        "        return list_products_links, next_page_url\n",
        "\n",
        "    async def scrape_all_products(self, start_page_url, region, max_pages=17):\n",
        "        all_product_links = set()\n",
        "        current_page_url = start_page_url\n",
        "        page_number = 1\n",
        "        pages_scraped = 0\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            while current_page_url and page_number <= max_pages:\n",
        "                logging.info(f\"Scraping page {page_number}: {current_page_url}\")\n",
        "                products, next_page = await self.scrape_page_products(session, current_page_url, region)\n",
        "\n",
        "                if products:\n",
        "                    all_product_links.update(products)\n",
        "                    logging.info(f\"Found {len(products)} product links on page {page_number}.\")\n",
        "                else:\n",
        "                    logging.info(f\"No products found on page {page_number}. Retrying...\")\n",
        "                    # Wait for 5 seconds before retrying\n",
        "                    await asyncio.sleep(5)\n",
        "                    continue\n",
        "                # Increment the pages scraped counter\n",
        "                pages_scraped += 1\n",
        "\n",
        "                if pages_scraped >= 10:  # After every 10 pages\n",
        "                    # Random delay between 10 to 30 seconds\n",
        "                    random_delay = random.uniform(5, 12)\n",
        "                    logging.info(f\"Pausing for {random_delay:.2f} seconds after scraping {pages_scraped} pages.\")\n",
        "                    await asyncio.sleep(random_delay)\n",
        "                    pages_scraped = 0  # Reset the counter after the delay\n",
        "\n",
        "                if not next_page:\n",
        "                    logging.info(\"No more pages to scrape.\")\n",
        "                    break\n",
        "\n",
        "                current_page_url = next_page\n",
        "                page_number += 1\n",
        "                await asyncio.sleep(random.uniform(3, 6))\n",
        "\n",
        "            logging.info(f\"Total product links found: {len(all_product_links)}\")\n",
        "\n",
        "            tasks = [self.scrape_product_data(\n",
        "                session, url, region) for url in all_product_links]\n",
        "            all_product_data = await asyncio.gather(*tasks)\n",
        "\n",
        "        product_df = pd.DataFrame(\n",
        "            [data for data in all_product_data if data is not None])\n",
        "\n",
        "        # product_df.to_csv(f'/content/drive/My Drive/Egypt-Data/Amazon_{region.upper()}_{page_number-1}_pages-Row.csv', index=False)\n",
        "        return product_df\n",
        "\n",
        "\n",
        "def main():\n",
        "    start_page_url = \"https://www.amazon.eg/s?i=electronics&rh=n%3A21832883031%2Cp_123%3A110955%7C1500397%7C329744%7C338933%7C339703%7C367594%7C380758%7C46655%7C559198%7C568349&dc&fs=true&language=en&qid=1726254193&rnid=91049076031&ref=sr_pg_1\"\n",
        "    scraper = WebScraper()\n",
        "    product_df = asyncio.run(scraper.scrape_all_products(\n",
        "        start_page_url, region=\"eg\", max_pages=25))\n",
        "    logging.info(f\"Script completed.\")\n",
        "    return product_df\n",
        "\n"
      ],
      "metadata": {
        "id": "rW5Mlk0wRgBB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the main function\n",
        "df_f = main()"
      ],
      "metadata": {
        "id": "rujRe5wtxC9V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "import os\n",
        "# Get today's date\n",
        "today_date = datetime.now().date()\n",
        "# Azure Storage connection string\n",
        "connect_str = \"DefaultEndpointsProtocol=https;AccountName=ynwa;AccountKey=aTLtGZuymmaAirlsL1/39g3dDjaQvBFu3lHzQaUrY0o6LDdGduAW8Wbk8qI7fuilCKS8chuiklq7+AStBN3UdA==;EndpointSuffix=core.windows.net\"\n",
        "\n",
        "# Initialize BlobServiceClient\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "\n",
        "# Name of the container and blob\n",
        "container_name = \"huayra\"  # Replace with your container name\n",
        "folder_name = \"raw\"\n",
        "\n",
        "# Name of the blob (file) with today's date\n",
        "blob_name = f\"{folder_name}/amazon_eg_raw{today_date}.csv\"  # Example: scraped_data_2024-10-07.csv\n",
        "\n",
        "# Create a container if it doesn't exist\n",
        "container_client = blob_service_client.get_container_client(container_name)\n",
        "try:\n",
        "    container_client.create_container()\n",
        "except Exception as e:\n",
        "    print(f\"Container already exists: {e}\")\n",
        "\n",
        "\n",
        "# Save the CSV locally (you can skip this if you already have the file path)\n",
        "csv_file_path = f\"scraped_data_{today_date}.csv\"\n",
        "df_f.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Upload the CSV file to Blob Storage with today's date in the name\n",
        "with open(csv_file_path, \"rb\") as data:\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "    blob_client.upload_blob(data, overwrite=True)\n",
        "    print(f\"CSV file {blob_name} uploaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sW0Em2_Ivqwi",
        "outputId": "6ace1de4-d7bc-44a9-cc91-76ad7baa3983"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Container already exists: The specified container already exists.\n",
            "RequestId:58290efb-d01e-0026-31b2-20b181000000\n",
            "Time:2024-10-17T16:33:27.9589888Z\n",
            "ErrorCode:ContainerAlreadyExists\n",
            "Content: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>ContainerAlreadyExists</Code><Message>The specified container already exists.\n",
            "RequestId:58290efb-d01e-0026-31b2-20b181000000\n",
            "Time:2024-10-17T16:33:27.9589888Z</Message></Error>\n",
            "CSV file raw/amazon_eg_raw2024-10-17.csv uploaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "\n",
        "df=df_f.copy()\n",
        "# Drop rows where the 'Title' column has no values\n",
        "df = df.dropna(subset=[\"Title\"], how='all', axis=0)\n",
        "\n",
        "# Define keywords related to phones that may appear in 'Title' or 'Description'\n",
        "phone_keywords = ['phone', 'smartphone', 'g', 'gb',]\n",
        "\n",
        "# Filter out rows that are likely not phones based on the absence of phone-related keywords\n",
        "non_phone_df = df[~(\n",
        "    df['Title'].str.lower().str.contains('|'.join(phone_keywords), case=False, na=False) |\n",
        "    df['Description'].str.lower().str.contains('|'.join(phone_keywords), case=False, na=False)\n",
        ")]\n",
        "\n",
        "# Display the non_phone_df DataFrame \"For Debugging\"\n",
        "# non_phone_df\n",
        "\n",
        "# Keep rows that are likely phones based on the presence of phone-related keywords\n",
        "filtered_df = df[\n",
        "    df['Title'].str.lower().str.contains('|'.join(phone_keywords), case=False, na=False) |\n",
        "    df['Description'].str.lower().str.contains('|'.join(phone_keywords), case=False, na=False)\n",
        "]\n",
        "\n",
        "# Drop columns where all values are NaN\n",
        "filtered_df = filtered_df.dropna(axis=1, how='all')\n",
        "\n",
        "# Fill NaN values in 'column' with values from 'same column with different name'\n",
        "if 'OS' in filtered_df.columns:\n",
        "    filtered_df['Operating System'] = filtered_df['Operating System'].fillna(filtered_df['OS'])\n",
        "\n",
        "filtered_df.loc[:,'Connectivity technologies'] = filtered_df['Connectivity technologies'].fillna(filtered_df['Connectivity Technology'])\n",
        "# filtered_df.loc[:,'Screen Size'] = filtered_df['Screen Size'].fillna(filtered_df['Standing screen display size'])\n",
        "filtered_df.loc[:,'Wireless Provider'] = filtered_df['Wireless Provider'].fillna(filtered_df['Wireless communication technologies'])\n",
        "# filtered_df.loc[:,'Other display features'] = filtered_df['Other display features'].fillna(filtered_df['Display resolution'])\n",
        "\n",
        "# Define a function to fill in 'Brand Name'\n",
        "def fill_brand_name(row):\n",
        "    if pd.notna(row['Brand Name']):\n",
        "        return row['Brand Name']\n",
        "    elif pd.notna(row['Manufacturer']):\n",
        "        return row['Manufacturer']\n",
        "    else:\n",
        "        return row['Title'].split()[0]  # Take the first word of the title\n",
        "\n",
        "# Apply the function to each row in the DataFrame\n",
        "filtered_df.loc[:,'Brand Name'] = filtered_df.apply(fill_brand_name, axis=1)\n",
        "\n",
        "# Function to extract RAM and Storage\n",
        "def extract_memory_capacity(row):\n",
        "    title = row['Title'].lower()\n",
        "    description = row['Description'].lower() if pd.notna(row['Description']) else \"\" # Handle Nan\n",
        "\n",
        "    text = title + \" \" + description\n",
        "\n",
        "    # Find all 'number GB/gb' in the title\n",
        "    matches = re.findall(r'(\\d+)\\s*(GB|gb)', text)\n",
        "\n",
        "    # Initialize variables for RAM and storage\n",
        "    ram_capacity = None\n",
        "    storage_capacity = None\n",
        "\n",
        "    for match in matches:\n",
        "        value, unit = match  # Get the value and unit\n",
        "\n",
        "        # Construct the full capacity string\n",
        "        capacity_str = f\"{value}{unit}\"\n",
        "\n",
        "        # Check proximity context for RAM\n",
        "        if 'ram' in title:\n",
        "            ram_match = re.search(r'(\\d+)\\s*(GB|gb)\\s*ram', title)\n",
        "            if ram_match and f\"{ram_match.group(1)}{ram_match.group(2)}\" == capacity_str:\n",
        "                ram_capacity = capacity_str\n",
        "\n",
        "        # Check proximity context for Storage (storage/rom)\n",
        "        if 'storage' in title or 'rom' in title:\n",
        "            storage_match = re.search(r'(\\d+)\\s*(GB|gb)\\s*(storage|rom)', title)\n",
        "            if storage_match and f\"{storage_match.group(1)}{storage_match.group(2)}\" == capacity_str:\n",
        "                storage_capacity = capacity_str\n",
        "\n",
        "        # If no explicit mention of 'ram' or 'storage', make an educated guess\n",
        "        if not ram_capacity and int(value) < 16:\n",
        "            ram_capacity = capacity_str\n",
        "        elif not storage_capacity and int(value) >= 16:\n",
        "            storage_capacity = capacity_str\n",
        "\n",
        "    # Update the 'storage' column if it's null and storage value is found\n",
        "    if pd.isnull(row['Memory Storage Capacity']) and storage_capacity:\n",
        "        row['Memory Storage Capacity'] = storage_capacity\n",
        "\n",
        "    # Update the 'ram_gb' column if it's null and RAM value is found\n",
        "    if pd.isnull(row['RAM Memory Installed']) and ram_capacity:\n",
        "        row['RAM Memory Installed'] = ram_capacity\n",
        "\n",
        "    return row\n",
        "\n",
        "# Apply the function to rows with missing storage values\n",
        "filtered_df = filtered_df.apply(extract_memory_capacity, axis=1)\n",
        "\n",
        "\n",
        "# Define a dictionary for popular operating systems based on the brand\n",
        "popular_os = {\n",
        "    'realme': 'Android',\n",
        "    'oppo': 'Android',\n",
        "    'nokia': 'Android',  # Newer Nokia smartphones\n",
        "    'samsung': 'Android',\n",
        "    'xiaomi': 'Android'\n",
        "}\n",
        "\n",
        "# Function to fill missing OS based on brand\n",
        "def fill_operating_system(row):\n",
        "    if pd.isnull(row['Operating System']) and row['Brand Name'].lower() in popular_os:\n",
        "        row['Operating System'] = popular_os[row['Brand Name'].lower()]  # Fill with popular OS\n",
        "    return row\n",
        "\n",
        "# Apply the function to the DataFrame\n",
        "filtered_df = filtered_df.apply(fill_operating_system, axis=1)\n",
        "\n",
        "# Check the result\n",
        "filtered_df[['Operating System']].isna().value_counts()\n",
        "\n",
        "# Drop unnecessary columns\n",
        "columns_to_drop = ['OS', 'Connectivity Technology',\n",
        "                   'Wireless communication technologies']\n",
        "\n",
        "# # Get the range of columns between 'Country of origin' and 'Display Type'\n",
        "# start_series1 = filtered_df.columns.get_loc('Country of origin')\n",
        "# end_series1 = filtered_df.columns.get_loc('Display Type')\n",
        "\n",
        "\n",
        "# # Get the range of columns between 'Product Dimensions' and 'Number Of Lithium Ion Cells'\n",
        "# start_series2 = filtered_df.columns.get_loc('Product Dimensions')\n",
        "# end_series12 = filtered_df.columns.get_loc('Number Of Lithium Ion Cells')\n",
        "\n",
        "# # Add those columns to the drop list\n",
        "# columns_to_drop.extend(filtered_df.columns[start_series1:end_series1+1])\n",
        "# columns_to_drop.extend(filtered_df.columns[start_series2:end_series12+1])\n",
        "\n",
        "# Drop the columns\n",
        "filtered_df = filtered_df.drop(columns=columns_to_drop)\n",
        "\n",
        "\n",
        "# Filter rows where 'Model Name' is NaN\n",
        "filtered_model_df = filtered_df[filtered_df['Model Name'].isna()]\n",
        "filtered_model_df[['Title', 'Model Name', 'Brand Name']].head(15)\n",
        "\n",
        "\n",
        "# Function to clean the text by stripping spaces and converting to lower or upper case\n",
        "def clean_text(df, columns):\n",
        "    for col in columns:\n",
        "        df.loc[:, col] = df[col].str.strip().str.lower()\n",
        "    return df\n",
        "\n",
        "# Generalized function to extract the model name using regex and specific word counts\n",
        "def extract_model_name(title, regex_pattern=None, word_count=3, capitalize=True):\n",
        "    if pd.isna(title):\n",
        "        return None\n",
        "\n",
        "    title = title.lower() if capitalize else title.upper()\n",
        "\n",
        "    if regex_pattern:\n",
        "        model_match = re.search(regex_pattern, title)\n",
        "        if model_match:\n",
        "            return model_match.group(0).title() if capitalize else model_match.group(0).upper()\n",
        "\n",
        "    # Default case: take the first few words\n",
        "    return ' '.join(title.split()[:word_count]).title() if capitalize else ' '.join(title.split()[:word_count]).upper()\n",
        "\n",
        "# Define brand-specific model extraction logic\n",
        "def get_extraction_rule(brand, title):\n",
        "    title_lower = title.lower()\n",
        "\n",
        "    if brand == 'oppo':\n",
        "        return {'regex': None, 'word_count': 4 if 'reno' in title_lower else 2}\n",
        "    elif brand == 'realme':\n",
        "        return {'regex': None, 'word_count': 4 if 'pro' in title_lower else 3}\n",
        "    elif brand == 'nokia':\n",
        "        return {'regex': r'^(nokia\\s*\\w+|\\w+)', 'word_count': 3}\n",
        "    elif brand == 'redmi':\n",
        "        return {'regex': r'redmi\\s*\\d+[a-z]*', 'word_count': 3}\n",
        "    elif brand == 'samsung':\n",
        "        return {'regex': r'(galaxy\\s+\\w+\\s*\\d*\\s*(5g|4g)?)', 'word_count': 3}\n",
        "    elif brand == 'honor':\n",
        "        return {'regex': r'(HONOR [\\w\\s]+?)(\\s*\\d+GB|\\s*(?:4G|5G|LTE)|$)', 'word_count': 3, 'capitalize': False}\n",
        "    elif brand == 'xiaomi':\n",
        "        return {'regex': None, 'word_count': 5 if 'redmi' in title_lower else 3}\n",
        "    elif brand == 'apple':\n",
        "        return {'regex': r'^([a-z\\s]+?)(\\d+)\\s*(\\w*)\\s*(pro|plus)?', 'word_count': 3}\n",
        "    else:\n",
        "        return {'regex': None, 'word_count': 3}\n",
        "\n",
        "# Apply the model extraction rules based on the brand\n",
        "def apply_model_extraction(row):\n",
        "    brand = row['Brand Name']\n",
        "    title = row['Title']\n",
        "\n",
        "    if pd.notna(row['Model Name']):\n",
        "        return row['Model Name']  # Keep existing model name if available\n",
        "\n",
        "    rule = get_extraction_rule(brand, title)\n",
        "    return extract_model_name(title, regex_pattern=rule.get('regex'), word_count=rule.get('word_count', 3), capitalize=rule.get('capitalize', True))\n",
        "\n",
        "# Apply the cleaning function and the model extraction to the DataFrame\n",
        "filtered_df = clean_text(filtered_df, ['Title', 'Brand Name'])\n",
        "filtered_df.loc[:,'Model Name'] = filtered_df.apply(apply_model_extraction, axis=1)\n",
        "\n",
        "\n",
        "# List of columns to exclude from conversion\n",
        "exclude_columns = ['Image URL', 'product_url']\n",
        "\n",
        "# Get object columns excluding the specified ones\n",
        "object_columns = filtered_df.select_dtypes(include=['object']).columns\n",
        "columns_to_clean = [col for col in object_columns if col not in exclude_columns]\n",
        "\n",
        "# Converting all applicable object columns to lowercase and stripping whitespace\n",
        "for col in columns_to_clean:\n",
        "    # Ensure the column is of string type before applying string methods\n",
        "    filtered_df.loc[:, col] = filtered_df[col].astype(str).str.lower().str.strip()\n",
        "\n",
        "\n",
        "# Select relevant columns\n",
        "final_df = filtered_df[['date_column','site','category','Brand Name', 'Model Name', 'Title', 'Price', 'Operating System',\n",
        "                                 'RAM Memory Installed', 'Memory Storage Capacity', 'Screen Size',\n",
        "                                 'Resolution', 'Refresh Rate', 'CPU Speed', 'Connectivity technologies',\n",
        "                                 'CPU Model', 'Color', 'Wireless Provider',\n",
        "                                 'Cellular Technology', 'reviews', 'Rate', 'Discount','product_url','Image URL','ASIN','Batteries','Item model number']]\n",
        "\n",
        "final_df = final_df.copy()\n",
        "\n",
        "final_df.rename(columns={\n",
        "    'Brand Name':'brand', 'Model Name':'model_name', 'Title':'product_title',\n",
        "    'Price':'price_egp', 'Operating System':'os','RAM Memory Installed' : 'ram_gb',\n",
        "    'Memory Storage Capacity':'storage', 'Screen Size':'screen_size_in',\n",
        "    'Resolution':'resolution', 'Refresh Rate':'refresh_rate_hz', 'CPU Speed':'cpu_speed_ghz',\n",
        "    'Connectivity technologies':'connectivity_technology','CPU Model':'cpu_model',\n",
        "    'Color':'color','Wireless Provider':'wireless_carrier',\n",
        "    'Cellular Technology':'cellular_technology', 'reviews':'all_reviews',\n",
        "    'Rate':'rate', 'Discount':'discount','Image URL':'image_url','ASIN':'asin','Batteries':'batteries','Item model number':'model_number'\n",
        "},inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "def clean_col_val(df, col, remove_patterns=None, strip=True):\n",
        "    # Remove the provided patterns\n",
        "    if remove_patterns:\n",
        "        for pattern in remove_patterns:\n",
        "            df.loc[:,col] = df[col].str.replace(pattern, \"\", regex=True)\n",
        "\n",
        "    # Optionally strip whitespace\n",
        "    if strip:\n",
        "        df.loc[:,col] = df[col].str.strip()\n",
        "\n",
        "    return df\n",
        "\n",
        "final_df = clean_col_val(final_df, \"refresh_rate_hz\", remove_patterns=[\"hz|hertz|ghz\"])\n",
        "final_df = clean_col_val(final_df, \"discount\", remove_patterns=[\"-|%\"])\n",
        "\n",
        "\n",
        "def clean_price(price):\n",
        "    if pd.isna(price):\n",
        "        return price  # If the price is NaN return as is\n",
        "\n",
        "    # Define a list of unwanted characters and words to remove\n",
        "    unwanted_chars = [',', '₹', '$', '£', '€', '₣', '¥']\n",
        "    unwanted_words = ['egp', 'usd', 'inr', 'eur', 'gbp', 'aud', 'cny']\n",
        "\n",
        "    # Remove unwanted words and characters\n",
        "    original_price = price  # original price for checking\n",
        "    price = price.lower()\n",
        "    for word in unwanted_words:\n",
        "        price = re.sub(r'\\b' + re.escape(word) + r'\\b', '', price)\n",
        "    for char in unwanted_chars:\n",
        "        price = price.replace(char, '')\n",
        "\n",
        "    # Remove trailing '.00' or similar patterns for prices with currency codes\n",
        "    if any(word in original_price.lower() for word in unwanted_words):\n",
        "        # Remove trailing .00 or .xx\n",
        "        price = re.sub(r'(\\d+)(?:,\\d{3})*(?:\\.\\d{2})$', r'\\1', price)\n",
        "    else:\n",
        "        # For other prices, remove only commas\n",
        "        price = price.replace(',', '')\n",
        "\n",
        "    # Ensure only numeric characters are kept\n",
        "    price = re.sub(r'\\D', '', price)\n",
        "\n",
        "    # Remove any leading or trailing whitespace\n",
        "    price = price.strip()\n",
        "\n",
        "    return price\n",
        "\n",
        "# Apply the function to the 'price_egp' column\n",
        "final_df['price_egp'] = final_df['price_egp'].apply(clean_price)\n",
        "\n",
        "# Fill NaN values of network_type from product_title\n",
        "final_df[\"cellular_technology\"] = final_df[\"cellular_technology\"].fillna(final_df[\"product_title\"].str.extract('( 4g| 5g)', flags=re.IGNORECASE).squeeze())\n",
        "final_df[\"cellular_technology\"] = final_df[\"cellular_technology\"].fillna(final_df[\"model_name\"].str.extract('( 4g| 5g)', flags=re.IGNORECASE).squeeze())\n",
        "# Remove network_type from the mode_name\n",
        "final_df.loc[:,\"model_name\"] = final_df[\"model_name\"].str.replace('( 4g| 5g)',\"\",regex=True)\n",
        "\n",
        "def format_samsung_model(model):\n",
        "    model = model.lower()\n",
        "\n",
        "    # Extract special terms like FE, Ultra, Plus, 5G, 4G\n",
        "    special_terms = re.findall(r'\\b(fe|ultra|plus|5g|4g)\\b', model, re.IGNORECASE)\n",
        "    special_terms = list(dict.fromkeys(special_terms))  # Remove duplicates while maintaining order\n",
        "    special_terms_str = ' '.join(special_terms).upper()\n",
        "\n",
        "    # Remove unwanted words and characters\n",
        "    model = re.sub(r'\\b(samsung|galaxy)\\b', '', model).strip()\n",
        "    model = re.sub(r'\\b(5g|4g)\\b', '', model).strip()  # Remove 5G/4G here to prevent duplication\n",
        "\n",
        "    # Extract core model name\n",
        "    if 'note' in model:\n",
        "        match = re.search(r'note\\s*(\\d*)\\s*(\\w*)', model)\n",
        "        if match:\n",
        "            note_num, note_suffix = match.groups()\n",
        "            core_model = f\"Note {note_num} {note_suffix}\".strip().title()\n",
        "    else:\n",
        "        match = re.search(r'([a-z]+\\s*\\d+(?:\\s*[a-z]+)?)', model)\n",
        "        if match:\n",
        "            core_model = match.group(1).strip().title()\n",
        "        else:\n",
        "            core_model = model.strip().title()\n",
        "\n",
        "    # Remove special terms from core_model if they're already present\n",
        "    for term in special_terms:\n",
        "        core_model = re.sub(rf'\\b{term}\\b', '', core_model, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    # Combine core model and special terms\n",
        "    result = f'{core_model} {special_terms_str}'.strip()\n",
        "\n",
        "    return result.lower()\n",
        "\n",
        "def format_realme_model(model):\n",
        "    model = model.lower()\n",
        "\n",
        "    # Remove brand name 'realme'\n",
        "    model = re.sub(r'\\b(realme)\\b', '', model).strip()\n",
        "\n",
        "    # Replace '+' with 'plus'\n",
        "    model = re.sub(r'\\+', ' plus', model)\n",
        "\n",
        "    # Remove extra descriptors like \"dual-sim\" and \"dual\"\n",
        "    model = re.sub(r'\\bdual[-\\s]?sim\\b', '', model)\n",
        "    model = re.sub(r'\\bdual\\b', '', model)\n",
        "\n",
        "    # Remove any extra descriptive information in parentheses or after commas\n",
        "    model = re.sub(r'\\(.*?\\)', '', model).strip()\n",
        "    model = re.sub(r',.*$', '', model).strip()\n",
        "\n",
        "    # Handle hyphens between numbers and specs\n",
        "    model = re.sub(r'(\\d+)-(\\d+)', r'\\1 \\2', model)\n",
        "\n",
        "    # Extract the core model name, keeping '4G' or '5G' if present\n",
        "    model = re.sub(r'\\b4g\\b', '4g', model)\n",
        "    model = re.sub(r'\\b5g\\b', '5g', model)\n",
        "\n",
        "    return model.strip()\n",
        "\n",
        "def format_xiaomi_brand(model):\n",
        "    model = model.lower()\n",
        "\n",
        "    # Ensure 'xiaomi' is removed, but not 'redmi'\n",
        "    model = re.sub(r'\\b(xiaomi|mi|xioami)\\b', '', model).strip()\n",
        "\n",
        "    # Handle hyphens between numbers and specs  (e.g., 12-128 becomes 12 128)\n",
        "    model = re.sub(r'(\\d+)-(\\d+)', r'\\1 \\2', model)\n",
        "\n",
        "    # Remove extra descriptors like \"dual-sim\" and \"dual\"\n",
        "    model = re.sub(r'\\bdual[-\\s]?sim\\b', '', model)\n",
        "    model = re.sub(r'\\bdual\\b', '', model)\n",
        "\n",
        "    # Replace '+' with 'plus'\n",
        "    model = re.sub(r'\\+', ' plus', model)\n",
        "\n",
        "    # Remove colors (e.g., \"midnight black\", \"shiny gold\")\n",
        "    model = re.sub(r'\\b(?:midnight|black|blue|gold|silver|white|gray|green|red|orange|pink|purple|yellow)\\b', '', model)\n",
        "\n",
        "    # Remove memory specs (e.g., \"8gb\", \"128gb\")\n",
        "    model = re.sub(r'\\b\\d+gb\\b', '', model)\n",
        "\n",
        "    # Remove any extra descriptive information in parentheses, after commas, or after hyphens\n",
        "    model = re.sub(r'\\(.*?\\)', '', model).strip()\n",
        "    model = re.sub(r',.*$', '', model).strip()\n",
        "    model = re.sub(r'[\\-\\/].*$', '', model).strip()\n",
        "\n",
        "    # Handle hyphens between numbers and specs (e.g., 12-128 becomes 12 128)\n",
        "    model = re.sub(r'(\\d+)-(\\d+)', r'\\1 \\2', model)\n",
        "\n",
        "    # Remove extra years (e.g., '2022') or other numeric descriptors after core model\n",
        "    model = re.sub(r'\\b\\d{4}\\b', '', model).strip()\n",
        "\n",
        "    # Keep '4G', '5G', 'NE', etc. if present\n",
        "    model = re.sub(r'\\b4g\\b', '4g', model)\n",
        "    model = re.sub(r'\\b5g\\b', '5g', model)\n",
        "    model = re.sub(r'\\bne\\b', 'ne', model)\n",
        "\n",
        "    # Remove any remaining trailing/leading spaces and return cleaned model\n",
        "    return model.strip()\n",
        "\n",
        "\n",
        "def format_honor_model(model):\n",
        "    model = model.lower().strip()\n",
        "\n",
        "    # Remove the brand name 'honor'\n",
        "    model = re.sub(r'\\bhonor\\b', '', model).strip()\n",
        "\n",
        "    # Remove any extra descriptive information in parentheses, after commas, or after hyphens\n",
        "    model = re.sub(r'\\(.*?\\)', '', model).strip()\n",
        "    model = re.sub(r',.*$', '', model).strip()\n",
        "    model = re.sub(r'[\\-\\/].*$', '', model).strip()\n",
        "\n",
        "    # Replace '+' with 'plus'\n",
        "    model = re.sub(r'\\+', ' plus', model)\n",
        "\n",
        "    # Handle 'pro' and 'plus'\n",
        "    model = re.sub(r'\\bpro\\b', 'pro', model)\n",
        "    model = re.sub(r'\\bplus\\b', 'plus', model)\n",
        "\n",
        "    # Remove memory specs, colors, and other unnecessary information\n",
        "    model = re.sub(r'\\b\\d+gb\\b', '', model).strip()  # Remove '4gb', '8gb', etc.\n",
        "    model = re.sub(r'\\b\\d+\\+\\d+\\b', '', model).strip()  # Remove '12+512'\n",
        "    model = re.sub(r'\\b(?:titanium|silver|gold|black|blue|white|red|green|pink|grey|purple|yellow)\\b', '', model).strip()  # Remove colors\n",
        "\n",
        "    # Ensure 'plus' is included in the model name if present\n",
        "    model = re.sub(r'\\bplus\\b', 'plus', model)\n",
        "\n",
        "    # Final cleanup: remove extra spaces and return formatted model name\n",
        "    model = re.sub(r'\\s+', ' ', model)  # Replace multiple spaces with a single space\n",
        "\n",
        "    return model.strip()\n",
        "\n",
        "\n",
        "def format_apple_model(model):\n",
        "    \"\"\"\n",
        "    Clean and format the model name for Apple products, including core model number and important descriptors.\n",
        "    \"\"\"\n",
        "    model = model.lower().strip()\n",
        "\n",
        "    # Remove the brand name 'apple' and 'iphone'\n",
        "    model = re.sub(r'\\bapple\\b', '', model).strip()\n",
        "    model = re.sub(r'\\biphone\\b', '', model).strip()\n",
        "\n",
        "    # Replace hyphens with spaces\n",
        "    model = re.sub(r'-', ' ', model)\n",
        "\n",
        "    # Remove memory specs and additional descriptors\n",
        "    model = re.sub(r'\\b\\d+gb\\b', '', model)\n",
        "    model = re.sub(r'\\b(?:blue|starlight|facetime|gre|with|new)\\b', '', model)\n",
        "\n",
        "    # Remove extra descriptive information in parentheses or after hyphens\n",
        "    model = re.sub(r'\\(.*?\\)', '', model).strip()\n",
        "    model = re.sub(r',.*$', '', model).strip()\n",
        "\n",
        "    # Split by spaces to extract core model name and descriptors\n",
        "    parts = model.split()\n",
        "\n",
        "    # Extract the core model number or name and important descriptors ('pro', 'max')\n",
        "    core_model = []\n",
        "    for part in parts:\n",
        "        if re.match(r'^\\d+', part):  # Check if part starts with a number\n",
        "            core_model.append(part)\n",
        "        elif part in ['pro', 'max', 'mini', 'plus']:  # Important descriptors\n",
        "            core_model.append(part)\n",
        "\n",
        "    return ' '.join(core_model).strip()\n",
        "\n",
        "def format_oppo_model(model):\n",
        "    model = model.lower()\n",
        "\n",
        "    # Remove brand name 'oppo'\n",
        "    model = re.sub(r'\\b(oppo)\\b', '', model).strip()\n",
        "\n",
        "    # Remove any descriptive words\n",
        "    model = re.sub(r'(android|smartphone|dual sim|mobile|\\.\\.\\.|glowing|black|blue|uae|version)', '', model, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove numbers followed by 'gb' or 'ram'\n",
        "    model = re.sub(r'\\d+\\s*(gb|ram)', '', model, flags=re.IGNORECASE)\n",
        "\n",
        "    # Initialize core_model with cleaned model string\n",
        "    core_model = model.strip()\n",
        "\n",
        "    # Extract the core model name (Reno series or A series)\n",
        "    reno_match = re.search(r'(reno\\s*\\d+\\s*\\w*)', model, re.IGNORECASE)\n",
        "    a_series_match = re.search(r'(a\\d+\\s*\\w*)', model, re.IGNORECASE)\n",
        "\n",
        "    if reno_match:\n",
        "        core_model = reno_match.group(1)\n",
        "    elif a_series_match:\n",
        "        core_model = a_series_match.group(1)\n",
        "\n",
        "    # Check for and append 4G/5G if present\n",
        "    network_match = re.search(r'(4g|5g)', model, re.IGNORECASE)\n",
        "    if network_match:\n",
        "        core_model += ' ' + network_match.group(1).lower()\n",
        "\n",
        "    return core_model.strip()\n",
        "\n",
        "def format_nokia_model(model, product_title):\n",
        "    model = model.lower()\n",
        "\n",
        "    if model == 'nokia':\n",
        "        product_title = product_title.lower()\n",
        "\n",
        "        # Replace '+' with 'plus'\n",
        "        product_title = re.sub(r'\\+', ' plus', product_title)\n",
        "\n",
        "        # Remove 'nokia' and split the title into words\n",
        "        product_title = re.sub(r'\\bnokia\\b', '', product_title).strip()\n",
        "        words = product_title.split()\n",
        "\n",
        "        if len(words) == 0:\n",
        "            return ''  # Return empty if no words are left after removing 'nokia'\n",
        "\n",
        "        # Handle cases where we have fewer than 2 words remaining\n",
        "        if len(words) == 1:\n",
        "            return words[0]\n",
        "\n",
        "        # Extract the second word and the third if it's '4G' or '5G'\n",
        "        base_model = words[0] + ' ' + words[1]\n",
        "\n",
        "        if len(words) > 2 and words[2] in ['4g', '5g']:\n",
        "            base_model += ' ' + words[2]\n",
        "        return base_model.strip().lower()\n",
        "    else:\n",
        "        # Replace '+' with 'plus'\n",
        "        model = re.sub(r'\\+', ' plus', model)\n",
        "        # Remove 'nokia'\n",
        "        model = re.sub(r'\\bnokia\\b', '', model).strip()\n",
        "        return model.strip()\n",
        "\n",
        "\n",
        "def format_infinix_model(model):\n",
        "    model = model.lower()\n",
        "\n",
        "    # Replace '+' with 'plus'\n",
        "    model = re.sub(r'\\+', ' plus', model)\n",
        "\n",
        "    # Remove the brand name 'infinix' if present\n",
        "    model = re.sub(r'\\binfinix\\b', '', model).strip()\n",
        "\n",
        "    # Remove any extra descriptive information in parentheses or after commas\n",
        "    model = re.sub(r'\\(.*?\\)', '', model).strip()\n",
        "    model = re.sub(r',.*$', '', model).strip()\n",
        "\n",
        "    # Handle cases with just the core model name\n",
        "    model = model.strip().lower()\n",
        "\n",
        "    return model\n",
        "\n",
        "def format_redmi_model(model):\n",
        "    \"\"\"\n",
        "    Ensure that the model_name for the brand 'Redmi' includes 'Redmi' at the beginning and clean the brand name without removing 'Redmi'.\n",
        "    \"\"\"\n",
        "    model = model.lower()  # Normalize case\n",
        "\n",
        "    # Check if 'redmi' is not at the beginning, add it if necessary\n",
        "    if not model.startswith('redmi'):\n",
        "        model = 'redmi ' + model\n",
        "\n",
        "    # Replace '+' with 'plus'\n",
        "    model = re.sub(r'\\+', ' plus', model)\n",
        "\n",
        "    # Remove any extra descriptive information in parentheses or after commas\n",
        "    model = re.sub(r'\\(.*?\\)', '', model).strip()\n",
        "    model = re.sub(r',.*$', '', model).strip()\n",
        "    model = re.sub(r'[\\-\\/].*$', '', model).strip()\n",
        "\n",
        "    # Handle hyphens between numbers and specs\n",
        "    model = re.sub(r'(\\d+)-(\\d+)', r'\\1 \\2', model)\n",
        "\n",
        "    # Clean up unnecessary extra text but keep 'redmi'\n",
        "    model = re.sub(r'\\bredmi\\b', 'redmi', model).strip()\n",
        "\n",
        "    # Extract the core model name, keeping '4G' or '5G' if present\n",
        "    model = re.sub(r'\\b4g\\b', '4g', model)\n",
        "    model = re.sub(r'\\b5g\\b', '5g', model)\n",
        "\n",
        "    return model.strip()\n",
        "\n",
        "\n",
        "# Function to apply based on brand\n",
        "def process_model(row):\n",
        "    brand = row['brand'].lower()\n",
        "    model_name = row['model_name']\n",
        "    product_title = row['product_title']\n",
        "\n",
        "    if brand == 'samsung':\n",
        "        return format_samsung_model(model_name)\n",
        "    elif brand == 'xiaomi':\n",
        "        return format_xiaomi_brand(model_name)\n",
        "    elif brand == 'apple':\n",
        "        return format_apple_model(model_name)\n",
        "    elif brand == 'honor':\n",
        "        return format_honor_model(model_name)\n",
        "    elif brand == 'oppo':\n",
        "        return format_oppo_model(model_name)\n",
        "    elif brand == 'nokia':\n",
        "        return format_nokia_model(model_name,product_title)\n",
        "    elif brand == 'infinix':\n",
        "        return format_infinix_model(model_name)\n",
        "    elif brand == 'realme':\n",
        "        return format_realme_model(model_name)\n",
        "    elif brand == 'redmi':\n",
        "        return format_redmi_model(model_name)\n",
        "    else:\n",
        "        return model_name\n",
        "\n",
        "\n",
        "# Apply function to the DataFrame\n",
        "final_df['model_name'] = final_df.apply(process_model, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "final_df = final_df.dropna(subset=[\"ram_gb\", \"storage\", \"screen_size_in\", \"resolution\", \"cpu_speed_ghz\", \"cpu_model\"], how='all')\n",
        "\n",
        "# Convert `date_column` to datetime\n",
        "final_df['date_column'] = pd.to_datetime(final_df['date_column'], errors='coerce')\n",
        "\n",
        "# Convert `price_usd` to numeric\n",
        "final_df['price_egp'] = pd.to_numeric(final_df['price_egp'], errors='coerce')\n",
        "\n",
        "# final_df.to_csv(f'/content/drive/My Drive/Egypt-Data/Amazon_EG_11_pages-Cleaned.csv', index=False)\n",
        "final_df['currency']='egp'\n"
      ],
      "metadata": {
        "id": "MXoRrbmUR9bS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract and remove network type\n",
        "def extract_network(text):\n",
        "    # Check if text is a string, otherwise return it unchanged\n",
        "    if isinstance(text, str):\n",
        "        match = re.search(r'(5g|4g)', text)\n",
        "        if match:\n",
        "            network = match.group(0)\n",
        "            # Remove 5G/4G from the original text\n",
        "            text = re.sub(r'\\s?(5g|4g)\\s?', '', text).strip()\n",
        "            return text, network\n",
        "    return text, None\n",
        "\n",
        "# Initialize 'network' column as None\n",
        "final_df['network'] = None\n",
        "\n",
        "# Apply the extraction logic to each column and update 'network' column\n",
        "for col in ['model_name', 'product_title', 'cellular_technology']:\n",
        "    # Apply the function to extract network and updated column text\n",
        "    final_df[col], extracted_networks = zip(*final_df[col].apply(extract_network))\n",
        "\n",
        "    # Fill the 'network' column where it is None with the extracted network\n",
        "    final_df['network'] = final_df['network'].combine_first(pd.Series(extracted_networks))"
      ],
      "metadata": {
        "id": "WMHGOt4zHfhX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Fill any remaining None values in 'network' using 'cellular_technology'\n",
        "final_df['network'] = final_df['network'].fillna(final_df['cellular_technology'])"
      ],
      "metadata": {
        "id": "H1yAdcQHIib_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_review_url( asin):\n",
        "        if asin:\n",
        "            return f\"https://www.amazon.com/product-reviews/{asin}\"\n",
        "        return None\n",
        "final_df['review_url']=final_df['asin'].apply(get_review_url)"
      ],
      "metadata": {
        "id": "FZM2561tJK53"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.rename(columns={'product_url': 'link','rate':'rating','Brand Name':'brand',\n",
        "          'Model Name':'model_name','price_egp':'price','date_column':'date','asin':'ASIN'}, inplace=True)\n",
        "final_df=final_df[['link', 'site', 'category', 'rating', 'image_url', 'brand','discount',\n",
        "       'model_name', 'product_title', 'price', 'currency', 'os', 'ram_gb',\n",
        "       'storage', 'screen_size_in', 'resolution', 'refresh_rate_hz',\n",
        "       'cpu_speed_ghz', 'connectivity_technology', 'cpu_model', 'color',\n",
        "       'wireless_carrier', 'date', 'all_reviews', 'ASIN', 'review_url',\n",
        "       'network']]"
      ],
      "metadata": {
        "id": "22Z9SedQHDIQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract numbers from 'storage', 'ram_gb', and 'screen_size_in' columns\n",
        "final_df['storage'] = final_df['storage'].str.extract('(\\d+)', expand=False).astype(float)\n",
        "final_df['ram_gb'] = final_df['ram_gb'].str.extract('(\\d+)', expand=False).astype(float)\n",
        "final_df['screen_size_in'] = final_df['screen_size_in'].str.extract('(\\d+(\\.\\d+)?)', expand=False)[0].astype(float)\n",
        "final_df['refresh_rate_hz'] = final_df['refresh_rate_hz'].str.extract('(\\d+)', expand=False).astype(float)\n",
        "final_df['cpu_speed_ghz'] = final_df['cpu_speed_ghz'].str.extract('(\\d+)', expand=False).astype(float)"
      ],
      "metadata": {
        "id": "ednT7u-TLnhZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0ed2a1-e852-4a2d-c881-6570dbd8be53"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-7eb98e252f81>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_df['storage'] = final_df['storage'].str.extract('(\\d+)', expand=False).astype(float)\n",
            "<ipython-input-21-7eb98e252f81>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_df['ram_gb'] = final_df['ram_gb'].str.extract('(\\d+)', expand=False).astype(float)\n",
            "<ipython-input-21-7eb98e252f81>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_df['screen_size_in'] = final_df['screen_size_in'].str.extract('(\\d+(\\.\\d+)?)', expand=False)[0].astype(float)\n",
            "<ipython-input-21-7eb98e252f81>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_df['refresh_rate_hz'] = final_df['refresh_rate_hz'].str.extract('(\\d+)', expand=False).astype(float)\n",
            "<ipython-input-21-7eb98e252f81>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_df['cpu_speed_ghz'] = final_df['cpu_speed_ghz'].str.extract('(\\d+)', expand=False).astype(float)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df['ram_gb'] = pd.to_numeric(final_df['ram_gb'], errors='coerce')\n",
        "final_df['storage'] = pd.to_numeric(final_df['storage'], errors='coerce')\n",
        "final_df['screen_size_in'] = pd.to_numeric(final_df['screen_size_in'], errors='coerce')\n",
        "final_df['rating'] = pd.to_numeric(final_df['rating'], errors='coerce')"
      ],
      "metadata": {
        "id": "Ao77ZHDWK3pw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "274860c4-fba5-401c-b10d-77cfe5f8aa1d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-2890f2d31fe3>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_df['ram_gb'] = pd.to_numeric(final_df['ram_gb'], errors='coerce')\n",
            "<ipython-input-22-2890f2d31fe3>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_df['storage'] = pd.to_numeric(final_df['storage'], errors='coerce')\n",
            "<ipython-input-22-2890f2d31fe3>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_df['screen_size_in'] = pd.to_numeric(final_df['screen_size_in'], errors='coerce')\n",
            "<ipython-input-22-2890f2d31fe3>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_df['rating'] = pd.to_numeric(final_df['rating'], errors='coerce')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'none' with 0 and convert the column to float\n",
        "final_df['discount'] = final_df['discount'].replace('none', 1).astype(np.int64)\n",
        "final_df['discount'] = pd.to_numeric(final_df['discount'], errors='coerce')"
      ],
      "metadata": {
        "id": "SjKk5_yeQiwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c90cee-f441-4d73-dfe5-093b79bcb829"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-06913fd254ee>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_df['discount'] = final_df['discount'].replace('none', 1).astype(np.int64)\n",
            "<ipython-input-23-06913fd254ee>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_df['discount'] = pd.to_numeric(final_df['discount'], errors='coerce')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "import os\n",
        "\n",
        "# Azure Storage connection string\n",
        "connect_str = \"DefaultEndpointsProtocol=https;AccountName=ynwa;AccountKey=aTLtGZuymmaAirlsL1/39g3dDjaQvBFu3lHzQaUrY0o6LDdGduAW8Wbk8qI7fuilCKS8chuiklq7+AStBN3UdA==;EndpointSuffix=core.windows.net\"\n",
        "\n",
        "# Initialize BlobServiceClient\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "\n",
        "# Name of the container and blob\n",
        "container_name = \"huayra\"  # Replace with your container name\n",
        "folder_name = \"transformed\"\n",
        "\n",
        "# Name of the blob (file) with today's date\n",
        "blob_name = f\"{folder_name}/amazon_eg_tr{today_date}.csv\"  # Example: scraped_data_2024-10-07.csv\n",
        "\n",
        "# Create a container if it doesn't exist\n",
        "container_client = blob_service_client.get_container_client(container_name)\n",
        "try:\n",
        "    container_client.create_container()\n",
        "except Exception as e:\n",
        "    print(f\"Container already exists: {e}\")\n",
        "\n",
        "\n",
        "# Save the CSV locally (you can skip this if you already have the file path)\n",
        "csv_file_path = f\"scraped_data_{today_date}.csv\"\n",
        "final_df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Upload the CSV file to Blob Storage with today's date in the name\n",
        "with open(csv_file_path, \"rb\") as data:\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "    blob_client.upload_blob(data, overwrite=True)\n",
        "    print(f\"CSV file {blob_name} uploaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr7Es2Wisx6G",
        "outputId": "0a4f6c2e-420a-40ad-af61-15ac031a8647"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Container already exists: The specified container already exists.\n",
            "RequestId:89587ec3-301e-0063-37b2-206462000000\n",
            "Time:2024-10-17T16:37:25.9973348Z\n",
            "ErrorCode:ContainerAlreadyExists\n",
            "Content: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>ContainerAlreadyExists</Code><Message>The specified container already exists.\n",
            "RequestId:89587ec3-301e-0063-37b2-206462000000\n",
            "Time:2024-10-17T16:37:25.9973348Z</Message></Error>\n",
            "CSV file transformed/amazon_eg_tr2024-10-17.csv uploaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=final_df.copy()"
      ],
      "metadata": {
        "id": "mBz2QF0DMQIJ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import io\n",
        "import json\n",
        "import hashlib\n",
        "\n",
        "def generate_unique_product_id(row):\n",
        "    unique_string = f\"{row['brand']}_{row['model_name']}_{row['storage']}_{row['ram_gb']}_{row['network']}\"\n",
        "    return hashlib.md5(unique_string.encode()).hexdigest()\n",
        "\n",
        "def assign_partition(row, id_column, num_partitions=7):\n",
        "    return int(hashlib.md5(str(row[id_column]).encode()).hexdigest(), 16) % num_partitions\n",
        "\n",
        "def read_parquet_from_blob(blob_service_client, container_name, folder_name, file_name):\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=f\"{folder_name}/{file_name}\")\n",
        "    stream = blob_client.download_blob()\n",
        "    bytes_data = stream.readall()\n",
        "    return pd.read_parquet(io.BytesIO(bytes_data))\n",
        "\n",
        "def read_partitioned_parquet_from_blob(blob_service_client, container_name, folder_name, file_name):\n",
        "    all_data = []\n",
        "    for i in range(7):  # Assuming 7 partitions\n",
        "        try:\n",
        "            blob_client = blob_service_client.get_blob_client(container=container_name, blob=f\"{folder_name}/{file_name}/{i}_{file_name}\")\n",
        "            stream = blob_client.download_blob()\n",
        "            bytes_data = stream.readall()\n",
        "            partition_data = pd.read_parquet(io.BytesIO(bytes_data))\n",
        "            all_data.append(partition_data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading partition {i}: {e}\")\n",
        "    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
        "\n",
        "def write_to_parquet(data, blob_service_client, container_name, folder_name, file_name, partition_strategy=None):\n",
        "    if partition_strategy is None:\n",
        "        table = pa.Table.from_pandas(data)\n",
        "        buf = io.BytesIO()\n",
        "        pq.write_table(table, buf)\n",
        "        buf.seek(0)\n",
        "\n",
        "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=f\"{folder_name}/{file_name}\")\n",
        "        blob_client.upload_blob(buf.getvalue(), overwrite=True)\n",
        "    else:\n",
        "        data['partition'] = data.apply(partition_strategy, axis=1)\n",
        "        for partition, partition_data in data.groupby('partition'):\n",
        "            partition_data = partition_data.drop('partition', axis=1)\n",
        "            table = pa.Table.from_pandas(partition_data)\n",
        "            buf = io.BytesIO()\n",
        "            pq.write_table(table, buf)\n",
        "            buf.seek(0)\n",
        "\n",
        "            blob_client = blob_service_client.get_blob_client(container=container_name,\n",
        "                                                              blob=f\"{folder_name}/{file_name}/{partition}_{file_name}\")\n",
        "            blob_client.upload_blob(buf.getvalue(), overwrite=True)\n",
        "\n",
        "def update_fact_table(existing_fact, new_data):\n",
        "    new_fact = process_fact_table(new_data)\n",
        "    updated_fact = pd.concat([existing_fact, new_fact]).drop_duplicates(subset=['product_id','site_id','date','url'], keep='last')\n",
        "    return updated_fact\n",
        "\n",
        "def update_dim_device_specification(existing_dim, new_data):\n",
        "    new_dim = process_dim_device_specification(new_data)\n",
        "    updated_dim = pd.concat([existing_dim, new_dim]).drop_duplicates(subset=['product_id'], keep='last')\n",
        "    return updated_dim\n",
        "\n",
        "def update_product_mapping(existing_mapping, new_data):\n",
        "    new_mapping = process_product_mapping(new_data)\n",
        "    updated_mapping = pd.concat([existing_mapping, new_mapping]).drop_duplicates(subset=['product_id'], keep='last')\n",
        "    return updated_mapping\n",
        "\n",
        "def update_dim_site(existing_site, new_data):\n",
        "    new_site = process_dim_site(new_data)\n",
        "    updated_site = pd.concat([existing_site, new_site]).drop_duplicates(subset=['site_name'], keep='last')\n",
        "    updated_site['site_id'] = range(len(updated_site))  # Reassign site_ids\n",
        "    return updated_site\n",
        "\n",
        "def update_dim_review(existing_review, new_data):\n",
        "    new_review = process_dim_review(new_data)\n",
        "    updated_review = pd.concat([existing_review, new_review]).drop_duplicates(subset=['review_id'], keep='last')\n",
        "    return updated_review\n",
        "\n",
        "def update_dim_date(existing_date, new_data):\n",
        "    new_date = process_dim_date(new_data)\n",
        "    updated_date = pd.concat([existing_date, new_date]).drop_duplicates(subset=['date_ID'], keep='last')\n",
        "    return updated_date\n",
        "\n",
        "def process_fact_table(df):\n",
        "    fact_table = df[['product_id', 'site', 'date', 'price', 'currency', 'rating','link']].copy()\n",
        "    fact_table['site_id'] = 2\n",
        "    fact_table['discount'] = df['discount']\n",
        "    fact_table['d_p'] = df['price']/df['discount']\n",
        "    fact_table['price_without_discount'] = fact_table['price'] +fact_table['d_p']\n",
        "    fact_table['rating_avg'] = fact_table['rating']\n",
        "    fact_table['url']=fact_table['link']\n",
        "    return fact_table[['product_id', 'url','site_id', 'date',  'price', 'currency', 'discount', 'price_without_discount', 'rating_avg']]\n",
        "\n",
        "def process_dim_device_specification(df):\n",
        "    return df[['product_id', 'product_title', 'image_url', 'os', 'screen_size_in', 'resolution', 'refresh_rate_hz', 'cpu_speed_ghz', 'cpu_model', 'color', 'wireless_carrier', 'category']].copy()\n",
        "\n",
        "def process_product_mapping(df):\n",
        "    return df[['product_id', 'model_name', 'brand', 'network', 'ram_gb', 'storage']].copy()\n",
        "\n",
        "def process_dim_site(df):\n",
        "    site_df = pd.DataFrame({'site_name': df['site'].unique()})\n",
        "    site_df['site_id'] = range(len(site_df))\n",
        "    return site_df\n",
        "\n",
        "def process_dim_review(df):\n",
        "    reviews_list = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Ensure that 'all_reviews' exists and is a list\n",
        "        if isinstance(row['all_reviews'], list):\n",
        "            for review in row['all_reviews']:\n",
        "                # Ensure that 'review' is a dictionary\n",
        "                if isinstance(review, dict):\n",
        "                    # Use .get() to safely access dictionary keys\n",
        "                    review_id = hashlib.md5(f\"{row['product_id']}_{review.get('Date', '')}_{row['site']}\".encode()).hexdigest()\n",
        "                    review_text = review.get('Review Body', None)\n",
        "                    review_rating = review.get('Rating', None).split()[0] if review.get('Rating') else None\n",
        "                    review_date = pd.to_datetime(review.get('Date', '').split('on ')[-1], format='%d %B %Y', errors='coerce')\n",
        "\n",
        "                    reviews_list.append({\n",
        "                        'review_id': review_id,  # Unique review ID\n",
        "                        'product_id': row['product_id'],\n",
        "                        'product_reviews_url': row['review_url'],\n",
        "                        'review_text': review_text,\n",
        "                        'review_rating': review_rating,\n",
        "                        'review_date': review_date,\n",
        "                        'site': row['site']  # Ensure site column is properly handled\n",
        "                    })\n",
        "\n",
        "\n",
        "\n",
        "    # Convert the list of review dictionaries into a DataFrame\n",
        "    review_df = pd.DataFrame(reviews_list)\n",
        "\n",
        "    # # Convert the 'site' column to string explicitly to avoid ArrowInvalid issues\n",
        "    # review_df['site'] = review_df['site'].astype(str)\n",
        "\n",
        "    return review_df\n",
        "\n",
        "def process_dim_date(df):\n",
        "    dates = pd.to_datetime(df['date'].unique())\n",
        "    date_df = pd.DataFrame({\n",
        "        'date_ID': dates,\n",
        "        'day': dates.day,\n",
        "        'month': dates.month,\n",
        "        'year': dates.year\n",
        "    })\n",
        "    return date_df\n",
        "\n",
        "def main():\n",
        "    # Azure Storage connection string\n",
        "    connect_str = \"DefaultEndpointsProtocol=https;AccountName=ynwa;AccountKey=aTLtGZuymmaAirlsL1/39g3dDjaQvBFu3lHzQaUrY0o6LDdGduAW8Wbk8qI7fuilCKS8chuiklq7+AStBN3UdA==;EndpointSuffix=core.windows.net\"\n",
        "\n",
        "    # Initialize BlobServiceClient\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "\n",
        "    # Name of the container and folder\n",
        "    container_name = \"huayra\"\n",
        "    folder_name = \"model\"\n",
        "\n",
        "    # Read new data (replace this with your actual data loading method)\n",
        "    new_df = df.copy()\n",
        "\n",
        "    # Read existing product ID mapping\n",
        "    existing_product_id_mapping = read_parquet_from_blob(blob_service_client, container_name, folder_name, \"product_id_mapping.parquet\")\n",
        "\n",
        "    # Generate unique product IDs for new data, using existing IDs where possible\n",
        "    new_df['temp_id'] = new_df.apply(generate_unique_product_id, axis=1)\n",
        "    new_df = pd.merge(new_df, existing_product_id_mapping[['brand', 'model_name', 'storage', 'ram_gb', 'network', 'product_id']],\n",
        "                      on=['brand', 'model_name', 'storage', 'ram_gb', 'network'], how='left')\n",
        "    new_df['product_id'] = new_df['product_id'].fillna(new_df['temp_id'])\n",
        "    new_df = new_df.drop('temp_id', axis=1)\n",
        "\n",
        "    # Update product ID mapping\n",
        "    updated_product_id_mapping = pd.concat([existing_product_id_mapping, new_df[['brand', 'model_name', 'storage', 'ram_gb', 'network', 'product_id']]])\n",
        "    updated_product_id_mapping = updated_product_id_mapping.drop_duplicates(subset=['brand', 'model_name', 'storage', 'ram_gb', 'network'], keep='last')\n",
        "\n",
        "    # Read existing data from Azure Blob Storage\n",
        "    existing_fact = read_partitioned_parquet_from_blob(blob_service_client, container_name, folder_name, \"fact_table.parquet\")\n",
        "    existing_dim_device = read_parquet_from_blob(blob_service_client, container_name, folder_name, \"dim_device_specification.parquet\")\n",
        "    existing_product_mapping = read_parquet_from_blob(blob_service_client, container_name, folder_name, \"product_mapping.parquet\")\n",
        "    existing_dim_site = read_parquet_from_blob(blob_service_client, container_name, folder_name, \"dim_site.parquet\")\n",
        "    existing_dim_review = read_partitioned_parquet_from_blob(blob_service_client, container_name, folder_name, \"dim_review.parquet\")\n",
        "    existing_dim_date = read_parquet_from_blob(blob_service_client, container_name, folder_name, \"dim_date.parquet\")\n",
        "\n",
        "    # Update tables\n",
        "    updated_fact = update_fact_table(existing_fact, new_df)\n",
        "    updated_dim_device = update_dim_device_specification(existing_dim_device, new_df)\n",
        "    updated_product_mapping = update_product_mapping(existing_product_mapping, new_df)\n",
        "    updated_dim_site = update_dim_site(existing_dim_site, new_df)\n",
        "    updated_dim_review = update_dim_review(existing_dim_review, new_df)\n",
        "    updated_dim_date = update_dim_date(existing_dim_date, new_df)\n",
        "\n",
        "    # Define partition strategies\n",
        "    fact_partition_strategy = lambda row: assign_partition(row, 'product_id')\n",
        "    review_partition_strategy = lambda row: assign_partition(row, 'review_id')\n",
        "\n",
        "    # Write updated tables back to Azure Blob Storage\n",
        "    write_to_parquet(updated_fact, blob_service_client, container_name, folder_name, \"fact_table.parquet\", partition_strategy=fact_partition_strategy)\n",
        "    write_to_parquet(updated_dim_device, blob_service_client, container_name, folder_name, \"dim_device_specification.parquet\")\n",
        "    write_to_parquet(updated_product_mapping, blob_service_client, container_name, folder_name, \"product_mapping.parquet\")\n",
        "    write_to_parquet(updated_dim_site, blob_service_client, container_name, folder_name, \"dim_site.parquet\")\n",
        "    write_to_parquet(updated_dim_review, blob_service_client, container_name, folder_name, \"dim_review.parquet\", partition_strategy=review_partition_strategy)\n",
        "    write_to_parquet(updated_dim_date, blob_service_client, container_name, folder_name, \"dim_date.parquet\")\n",
        "    write_to_parquet(updated_product_id_mapping, blob_service_client, container_name, folder_name, \"product_id_mapping.parquet\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "XE04mklyyoL9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ZzNrq0gDlhJ"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}