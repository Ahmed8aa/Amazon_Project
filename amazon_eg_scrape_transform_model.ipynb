{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhyGhmuJ0uNGQUCqA65fKX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmed8aa/az_sc_tr/blob/main/amazon_sc_tr_egy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43XinfOysjpF",
        "outputId": "c0dedb09-07a3-451a-97f4-415e6c8f8c24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.25.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.27.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Ign:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Requirement already satisfied: webdriver_manager in /usr/local/lib/python3.10/dist-packages (4.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (2.32.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (1.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (24.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2024.8.30)\n",
            "Requirement already satisfied: azure-storage-blob in /usr/local/lib/python3.10/dist-packages (12.23.1)\n",
            "Requirement already satisfied: azure-core>=1.30.0 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (1.31.0)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (43.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (4.12.2)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (0.7.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.30.0->azure-storage-blob) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.30.0->azure-storage-blob) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium\n",
        "!apt-get update\n",
        "# !apt install chromium-chromedriver\n",
        "# !cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install webdriver_manager\n",
        "!pip install azure-storage-blob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "7NauHIjIo9PX"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "import io\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementNotInteractableException, ElementClickInterceptedException\n",
        "#from google.colab import files\n",
        "\n",
        "class AmazonScraper:\n",
        "    def __init__(self, search_term, num_pages, country=\"Egypt\"):\n",
        "        self.search_term = search_term\n",
        "        self.num_pages = num_pages\n",
        "        self.country = country\n",
        "        self.driver = self.web_driver()\n",
        "        self.product_data = []\n",
        "\n",
        "    def web_driver(self):\n",
        "        options = webdriver.ChromeOptions()\n",
        "        options.add_argument(\"--verbose\")\n",
        "        options.add_argument('--no-sandbox')\n",
        "        options.add_argument('--headless')\n",
        "        options.add_argument('--disable-gpu')\n",
        "        options.add_argument(\"--window-size=1920,1200\")\n",
        "        options.add_argument('--disable-dev-shm-usage')\n",
        "        driver = webdriver.Chrome(options=options)\n",
        "        return driver\n",
        "\n",
        "    def access_amazon(self):\n",
        "        print(\"Accessing Amazon...\")\n",
        "        max_attempts = 3\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                self.driver.get('https://www.amazon.eg/-/en')\n",
        "                self.handle_captcha()\n",
        "                return\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
        "                if attempt < max_attempts - 1:\n",
        "                    print(\"Retrying...\")\n",
        "                    time.sleep(random.uniform(2, 5))\n",
        "                else:\n",
        "                    print(\"Max attempts reached. Unable to access Amazon.\")\n",
        "                    raise\n",
        "\n",
        "    def handle_captcha(self):\n",
        "        try:\n",
        "            captcha_input = WebDriverWait(self.driver, 5).until(\n",
        "                EC.presence_of_element_located((By.ID, 'captchacharacters'))\n",
        "            )\n",
        "            print(\"CAPTCHA detected. Taking screenshot...\")\n",
        "            self.take_captcha_screenshot()\n",
        "            time.sleep(5)\n",
        "            captcha_code = input(\"Enter CAPTCHA code (or press Enter if no CAPTCHA): \")\n",
        "            if captcha_code:\n",
        "                captcha_input.send_keys(captcha_code)\n",
        "                continue_button = self.driver.find_element(By.CSS_SELECTOR, 'button.a-button-text')\n",
        "                continue_button.click()\n",
        "            time.sleep(3)  # Wait for page to load after CAPTCHA\n",
        "        except TimeoutException:\n",
        "            print(\"No CAPTCHA detected.\")\n",
        "\n",
        "    def take_captcha_screenshot(self):\n",
        "        screenshot = self.driver.get_screenshot_as_png()\n",
        "        image = Image.open(io.BytesIO(screenshot))\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    def change_amazon_location(self):\n",
        "        print(f\"Changing location to {self.country}\")\n",
        "        max_attempts = 3\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                self.driver.get('https://www.amazon.com/s?k=mobile+phones&crid=26WIEUO05NS2V&sprefix=mobile%2Caps%2C411&ref=nb_sb_ss_pltr-xclick_4_6')\n",
        "                time.sleep(5)\n",
        "                location_link = WebDriverWait(self.driver, 10).until(\n",
        "                    EC.element_to_be_clickable((By.ID, \"nav-global-location-popover-link\"))\n",
        "                )\n",
        "                location_link.click()\n",
        "                country_input = WebDriverWait(self.driver, 10).until(\n",
        "                    EC.element_to_be_clickable((By.ID, \"GLUXCountryListDropdown\"))\n",
        "                )\n",
        "                country_input.click()\n",
        "                country_option = WebDriverWait(self.driver, 10).until(\n",
        "                    EC.element_to_be_clickable((By.XPATH, f\"//a[contains(text(), '{self.country}')]\"))\n",
        "                )\n",
        "                country_option.click()\n",
        "                done_button = WebDriverWait(self.driver, 10).until(\n",
        "                    EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Done')]\"))\n",
        "                )\n",
        "                done_button.click()\n",
        "                WebDriverWait(self.driver, 10).until(\n",
        "                    EC.text_to_be_present_in_element((By.ID, \"glow-ingress-line2\"), self.country)\n",
        "                )\n",
        "                print(f\"Successfully changed location to {self.country}\")\n",
        "                return\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt + 1} to change location failed: {str(e)}\")\n",
        "                if attempt < max_attempts - 1:\n",
        "                    print(\"Retrying...\")\n",
        "                    time.sleep(random.uniform(2, 5))\n",
        "                else:\n",
        "                    print(f\"Failed to change location to {self.country} after {max_attempts} attempts.\")\n",
        "\n",
        "    def scroll_with_pauses(self):\n",
        "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        while True:\n",
        "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "            time.sleep(random.uniform(1, 2))\n",
        "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
        "            if new_height == last_height:\n",
        "                break\n",
        "            last_height = new_height\n",
        "\n",
        "    def extract_product_links_and_prices(self):\n",
        "        print(\"Extracting product links and prices...\")\n",
        "        self.driver.get(f\"https://www.amazon.eg/-/en/s?k={self.search_term.replace(' ', '+')}\")\n",
        "        for page in tqdm(range(self.num_pages), desc=\"Pages\"):\n",
        "            WebDriverWait(self.driver, 10).until(\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.s-result-item\"))\n",
        "            )\n",
        "            self.scroll_with_pauses()\n",
        "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "            products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
        "\n",
        "            for product in products:\n",
        "                try:\n",
        "                    link_tag = product.find('a', class_='a-link-normal')\n",
        "                    link = 'https://www.amazon.eg/-/en' + link_tag['href'] if link_tag else None\n",
        "\n",
        "                    price_span = product.select_one('span.a-price-whole')\n",
        "                    if not price_span:\n",
        "                        price_span = product.select_one('div[data-cy=\"secondary-offer-recipe\"] span.a-color-base')\n",
        "                    price = price_span.get_text(strip=True) if price_span else \"No Price\"\n",
        "                    list_price_span = product.select_one('span.a-price.a-text-price span.a-offscreen')\n",
        "                    if not list_price_span:\n",
        "                        list_price_span = product.select_one('div[data-cy=\"secondary-offer-recipe\"] span.a-color-base')\n",
        "\n",
        "                    # Extract the text or set a default if not found\n",
        "                    list_price = list_price_span.get_text(strip=True) if list_price_span else \"No List Price\"\n",
        "                    rating_span = product.select_one('span.a-icon-alt')\n",
        "                    rating = rating_span.get_text(strip=True) if rating_span else \"No Rating\"\n",
        "\n",
        "\n",
        "                    img_tag = product.find('img', class_='s-image')\n",
        "                    image_url = img_tag['src'] if img_tag else \"No Image\"\n",
        "\n",
        "                    if link:\n",
        "                        self.product_data.append({\"link\": link, \"price\": price,'price_before_discount':list_price,'image_url':image_url,'rating':rating})\n",
        "                        # print(f'Link: {link}')\n",
        "                        # print(f'Price: {price}')\n",
        "                        # print('-' * 80)\n",
        "                except Exception as e:\n",
        "                       continue\n",
        "                    # print(f\"Error processing product: {str(e)}\")\n",
        "\n",
        "            print(f\"Extracted {len(self.product_data)} products so far...\")\n",
        "\n",
        "            try:\n",
        "                next_button = WebDriverWait(self.driver, 10).until(\n",
        "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.s-pagination-next\"))\n",
        "                )\n",
        "                self.driver.execute_script(\"arguments[0].click();\", next_button)\n",
        "                time.sleep(random.uniform(3, 5))\n",
        "            except Exception as e:\n",
        "                print(f\"Reached the last page or encountered an error: {str(e)}. Stopping at page {page + 1}\")\n",
        "                break\n",
        "\n",
        "    def extract_product_details(self, url):\n",
        "        max_attempts = 3\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                self.driver.get(url)\n",
        "                WebDriverWait(self.driver, 10).until(\n",
        "                    EC.presence_of_element_located((By.ID, 'productTitle'))\n",
        "                )\n",
        "                time.sleep(random.uniform(2, 4))\n",
        "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "\n",
        "               # Extract product title\n",
        "                product_title_element = soup.find('span', {'id': 'productTitle'})\n",
        "                product_title = product_title_element.get_text(strip=True) if product_title_element else 'No title found'\n",
        "\n",
        "                # Initialize details dictionary\n",
        "                details = {\n",
        "                    \"Product Title\": product_title,\n",
        "                    \"Category\": \"Mobile Phones\",\n",
        "                    \"Site\": \"Amazon\"\n",
        "                }\n",
        "\n",
        "                # Extract product details from the table\n",
        "                table = soup.find('table', class_='a-normal a-spacing-micro')\n",
        "                if table:\n",
        "                    rows = table.find_all('tr')\n",
        "                    for row in rows:\n",
        "                        key_element = row.find('td', class_='a-span3')\n",
        "                        value_element = row.find('td', class_='a-span9')\n",
        "\n",
        "                        if key_element and value_element:\n",
        "                            key = key_element.get_text(strip=True)\n",
        "                            value = value_element.get_text(strip=True)\n",
        "                            details[key] = value\n",
        "                else:\n",
        "                    rows = soup.select('#productDetails_detailBullets_sections1 tr')\n",
        "                    for row in rows:\n",
        "                        key_element = row.find('th')\n",
        "                        value_element = row.find('td')\n",
        "\n",
        "                        if key_element and value_element:\n",
        "                            key = key_element.get_text(strip=True)\n",
        "                            value = value_element.get_text(strip=True)\n",
        "                            details[key] = value\n",
        "\n",
        "                                # Now extract reviews\n",
        "                all_reviews = []\n",
        "                if \"No customer reviews\" in soup:\n",
        "                    print(f\"No customer reviews found for URL {url}\")\n",
        "                    reviews_info = {\n",
        "\n",
        "                        \"All Reviews\": []\n",
        "                    }\n",
        "                else:\n",
        "                    reviews = soup.find_all('div', {'data-hook': 'review'})\n",
        "                    for review in reviews:\n",
        "                        try:\n",
        "                            reviewer_name = review.find('span', {'class': 'a-profile-name'})\n",
        "                            rating = review.find('i', {'data-hook': 'review-star-rating'})\n",
        "                            review_title = review.find('a', {'data-hook': 'review-title'})\n",
        "                            review_date = review.find('span', {'data-hook': 'review-date'})\n",
        "                            review_body = review.find('span', {'data-hook': 'review-body'})\n",
        "\n",
        "                            review_dict = {\n",
        "                                \"Reviewer Name\": reviewer_name.text.strip() if reviewer_name else \"N/A\",\n",
        "                                \"Rating\": rating.text.strip() if rating else \"N/A\",\n",
        "                                \"Title\": review_title.find_all('span')[2].text.strip() if review_title and len(review_title.find_all('span')) > 2 else \"N/A\",\n",
        "                                \"Date\": review_date.text.strip() if review_date else \"N/A\",\n",
        "                                \"Review Body\": review_body.text.strip() if review_body else \"N/A\"\n",
        "                            }\n",
        "\n",
        "                            all_reviews.append(review_dict)\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error extracting review details: {e}\")\n",
        "\n",
        "\n",
        "                    reviews_info = {\n",
        "\n",
        "                        \"All Reviews\": all_reviews\n",
        "                    }\n",
        "\n",
        "                # Combine product details and reviews\n",
        "                details.update(reviews_info)\n",
        "\n",
        "\n",
        "                return details\n",
        "            except (NoSuchElementException, TimeoutException, ElementNotInteractableException) as e:\n",
        "                print(f\"Attempt {attempt + 1} failed for URL {url}: {str(e)}\")\n",
        "                if attempt < max_attempts - 1:\n",
        "                    print(\"Retrying...\")\n",
        "                    time.sleep(random.uniform(2, 5))\n",
        "                else:\n",
        "                    print(f\"Failed to extract product details for {url} after {max_attempts} attempts.\")\n",
        "                    return {\"Product Title\": \"Error\", \"Category\": \"Error\"}\n",
        "\n",
        "\n",
        "    def click_see_more_reviews(self):\n",
        "        try:\n",
        "            see_more_reviews_link = WebDriverWait(self.driver, 10).until(\n",
        "                EC.element_to_be_clickable((By.CSS_SELECTOR, 'a[data-hook=\"see-all-reviews-link-foot\"]'))\n",
        "            )\n",
        "            self.driver.execute_script(\"arguments[0].click();\", see_more_reviews_link)\n",
        "            # print(\"Clicked on 'See more reviews' link.\")\n",
        "            time.sleep(random.uniform(3, 5))  # Wait for reviews to load\n",
        "        except Exception as e:\n",
        "            print(f\"Error clicking 'See more reviews' link: {e}\")\n",
        "\n",
        "    def extract_reviews(self, url):\n",
        "        max_attempts = 3\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                self.driver.get(url)\n",
        "                WebDriverWait(self.driver, 10).until(\n",
        "                    EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-hook=\"review\"]'))\n",
        "                )\n",
        "\n",
        "                # Click \"See more reviews\" if available\n",
        "                self.click_see_more_reviews()\n",
        "\n",
        "                # Extract reviews\n",
        "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "                reviews = soup.find_all('div', {'data-hook': 'review'})\n",
        "                all_reviews = []\n",
        "\n",
        "                for review in reviews:\n",
        "                    try:\n",
        "                        reviewer_name = review.find('span', {'class': 'a-profile-name'})\n",
        "                        rating = review.find('i', {'data-hook': 'review-star-rating'})\n",
        "                        review_title = review.find('a', {'data-hook': 'review-title'})\n",
        "                        review_date = review.find('span', {'data-hook': 'review-date'})\n",
        "                        review_body = review.find('span', {'data-hook': 'review-body'})\n",
        "\n",
        "                        review_dict = {\n",
        "                            \"Reviewer Name\": reviewer_name.text.strip() if reviewer_name else \"N/A\",\n",
        "                            \"Rating\": rating.text.strip() if rating else \"N/A\",\n",
        "                            \"Title\": review_title.find_all('span')[2].text.strip() if review_title and len(review_title.find_all('span')) > 2 else \"N/A\",\n",
        "                            \"Date\": review_date.text.strip() if review_date else \"N/A\",\n",
        "                            \"Review Body\": review_body.text.strip() if review_body else \"N/A\"\n",
        "                        }\n",
        "\n",
        "                        all_reviews.append(review_dict)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error extracting review details: {e}\")\n",
        "\n",
        "                # Extract top positive and critical reviews\n",
        "                positive_review_div = soup.find('div', class_='a-column a-span6 view-point-review positive-review')\n",
        "                top_positive_review = self.extract_top_review(positive_review_div)\n",
        "\n",
        "                critical_review_div = soup.find('div', class_='a-column a-span6 view-point-review critical-review a-span-last')\n",
        "                top_critical_review = self.extract_top_review(critical_review_div)\n",
        "\n",
        "                # Update product data with extracted reviews\n",
        "                return {\n",
        "                    \"Top Positive Review\": top_positive_review,\n",
        "                    \"Top Critical Review\": top_critical_review,\n",
        "                    \"All Reviews\": all_reviews\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt + 1} failed to extract reviews for URL {url}: {str(e)}\")\n",
        "                if attempt < max_attempts - 1:\n",
        "                    print(\"Retrying...\")\n",
        "                    time.sleep(random.uniform(2, 5))\n",
        "                else:\n",
        "                    print(f\"Failed to extract reviews for {url} after {max_attempts} attempts.\")\n",
        "                    return {\n",
        "                        \"Top Positive Review\": None,\n",
        "                        \"Top Critical Review\": None,\n",
        "                        \"All Reviews\": []\n",
        "                    }\n",
        "\n",
        "    def extract_top_review(self, review_div):\n",
        "        if review_div:\n",
        "            rating = review_div.find('i', class_='review-rating')\n",
        "            title = review_div.find('span', class_='review-title')\n",
        "            date = review_div.find('span', class_='review-date')\n",
        "            review_text = review_div.find('div', class_='a-row a-spacing-top-mini')\n",
        "\n",
        "            return {\n",
        "                'rating': rating.get_text(strip=True).split(' ')[0] if rating else None,\n",
        "                'title': title.get_text(strip=True) if title else None,\n",
        "                'date': date.get_text(strip=True) if date else None,\n",
        "                'review_text': review_text.get_text(strip=True) if review_text else None\n",
        "            }\n",
        "        return None\n",
        "\n",
        "    def run(self):\n",
        "        try:\n",
        "            self.access_amazon()\n",
        "            # self.change_amazon_location()\n",
        "            self.extract_product_links_and_prices()\n",
        "\n",
        "            for product in tqdm(self.product_data, desc=\"Processing Product URLs\"):\n",
        "                try:\n",
        "                    details = self.extract_product_details(product[\"link\"])\n",
        "                    # reviews = self.extract_reviews(product[\"link\"])\n",
        "                    product.update(details)\n",
        "                    # product.update(reviews)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing URL {product['link']}: {str(e)}\")\n",
        "\n",
        "            df = pd.DataFrame(self.product_data)\n",
        "\n",
        "\n",
        "            # Convert nested dictionaries to strings\n",
        "            # df['Top Positive Review'] = df['Top Positive Review'].apply(lambda x: str(x) if x else None)\n",
        "            # df['Top Critical Review'] = df['Top Critical Review'].apply(lambda x: str(x) if x else None)\n",
        "            # df['All Reviews'] = df['All Reviews'].apply(lambda x: str(x) if x else None)\n",
        "\n",
        "            # print(\"Saving data to CSV file...\")\n",
        "            # df.to_csv('amazon_products_m.csv', index=False)\n",
        "            # print(\"Data has been saved successfully.\")\n",
        "            # files.download('amazon_products_m.csv')\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the scraping process: {str(e)}\")\n",
        "        finally:\n",
        "            self.driver.quit()\n",
        "def main():\n",
        "    search_term = 'mobile phone'\n",
        "    num_pages =21\n",
        "    country = \"Egypt\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    scraper = AmazonScraper(search_term, num_pages, country)\n",
        "    scraper.run()\n",
        "    return scraper.product_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_df(df):\n",
        "    # Check if df is a list or DataFrame and validate accordingly\n",
        "    if isinstance(df, list) and df:  # Check if it's a non-empty list\n",
        "        return True\n",
        "    elif hasattr(df, 'empty'):  # Check if it's a DataFrame\n",
        "        return not df.empty\n",
        "    return False\n",
        "\n",
        "# Loop until a valid df_f is returned\n",
        "df_f = None\n",
        "max_attempts = 5  # Set a limit to avoid infinite loops\n",
        "attempt = 0\n",
        "\n",
        "while attempt < max_attempts:\n",
        "    df_f = main()\n",
        "    if validate_df(df_f):\n",
        "        print(\"DataFrame successfully processed.\")\n",
        "        break\n",
        "    else:\n",
        "        print(f\"Attempt {attempt + 1}: DataFrame is not valid. Reprocessing...\")\n",
        "        attempt += 1\n",
        "\n",
        "if df_f is None or not validate_df(df_f):\n",
        "    print(\"Failed to generate a valid DataFrame after multiple attempts.\")\n",
        "else:\n",
        "    print(\"Final DataFrame\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTakxs3L0Lb1",
        "outputId": "ff43fa0b-d16a-4208-b704-f9123441a843"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accessing Amazon...\n",
            "No CAPTCHA detected.\n",
            "Extracting product links and prices...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPages:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 48 products so far...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pages: 100%|██████████| 1/1 [00:08<00:00,  8.17s/it]\n",
            "Processing Product URLs: 100%|██████████| 10/10 [01:27<00:00,  8.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame successfully processed.\n",
            "Final DataFrame\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "bS_l63vGLmUj"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_f=pd.DataFrame(df_f)"
      ],
      "metadata": {
        "id": "NKf4PAx806wd"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get today's date\n",
        "today_date = datetime.now().date()\n",
        "\n",
        "# Add a column with today's date\n",
        "df_f['date'] = today_date"
      ],
      "metadata": {
        "id": "VvT_s4M5Lkoc"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_f['Site']='amazon_eg'\n",
        "df_f['currency']='egp'"
      ],
      "metadata": {
        "id": "rAB7ABhgLyxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_f.rename(columns={'Brand Name': 'Brand','RAM Memory Installed':'Ram Memory Installed Size','Wireless Provider':'Wireless Carrier'}, inplace=True)"
      ],
      "metadata": {
        "id": "FXIruy8HL3XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new=df_f.copy()"
      ],
      "metadata": {
        "id": "UHpJuZj0KkMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "import os\n",
        "\n",
        "# Azure Storage connection string\n",
        "connect_str = \"DefaultEndpointsProtocol=https;AccountName=ynwa;AccountKey=aTLtGZuymmaAirlsL1/39g3dDjaQvBFu3lHzQaUrY0o6LDdGduAW8Wbk8qI7fuilCKS8chuiklq7+AStBN3UdA==;EndpointSuffix=core.windows.net\"\n",
        "\n",
        "# Initialize BlobServiceClient\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "\n",
        "# Name of the container and blob\n",
        "container_name = \"disco\"  # Replace with your container name\n",
        "folder_name = \"raw\"\n",
        "\n",
        "# Name of the blob (file) with today's date\n",
        "blob_name = f\"{folder_name}/amazon_eg_raw{today_date}.csv\"  # Example: scraped_data_2024-10-07.csv\n",
        "\n",
        "# Create a container if it doesn't exist\n",
        "container_client = blob_service_client.get_container_client(container_name)\n",
        "try:\n",
        "    container_client.create_container()\n",
        "except Exception as e:\n",
        "    print(f\"Container already exists: {e}\")\n",
        "\n",
        "\n",
        "# Save the CSV locally (you can skip this if you already have the file path)\n",
        "csv_file_path = f\"scraped_data_{today_date}.csv\"\n",
        "df_f.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Upload the CSV file to Blob Storage with today's date in the name\n",
        "with open(csv_file_path, \"rb\") as data:\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "    blob_client.upload_blob(data, overwrite=True)\n",
        "    print(f\"CSV file {blob_name} uploaded successfully.\")\n"
      ],
      "metadata": {
        "id": "alqUMxZ7ZTNz",
        "outputId": "5a9de7b2-11a2-4d2a-8b41-1ae481139a5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Container already exists: The specified container already exists.\n",
            "RequestId:631645ef-501e-0007-23e8-2095fa000000\n",
            "Time:2024-10-17T23:01:04.6975689Z\n",
            "ErrorCode:ContainerAlreadyExists\n",
            "Content: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>ContainerAlreadyExists</Code><Message>The specified container already exists.\n",
            "RequestId:631645ef-501e-0007-23e8-2095fa000000\n",
            "Time:2024-10-17T23:01:04.6975689Z</Message></Error>\n",
            "CSV file raw/amazon_eg_raw2024-10-17.csv uploaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ordered_columns_original = [ 'link','Site','Category','rating', 'image_url',\n",
        "    'Brand', 'Model Name', 'Product Title', 'price','currency',\n",
        "    'Operating System', 'Ram Memory Installed Size',\n",
        "    'Memory Storage Capacity', 'Screen Size', 'Resolution', 'Refresh Rate',\n",
        "    'CPU Speed', 'Connectivity Technology', 'CPU Model', 'Color',\n",
        "    'Wireless Carrier', 'Cellular Technology','date','price_before_discount',\n",
        "     'All Reviews'\n",
        "]\n",
        "# Ensure all desired columns are present in the DataFrame\n",
        "missing_columns = set(ordered_columns_original) - set(df_new.columns)\n",
        "if missing_columns:\n",
        "    print(f\"Warning: Missing columns: {missing_columns}\")\n",
        "\n",
        "df_new = df_new[ordered_columns_original]"
      ],
      "metadata": {
        "id": "DHYexbszLGCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# Function to clean and convert the price to an integer\n",
        "def clean_price(price_str):\n",
        "    # Check if the input is a valid string\n",
        "    if not isinstance(price_str, str) or not price_str.strip():\n",
        "        return None  # Return None if it's not a valid string\n",
        "\n",
        "    # Remove any non-numeric characters except for the decimal point and commas\n",
        "    cleaned_price = re.sub(r'[^\\d.,]', '', price_str)\n",
        "\n",
        "    # Replace commas with nothing to clean thousands separator\n",
        "    cleaned_price = cleaned_price.replace(',', '')\n",
        "\n",
        "    # If the cleaned price is empty or just a period, return None\n",
        "    if not cleaned_price or cleaned_price == '.':\n",
        "        return None\n",
        "\n",
        "    # Convert the cleaned price to a float first and then cast it to an integer\n",
        "    price_value = float(cleaned_price)  # Convert to float to handle decimal if any\n",
        "    return int(price_value)  # Cast to integer, truncating any decimals\n",
        "\n",
        "# Apply the cleaning function to the 'price_before_discount' column\n",
        "df_new['price'] = df_new['price'].apply(clean_price)"
      ],
      "metadata": {
        "id": "U6YvrZY5kfjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean and convert the price to an integer\n",
        "def clean_price(price_str):\n",
        "    # Check if the input is a valid string\n",
        "    if not isinstance(price_str, str) or not price_str.strip():\n",
        "        return None  # Return None if it's not a valid string\n",
        "\n",
        "    # Remove any non-numeric characters except the decimal point\n",
        "    cleaned_price = re.sub(r'[^\\d.]', '', price_str)\n",
        "\n",
        "    # If cleaned_price is empty, return None\n",
        "    if not cleaned_price:\n",
        "        return None\n",
        "\n",
        "    # Convert the cleaned price to a float first and then cast it to an integer\n",
        "    price_value = float(cleaned_price)  # Convert to float first to handle decimals\n",
        "    return int(price_value)  # Cast to integer (truncates the decimal part)\n",
        "\n",
        "# Apply the cleaning function to the 'price_before_discount' column\n",
        "df_new['price_before_discount'] = df_new['price_before_discount'].apply(clean_price)"
      ],
      "metadata": {
        "id": "6HAsbVsql-21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of df_new if it is a slice\n",
        "df_new = df_new.copy()\n",
        "\n",
        "# Drop rows with NaN values in the specified subset\n",
        "df_new.dropna(subset=['Model Name', 'price'], inplace=True)\n",
        "df_new.dropna(subset=['Model Name','price'],inplace=True)"
      ],
      "metadata": {
        "id": "pPoyByFloigt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set 'price' to None where 'discount' is less than 0\n",
        "df_new.loc[df_new['price'] > df_new['price_before_discount'], 'price_before_discount'] = None"
      ],
      "metadata": {
        "id": "epH8Pht0kpOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.reset_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "AugZ6dWzlPvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.info()"
      ],
      "metadata": {
        "id": "3vwAea_gZTp5",
        "outputId": "b8ffaf5d-faa5-4999-bbab-06dbdddab81a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 10 entries, 0 to 9\n",
            "Data columns (total 25 columns):\n",
            " #   Column                     Non-Null Count  Dtype  \n",
            "---  ------                     --------------  -----  \n",
            " 0   link                       10 non-null     object \n",
            " 1   Site                       10 non-null     object \n",
            " 2   Category                   10 non-null     object \n",
            " 3   rating                     10 non-null     object \n",
            " 4   image_url                  10 non-null     object \n",
            " 5   Brand                      10 non-null     object \n",
            " 6   Model Name                 10 non-null     object \n",
            " 7   Product Title              10 non-null     object \n",
            " 8   price                      10 non-null     float64\n",
            " 9   currency                   10 non-null     object \n",
            " 10  Operating System           10 non-null     object \n",
            " 11  Ram Memory Installed Size  7 non-null      object \n",
            " 12  Memory Storage Capacity    10 non-null     object \n",
            " 13  Screen Size                10 non-null     object \n",
            " 14  Resolution                 8 non-null      object \n",
            " 15  Refresh Rate               9 non-null      object \n",
            " 16  CPU Speed                  2 non-null      object \n",
            " 17  Connectivity Technology    2 non-null      object \n",
            " 18  CPU Model                  10 non-null     object \n",
            " 19  Color                      1 non-null      object \n",
            " 20  Wireless Carrier           9 non-null      object \n",
            " 21  Cellular Technology        2 non-null      object \n",
            " 22  date                       10 non-null     object \n",
            " 23  price_before_discount      6 non-null      float64\n",
            " 24  All Reviews                10 non-null     object \n",
            "dtypes: float64(2), object(23)\n",
            "memory usage: 2.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_new['discount']=((df_new['price_before_discount']-df_new['price'])/df_new['price_before_discount'])*100"
      ],
      "metadata": {
        "id": "LmVJ3fnIaY9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'price' column from float to int, handling None/NaN values\n",
        "df_new['price'] = df_new['price'].astype(int)\n",
        "df_new['price_before_discount'] = df_new['price_before_discount'].fillna(df_new['price'])\n",
        "# Convert 'price_before_discount' column from float to int, handling None/NaN values\n",
        "df_new['price_before_discount'] = df_new['price_before_discount'].astype(int)"
      ],
      "metadata": {
        "id": "0vXGeC7sbGPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.info()"
      ],
      "metadata": {
        "id": "o4n2_RtWnq5n",
        "outputId": "3e9f02c8-d7e8-47bd-b1bf-ad70906a3949",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 10 entries, 0 to 9\n",
            "Data columns (total 26 columns):\n",
            " #   Column                     Non-Null Count  Dtype  \n",
            "---  ------                     --------------  -----  \n",
            " 0   link                       10 non-null     object \n",
            " 1   Site                       10 non-null     object \n",
            " 2   Category                   10 non-null     object \n",
            " 3   rating                     10 non-null     object \n",
            " 4   image_url                  10 non-null     object \n",
            " 5   Brand                      10 non-null     object \n",
            " 6   Model Name                 10 non-null     object \n",
            " 7   Product Title              10 non-null     object \n",
            " 8   price                      10 non-null     int64  \n",
            " 9   currency                   10 non-null     object \n",
            " 10  Operating System           10 non-null     object \n",
            " 11  Ram Memory Installed Size  7 non-null      object \n",
            " 12  Memory Storage Capacity    10 non-null     object \n",
            " 13  Screen Size                10 non-null     object \n",
            " 14  Resolution                 8 non-null      object \n",
            " 15  Refresh Rate               9 non-null      object \n",
            " 16  CPU Speed                  2 non-null      object \n",
            " 17  Connectivity Technology    2 non-null      object \n",
            " 18  CPU Model                  10 non-null     object \n",
            " 19  Color                      1 non-null      object \n",
            " 20  Wireless Carrier           9 non-null      object \n",
            " 21  Cellular Technology        2 non-null      object \n",
            " 22  date                       10 non-null     object \n",
            " 23  price_before_discount      10 non-null     int64  \n",
            " 24  All Reviews                10 non-null     object \n",
            " 25  discount                   6 non-null      float64\n",
            "dtypes: float64(1), int64(2), object(23)\n",
            "memory usage: 2.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_column_names(column_names):\n",
        "    return [name.replace(' ', '_').lower() for name in column_names]\n",
        "\n",
        "# Preprocess column names in the DataFrame\n",
        "df_new.columns = format_column_names(df_new.columns)"
      ],
      "metadata": {
        "id": "3UinT4jFoZ0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.rename(columns={'operating_system':'os','ram_memory_installed_size':'ram',\n",
        "                         'memory_storage_capacity':'storage','today':'date'}, inplace=True)"
      ],
      "metadata": {
        "id": "9dMhWf_KpRLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop nulls in model name\n",
        "df_new.dropna(subset=['model_name','price'], inplace=True)\n",
        "df_new.reset_index(drop=True, inplace=True)\n",
        "df_new.shape"
      ],
      "metadata": {
        "id": "uHmHoY_qpTl8",
        "outputId": "bd981d5f-0bcb-4890-a300-626949450c95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all values to lowercase\n",
        "df_new = df_new.apply(lambda x: x.apply(lambda y: y.lower() if isinstance(y, str) else y) if x.name not in ['link', 'image_url'] else x)"
      ],
      "metadata": {
        "id": "9iQg7pwzpWik",
        "outputId": "b56f88f5-143f-43fc-e35f-dcbf72fd2747",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-f0591b57d383>:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df_new = df_new.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_last=df_new.copy()"
      ],
      "metadata": {
        "id": "NOl_PYwzpaOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function definitions with updated names\n",
        "def format_samsung_model(model):\n",
        "    model = model.lower()  # Convert to lowercase for consistency\n",
        "\n",
        "    # Extract special terms like FE, Ultra, Plus, 5G, 4G\n",
        "    special_terms = re.findall(r'\\b(fe|ultra|plus)\\b', model, re.IGNORECASE)\n",
        "    special_terms = list(dict.fromkeys(special_terms))  # Remove duplicates while maintaining order\n",
        "    special_terms_str = ' '.join(special_terms).upper()\n",
        "\n",
        "    # Remove unwanted words and characters\n",
        "    model = re.sub(r'\\b(samsung|galaxy)\\b', '', model).strip()\n",
        "    model = re.sub(r'\\b(5g|4g)\\b', '', model).strip()  # Remove 5G/4G here to prevent duplication\n",
        "\n",
        "    # Extract core model name\n",
        "    if 'note' in model:\n",
        "        match = re.search(r'note\\s*(\\d*)\\s*(\\w*)', model)\n",
        "        if match:\n",
        "            note_num, note_suffix = match.groups()\n",
        "            core_model = f\"Note {note_num} {note_suffix}\".strip().title()\n",
        "    else:\n",
        "        match = re.search(r'([a-z]+\\s*\\d+(?:\\s*[a-z]+)?)', model)\n",
        "        if match:\n",
        "            core_model = match.group(1).strip().title()\n",
        "        else:\n",
        "            core_model = model.strip().title()\n",
        "\n",
        "    # Remove special terms from core_model if they're already present\n",
        "    for term in special_terms:\n",
        "        core_model = re.sub(rf'\\b{term}\\b', '', core_model, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    # Combine core model and special terms\n",
        "    result = f'{core_model} {special_terms_str}'.strip()\n",
        "\n",
        "    return result.lower()\n",
        "\n",
        "def clean_motorola_model(model):\n",
        "    if not isinstance(model, str):\n",
        "        return model\n",
        "\n",
        "    # Remove \"moto\" and \"motorola\"\n",
        "    model = re.sub(r'\\bmoto\\b|\\bmotorola\\b', '', model)\n",
        "\n",
        "    # Remove '+' characters\n",
        "    model = re.sub(r'\\+', '', model)\n",
        "\n",
        "    # Remove parentheses but preserve the content\n",
        "    model = re.sub(r'\\s*\\((\\d{4})\\)', r' \\1', model)  # Preserve year in parentheses\n",
        "\n",
        "    # Remove single quotes in years (e.g., '22)\n",
        "    model = re.sub(r'\\'(\\d{2})', r'\\1', model)\n",
        "\n",
        "    # Replace multiple spaces with a single space\n",
        "    model = re.sub(r'\\s+', ' ', model)\n",
        "\n",
        "    # Strip leading and trailing whitespace\n",
        "    model = model.strip()\n",
        "\n",
        "    return model\n",
        "def extract_google_model(model):\n",
        "    # Ensure model is a string\n",
        "    if not isinstance(model, str):\n",
        "        return model  # Return the original value if it's not a string\n",
        "\n",
        "    # Convert to lowercase for consistency\n",
        "    model = model.lower()\n",
        "\n",
        "    # Use regex to extract everything after \"pixel\"\n",
        "    match = re.search(r'\\bpixel\\s+(.*)', model)\n",
        "    if match:\n",
        "        return match.group(1).strip()  # Return the portion after \"pixel\"\n",
        "\n",
        "    return model\n",
        "\n",
        "def clean_oneplus_model(model):\n",
        "    if not isinstance(model, str):\n",
        "        return model\n",
        "\n",
        "    # Remove \"oneplus\" and leading whitespace\n",
        "    model = re.sub(r'\\boneplus\\b', '', model).strip()\n",
        "\n",
        "    # Remove parentheses but keep the content inside\n",
        "    model = re.sub(r'\\s*\\(', ' ', model)\n",
        "    model = re.sub(r'\\)', '', model)\n",
        "\n",
        "    # Remove any text associated with \"gb\"\n",
        "    model = re.sub(r'\\s*\\d+gb\\b', '', model, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    # Extract text up to and including \"5g\"\n",
        "    match = re.search(r'(.*?\\b5g\\b)', model)\n",
        "    if match:\n",
        "        model = match.group(1).strip()\n",
        "\n",
        "    return model\n",
        "\n",
        "def remove_xiaomi_brand(model):\n",
        "    return model.replace('xiaomi', '').strip()\n",
        "\n",
        "def extract_iphone_model(model):\n",
        "    parts = model.split('iphone ', 1)\n",
        "    if len(parts) > 1:\n",
        "        return parts[1].split('\\t', 1)[0].strip()\n",
        "    return model.strip()\n",
        "\n",
        "# Function to apply based on brand\n",
        "def process_model(row):\n",
        "    brand = row['brand'].lower()\n",
        "    model_name = row['model_name']\n",
        "\n",
        "    if brand == 'samsung':\n",
        "        return format_samsung_model(model_name)\n",
        "    elif brand == 'motorola':\n",
        "        return clean_motorola_model(model_name)\n",
        "    elif brand == 'google':\n",
        "        return extract_google_model(model_name)\n",
        "    elif brand == 'oneplus':\n",
        "        return clean_oneplus_model(model_name)\n",
        "    elif brand == 'xiaomi':\n",
        "        return remove_xiaomi_brand(model_name)\n",
        "    elif brand == 'apple':\n",
        "        return extract_iphone_model(model_name)\n",
        "    else:\n",
        "        return model_name\n",
        "\n",
        "\n",
        "# Apply function to the DataFrame\n",
        "df_last['model_name'] = df_last.apply(process_model, axis=1)"
      ],
      "metadata": {
        "id": "q0HNf6zCpcSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the function to extract and format OS versions\n",
        "def extract_versions(text):\n",
        "    if not isinstance(text, str):  # Ensure the input is a string\n",
        "        return ''\n",
        "\n",
        "    text = text.lower()  # Convert to lowercase for consistency\n",
        "\n",
        "    results = set()  # Use a set to avoid duplicates\n",
        "\n",
        "    # Helper function to format version numbers\n",
        "    def format_version(version):\n",
        "        if '.' not in version:\n",
        "            return f\"{version}.0\"  # Add '.0' if there is no decimal point\n",
        "        return version\n",
        "\n",
        "    # Special case for 'google_android'\n",
        "    if 'google_android' in text:\n",
        "        text = 'android' + text.split('google_android')[1]\n",
        "\n",
        "    # Extract Android version numbers\n",
        "    android_match = re.search(r'\\bandroid\\s+(\\d+(\\.\\d+)?)\\b', text)\n",
        "    if android_match:\n",
        "        version = format_version(android_match.group(1))\n",
        "        results.add(f\"android {version}\")\n",
        "\n",
        "    # Extract iOS version numbers\n",
        "    ios_match = re.search(r'\\bios\\s+(\\d+(\\.\\d+)?)\\b', text)\n",
        "    if ios_match:\n",
        "        version = format_version(ios_match.group(1))\n",
        "        results.add(f\"ios {version}\")\n",
        "\n",
        "    # Extract any other OS version numbers\n",
        "    os_match = re.search(r'\\b(\\w+os)\\s+(\\d+(\\.\\d+)?)\\b', text)\n",
        "    if os_match:\n",
        "        os_name = os_match.group(1).lower()\n",
        "        version = format_version(os_match.group(2))\n",
        "        results.add(f\"{os_name} {version}\")\n",
        "\n",
        "    # Return the results as a comma-separated string\n",
        "    return ', '.join(results) if results else text\n",
        "df_last['os'] = df_last['os'].apply(extract_versions)"
      ],
      "metadata": {
        "id": "aHRmzA8OpfCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_last['os']=df_last.os.map(lambda x: 'android' if 'oxygenos' in x else x)\n"
      ],
      "metadata": {
        "id": "qFApYsbZpitD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to extract RAM size\n",
        "def extract_ram(text):\n",
        "    if not isinstance(text, str):  # Ensure the input is a string\n",
        "        return ''\n",
        "\n",
        "    # Use regex to find the RAM size\n",
        "    match = re.search(r'(\\d+)\\s*gb', text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1)  # Return only the numeric part of the RAM size\n",
        "\n",
        "    return ''  # Return empty string if no match is found\n",
        "df_last['ram']=df_last['ram'].apply(extract_ram)"
      ],
      "metadata": {
        "id": "Mk8ImNF3pkei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract storage size\n",
        "def extract_storage(text):\n",
        "    if not isinstance(text, str):  # Ensure the input is a string\n",
        "        return ''\n",
        "\n",
        "    match = re.search(r'(\\d+)\\s*gb', text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1)  # Return only the numeric part of the storage size\n",
        "\n",
        "    return ''  # Return empty string if no match is found\n",
        "\n",
        "# Function to extract screen size\n",
        "def extract_screen_size(text):\n",
        "    if not isinstance(text, str):  # Ensure the input is a string\n",
        "        return ''\n",
        "\n",
        "    match = re.search(r'(\\d+(\\.\\d+)?)\\s*inches?', text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1)  # Return the screen size\n",
        "\n",
        "    return ''  # Return empty string if no match is found\n",
        "\n",
        "# Function to extract resolution\n",
        "def extract_resolution(text):\n",
        "    if not isinstance(text, str):  # Ensure the input is a string\n",
        "        return ''\n",
        "\n",
        "    # Use regex to find resolution in formats like '1920 x 1080', '1920x1080', '1280 x 720 pixels', etc.\n",
        "    match = re.search(r'(\\d{3,4})\\s*x\\s*(\\d{3,4})\\b', text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return f\"{match.group(1)} x {match.group(2)}\"  # Return resolution in 'width x height' format\n",
        "\n",
        "    # Handle cases where resolution might be written as '1920x1080' without spaces\n",
        "    match_no_space = re.search(r'(\\d{3,4})\\s*x\\s*(\\d{3,4})', text, re.IGNORECASE)\n",
        "    if match_no_space:\n",
        "        return f\"{match_no_space.group(1)} x {match_no_space.group(2)}\"  # Return resolution in 'width x height' format\n",
        "\n",
        "    return ''  # Return empty string if no match is found\n",
        "\n",
        "# Function to extract refresh rate\n",
        "def extract_refresh_rate(text):\n",
        "    if not isinstance(text, str):  # Ensure the input is a string\n",
        "        return ''\n",
        "\n",
        "    match = re.search(r'(\\d+)\\s*hz', text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1)  # Return the refresh rate\n",
        "\n",
        "    return ''  # Return empty string if no match is found\n",
        "\n",
        "# Function to extract CPU speed\n",
        "def extract_cpu_speed(text):\n",
        "    if not isinstance(text, str):  # Ensure the input is a string\n",
        "        return ''\n",
        "\n",
        "    match = re.search(r'(\\d+(\\.\\d+)?)\\s*ghz', text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1)  # Return the CPU speed\n",
        "\n",
        "    return ''  # Return empty string if no match is found\n"
      ],
      "metadata": {
        "id": "0hrDVSFopmAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEJmZpsqylp8"
      },
      "outputs": [],
      "source": [
        "df_last['storage']=df_last['storage'].apply(extract_storage)\n",
        "df_last['screen_size']=df_last['screen_size'].apply(extract_screen_size)\n",
        "df_last['resolution']=df_last['resolution'].apply(extract_resolution)\n",
        "df_last['refresh_rate']=df_last['refresh_rate'].apply(extract_refresh_rate)\n",
        "df_last['cpu_speed']=df_last['cpu_speed'].apply(extract_cpu_speed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7_Np1zNylnl"
      },
      "outputs": [],
      "source": [
        "df_last.rename(columns={'ram':'ram_gb','screen_size':'screen_size_in','refresh_rate':'refresh_rate_hz','cpu_speed':'cpu_speed_ghz'},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNHcjh7JyllQ"
      },
      "outputs": [],
      "source": [
        "df_last['wireless_carrier']=df_last['wireless_carrier'].map(lambda x: 'unlocked' if x=='unlocked for all carriers' or x==' unlocked' else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uznrBIsfylir"
      },
      "outputs": [],
      "source": [
        "# Define the mapping dictionary\n",
        "carrier_mapping = {\n",
        "    'unlocked': 'unlocked',\n",
        "    'tracfone': 'tracfone',\n",
        "    'verizon': 'verizon',\n",
        "    'verizon wireless': 'verizon',\n",
        "    't-mobile': 't-mobile',\n",
        "    'simple mobile': 'simple mobile',\n",
        "    '3': '3',\n",
        "    'mvno': 'mvno',\n",
        "    't-mobile, at&t': 't-mobile',\n",
        "    'at&t': 'at&t',\n",
        "    'straight talk': 'straight talk',\n",
        "    'boost mobile': 'boost mobile',\n",
        "    'vodafone': 'vodafone',\n",
        "    't-mobile, unlocked': 't-mobile',\n",
        "    't-mobile, unlocked, verizon, sprint': 't-mobile',\n",
        "    't-mobile, unlocked, sprint': 't-mobile',\n",
        "    'sprint': 'sprint',\n",
        "    'total wireless': 'total wireless'\n",
        "}\n",
        "\n",
        "# Handle NaN values separately\n",
        "df_last['wireless_carrier'] = df_last['wireless_carrier'].fillna('')\n",
        "\n",
        "# Apply the mapping dictionary\n",
        "df_last['wireless_carrier'] = df_last['wireless_carrier'].map(carrier_mapping).fillna(df_last['wireless_carrier'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpHnpdBIylgd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Replace all zeroes with NaNs\n",
        "df_last.replace(0, np.nan, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJahX59KyleD"
      },
      "outputs": [],
      "source": [
        "# Replace empty strings and spaces with NaNs\n",
        "df_last.replace(r'^\\s*$', np.nan, regex=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVc38f9sylTT"
      },
      "outputs": [],
      "source": [
        "# Convert `price_usd` to numeric\n",
        "df_last['price'] = pd.to_numeric(df_last['price'], errors='coerce')\n",
        "\n",
        "# Convert `ram_gb`, `storage`, `screen_size_in`, `refresh_rate_hz`, `cpu_speed_ghz` to numeric\n",
        "df_last['ram_gb'] = pd.to_numeric(df_last['ram_gb'], errors='coerce')\n",
        "df_last['storage'] = pd.to_numeric(df_last['storage'], errors='coerce')\n",
        "df_last['screen_size_in'] = pd.to_numeric(df_last['screen_size_in'], errors='coerce')\n",
        "df_last['refresh_rate_hz'] = pd.to_numeric(df_last['refresh_rate_hz'], errors='coerce')\n",
        "df_last['cpu_speed_ghz'] = pd.to_numeric(df_last['cpu_speed_ghz'], errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JftXT-K8yHCM"
      },
      "outputs": [],
      "source": [
        "def extract_product_id(url):\n",
        "    # Regular expression pattern to capture product ID after '/dp/'\n",
        "    pattern = r'/dp/([A-Za-z0-9]+)'\n",
        "    match = re.search(pattern, url)\n",
        "\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_review_url( product_url):\n",
        "        asin =extract_product_id(product_url)\n",
        "        if asin:\n",
        "            return f\"https://www.amazon.eg/-/en/product-reviews/{asin}\"\n",
        "        return None\n",
        "def extract_rating(text):\n",
        "    # Regular expression to match a floating point number at the start of the string\n",
        "    pattern = r'(\\d+\\.\\d+)'\n",
        "    match = re.search(pattern, text)\n",
        "\n",
        "    if match:\n",
        "        return float(match.group(1))\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu451_Gaznw0"
      },
      "outputs": [],
      "source": [
        "df_last['ASIN']=df_last['link'].apply(extract_product_id)\n",
        "df_last['review_url']=df_last['link'].apply(get_review_url)\n",
        "df_last['rating']=df_last['rating'].apply(extract_rating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaTQyvQ5yG_u"
      },
      "outputs": [],
      "source": [
        "df=df_last.copy()\n",
        "# Function to extract and remove network type\n",
        "def extract_network(text):\n",
        "    # Check if text is a string, otherwise return it unchanged\n",
        "    if isinstance(text, str):\n",
        "        match = re.search(r'(5g|4g)', text)\n",
        "        if match:\n",
        "            network = match.group(0)\n",
        "            # Remove 5G/4G from the original text\n",
        "            text = re.sub(r'\\s?(5g|4g)\\s?', '', text).strip()\n",
        "            return text, network\n",
        "    return text, None\n",
        "\n",
        "# Initialize 'network' column as None\n",
        "df['network'] = None\n",
        "\n",
        "# Apply the extraction logic to each column and update 'network' column\n",
        "for col in ['model_name', 'product_title', 'cellular_technology']:\n",
        "    # Apply the function to extract network and updated column text\n",
        "    df[col], extracted_networks = zip(*df[col].apply(extract_network))\n",
        "\n",
        "    # Fill the 'network' column where it is None with the extracted network\n",
        "    df['network'] = df['network'].combine_first(pd.Series(extracted_networks))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmJkwbaFyG88"
      },
      "outputs": [],
      "source": [
        "# Fill any remaining None values in 'network' using 'cellular_technology'\n",
        "df['network'] = df['network'].fillna(df['cellular_technology'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO3KL4IGyG6T"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=['cellular_technology'],inplace=True)\n",
        "df.dropna(subset=['price'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9txlLO_HyGut"
      },
      "outputs": [],
      "source": [
        "df.date=df.date.astype('datetime64[ns]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "se7C5DAiOH3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b036598f-4123-44ed-a7c4-34a5afa3e921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Container already exists: The specified container already exists.\n",
            "RequestId:0a15f60d-c01e-002a-48e8-202689000000\n",
            "Time:2024-10-17T23:03:01.3299556Z\n",
            "ErrorCode:ContainerAlreadyExists\n",
            "Content: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>ContainerAlreadyExists</Code><Message>The specified container already exists.\n",
            "RequestId:0a15f60d-c01e-002a-48e8-202689000000\n",
            "Time:2024-10-17T23:03:01.3299556Z</Message></Error>\n",
            "CSV file transformed/amazon_eg_tr2024-10-17.csv uploaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "import os\n",
        "\n",
        "# Azure Storage connection string\n",
        "connect_str = \"DefaultEndpointsProtocol=https;AccountName=ynwa;AccountKey=aTLtGZuymmaAirlsL1/39g3dDjaQvBFu3lHzQaUrY0o6LDdGduAW8Wbk8qI7fuilCKS8chuiklq7+AStBN3UdA==;EndpointSuffix=core.windows.net\"\n",
        "\n",
        "# Initialize BlobServiceClient\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "\n",
        "# Name of the container and blob\n",
        "container_name = \"disco\"  # Replace with your container name\n",
        "folder_name = \"transformed\"\n",
        "\n",
        "# Name of the blob (file) with today's date\n",
        "blob_name = f\"{folder_name}/amazon_eg_tr{today_date}.csv\"  # Example: scraped_data_2024-10-07.csv\n",
        "\n",
        "# Create a container if it doesn't exist\n",
        "container_client = blob_service_client.get_container_client(container_name)\n",
        "try:\n",
        "    container_client.create_container()\n",
        "except Exception as e:\n",
        "    print(f\"Container already exists: {e}\")\n",
        "\n",
        "\n",
        "# Save the CSV locally (you can skip this if you already have the file path)\n",
        "csv_file_path = f\"scraped_data_{today_date}.csv\"\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Upload the CSV file to Blob Storage with today's date in the name\n",
        "with open(csv_file_path, \"rb\") as data:\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "    blob_client.upload_blob(data, overwrite=True)\n",
        "    print(f\"CSV file {blob_name} uploaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['site']='amazon_eg'"
      ],
      "metadata": {
        "id": "DE-U5Il3qMK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import io\n",
        "import json\n",
        "import hashlib\n",
        "\n",
        "def generate_unique_product_id(row):\n",
        "    unique_string = f\"{row['brand']}_{row['model_name']}_{row['storage']}_{row['ram_gb']}_{row['network']}\"\n",
        "    return hashlib.md5(unique_string.encode()).hexdigest()\n",
        "\n",
        "def assign_partition(row, id_column, num_partitions=7):\n",
        "    return int(hashlib.md5(str(row[id_column]).encode()).hexdigest(), 16) % num_partitions\n",
        "\n",
        "def read_parquet_from_blob(blob_service_client, container_name, folder_name, file_name):\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=f\"{folder_name}/{file_name}\")\n",
        "    stream = blob_client.download_blob()\n",
        "    bytes_data = stream.readall()\n",
        "    return pd.read_parquet(io.BytesIO(bytes_data))\n",
        "\n",
        "def read_partitioned_parquet_from_blob(blob_service_client, container_name, folder_name, file_name):\n",
        "    all_data = []\n",
        "    for i in range(7):  # Assuming 7 partitions\n",
        "        try:\n",
        "            blob_client = blob_service_client.get_blob_client(container=container_name, blob=f\"{folder_name}/{file_name}/{i}_{file_name}\")\n",
        "            stream = blob_client.download_blob()\n",
        "            bytes_data = stream.readall()\n",
        "            partition_data = pd.read_parquet(io.BytesIO(bytes_data))\n",
        "            all_data.append(partition_data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading partition {i}: {e}\")\n",
        "    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
        "\n",
        "def write_to_parquet(data, blob_service_client, container_name, folder_name, file_name, partition_strategy=None):\n",
        "    if partition_strategy is None:\n",
        "        table = pa.Table.from_pandas(data)\n",
        "        buf = io.BytesIO()\n",
        "        pq.write_table(table, buf)\n",
        "        buf.seek(0)\n",
        "\n",
        "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=f\"{folder_name}/{file_name}\")\n",
        "        blob_client.upload_blob(buf.getvalue(), overwrite=True)\n",
        "    else:\n",
        "        data['partition'] = data.apply(partition_strategy, axis=1)\n",
        "        for partition, partition_data in data.groupby('partition'):\n",
        "            partition_data = partition_data.drop('partition', axis=1)\n",
        "            table = pa.Table.from_pandas(partition_data)\n",
        "            buf = io.BytesIO()\n",
        "            pq.write_table(table, buf)\n",
        "            buf.seek(0)\n",
        "\n",
        "            blob_client = blob_service_client.get_blob_client(container=container_name,\n",
        "                                                              blob=f\"{folder_name}/{file_name}/{partition}_{file_name}\")\n",
        "            blob_client.upload_blob(buf.getvalue(), overwrite=True)\n",
        "\n",
        "def update_fact_table(existing_fact, new_data):\n",
        "    new_fact = process_fact_table(new_data)\n",
        "    updated_fact = pd.concat([existing_fact, new_fact]).drop_duplicates(subset=['product_id','site_id','date','url'], keep='last')\n",
        "    return updated_fact\n",
        "\n",
        "def update_dim_device_specification(existing_dim, new_data):\n",
        "    new_dim = process_dim_device_specification(new_data)\n",
        "    updated_dim = pd.concat([existing_dim, new_dim]).drop_duplicates(subset=['product_id'], keep='last')\n",
        "    return updated_dim\n",
        "\n",
        "def update_product_mapping(existing_mapping, new_data):\n",
        "    new_mapping = process_product_mapping(new_data)\n",
        "    updated_mapping = pd.concat([existing_mapping, new_mapping]).drop_duplicates(subset=['product_id'], keep='last')\n",
        "    return updated_mapping\n",
        "\n",
        "def update_dim_site(existing_site, new_data):\n",
        "    new_site = process_dim_site(new_data)\n",
        "    updated_site = pd.concat([existing_site, new_site]).drop_duplicates(subset=['site_name'], keep='last')\n",
        "    updated_site['site_id'] = range(len(updated_site))  # Reassign site_ids\n",
        "    return updated_site\n",
        "\n",
        "def update_dim_review(existing_review, new_data):\n",
        "    new_review = process_dim_review(new_data)\n",
        "    updated_review = pd.concat([existing_review, new_review]).drop_duplicates(subset=['review_id'], keep='last')\n",
        "    return updated_review\n",
        "\n",
        "def update_dim_date(existing_date, new_data):\n",
        "    new_date = process_dim_date(new_data)\n",
        "    updated_date = pd.concat([existing_date, new_date]).drop_duplicates(subset=['date_ID'], keep='last')\n",
        "    return updated_date\n",
        "\n",
        "def process_fact_table(df):\n",
        "    fact_table = df[['product_id', 'site', 'date', 'price', 'currency', 'rating','link']].copy()\n",
        "    fact_table['site_id'] = 2\n",
        "    fact_table['discount'] = df['discount']\n",
        "    fact_table['price_before_discount'] = df['price_before_discount']\n",
        "    fact_table['rating_avg'] = fact_table['rating']\n",
        "    fact_table['url']=fact_table['link']\n",
        "    return fact_table[['product_id', 'url','site_id', 'date',  'price', 'currency', 'discount', 'price_before_discount', 'rating_avg']]\n",
        "\n",
        "def process_dim_device_specification(df):\n",
        "    return df[['product_id', 'product_title', 'image_url', 'os', 'screen_size_in', 'resolution', 'refresh_rate_hz', 'cpu_speed_ghz', 'cpu_model', 'color', 'wireless_carrier', 'category']].copy()\n",
        "\n",
        "def process_product_mapping(df):\n",
        "    return df[['product_id', 'model_name', 'brand', 'network', 'ram_gb', 'storage']].copy()\n",
        "\n",
        "def process_dim_site(df):\n",
        "    site_df = pd.DataFrame({'site_name': df['site'].unique()})\n",
        "    site_df['site_id'] = range(len(site_df))\n",
        "    return site_df\n",
        "\n",
        "def process_dim_review(df):\n",
        "    reviews_list = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Extract the list of reviews (list of dicts) for the current row\n",
        "        if row['all_reviews']:\n",
        "            for review in row['all_reviews']:\n",
        "                reviews_list.append({\n",
        "                    'review_id': hashlib.md5(f\"{row['product_id']}_{review['Date']}_{row['site']}\".encode()).hexdigest(),  # Unique review ID\n",
        "                    'product_id': row['product_id'],\n",
        "                    'product_reviews_url': row['review_url'],\n",
        "                    'review_text': review.get('Review Body', None),\n",
        "                    'review_rating': review.get('Rating', None).split()[0] if review.get('Rating') else None,\n",
        "                    'review_date': pd.to_datetime(review.get('Date').split('on ')[-1], format='%d %B %Y', errors='coerce'),\n",
        "                    'site': 2# Ensure site column is properly handled\n",
        "                })\n",
        "\n",
        "    # Convert the list of review dictionaries into a DataFrame\n",
        "    review_df = pd.DataFrame(reviews_list)\n",
        "\n",
        "    # Convert the 'site' column to string explicitly to avoid ArrowInvalid\n",
        "    # review_df['site'] = review_df['site'].astype(str)\n",
        "\n",
        "    return review_df\n",
        "\n",
        "def process_dim_date(df):\n",
        "    dates = pd.to_datetime(df['date'].unique())\n",
        "    date_df = pd.DataFrame({\n",
        "        'date_ID': dates,\n",
        "        'day': dates.day,\n",
        "        'month': dates.month,\n",
        "        'year': dates.year\n",
        "    })\n",
        "    return date_df\n",
        "\n",
        "def main():\n",
        "    # Azure Storage connection string\n",
        "    connect_str = \"DefaultEndpointsProtocol=https;AccountName=ynwa;AccountKey=aTLtGZuymmaAirlsL1/39g3dDjaQvBFu3lHzQaUrY0o6LDdGduAW8Wbk8qI7fuilCKS8chuiklq7+AStBN3UdA==;EndpointSuffix=core.windows.net\"\n",
        "\n",
        "    # Initialize BlobServiceClient\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "\n",
        "    # Name of the container and folder\n",
        "    container_name = \"disco\"\n",
        "    folder_name = \"model\"\n",
        "\n",
        "    # Read new data (replace this with your actual data loading method)\n",
        "    new_df = df.copy()\n",
        "\n",
        "    # Read existing product ID mapping\n",
        "    existing_product_id_mapping = read_parquet_from_blob(blob_service_client, container_name, folder_name, \"product_id_mapping.parquet\")\n",
        "\n",
        "    # Generate unique product IDs for new data, using existing IDs where possible\n",
        "    new_df['temp_id'] = new_df.apply(generate_unique_product_id, axis=1)\n",
        "    new_df = pd.merge(new_df, existing_product_id_mapping[['brand', 'model_name', 'storage', 'ram_gb', 'network', 'product_id']],\n",
        "                      on=['brand', 'model_name', 'storage', 'ram_gb', 'network'], how='left')\n",
        "    new_df['product_id'] = new_df['product_id'].fillna(new_df['temp_id'])\n",
        "    new_df = new_df.drop('temp_id', axis=1)\n",
        "\n",
        "    # Update product ID mapping\n",
        "    updated_product_id_mapping = pd.concat([existing_product_id_mapping, new_df[['brand', 'model_name', 'storage', 'ram_gb', 'network', 'product_id']]])\n",
        "    updated_product_id_mapping = updated_product_id_mapping.drop_duplicates(subset=['brand', 'model_name', 'storage', 'ram_gb', 'network'], keep='last')\n",
        "\n",
        "    # Read existing data from Azure Blob Storage\n",
        "    existing_fact = read_partitioned_parquet_from_blob(blob_service_client, container_name, folder_name, \"fact_table.parquet\")\n",
        "    existing_dim_device = read_parquet_from_blob(blob_service_client, container_name, folder_name, \"dim_device_specification.parquet\")\n",
        "    existing_product_mapping = read_parquet_from_blob(blob_service_client, container_name, folder_name, \"product_mapping.parquet\")\n",
        "    existing_dim_site = read_parquet_from_blob(blob_service_client, container_name, folder_name, \"dim_site.parquet\")\n",
        "    existing_dim_review = read_partitioned_parquet_from_blob(blob_service_client, container_name, folder_name, \"dim_review.parquet\")\n",
        "    existing_dim_date = read_parquet_from_blob(blob_service_client, container_name, folder_name, \"dim_date.parquet\")\n",
        "\n",
        "    # Update tables\n",
        "    updated_fact = update_fact_table(existing_fact, new_df)\n",
        "    updated_dim_device = update_dim_device_specification(existing_dim_device, new_df)\n",
        "    updated_product_mapping = update_product_mapping(existing_product_mapping, new_df)\n",
        "    updated_dim_site = update_dim_site(existing_dim_site, new_df)\n",
        "    updated_dim_review = update_dim_review(existing_dim_review, new_df)\n",
        "    updated_dim_date = update_dim_date(existing_dim_date, new_df)\n",
        "\n",
        "    # Define partition strategies\n",
        "    fact_partition_strategy = lambda row: assign_partition(row, 'product_id')\n",
        "    review_partition_strategy = lambda row: assign_partition(row, 'review_id')\n",
        "\n",
        "    # Write updated tables back to Azure Blob Storage\n",
        "    write_to_parquet(updated_fact, blob_service_client, container_name, folder_name, \"fact_table.parquet\", partition_strategy=fact_partition_strategy)\n",
        "    write_to_parquet(updated_dim_device, blob_service_client, container_name, folder_name, \"dim_device_specification.parquet\")\n",
        "    write_to_parquet(updated_product_mapping, blob_service_client, container_name, folder_name, \"product_mapping.parquet\")\n",
        "    write_to_parquet(updated_dim_site, blob_service_client, container_name, folder_name, \"dim_site.parquet\")\n",
        "    write_to_parquet(updated_dim_review, blob_service_client, container_name, folder_name, \"dim_review.parquet\", partition_strategy=review_partition_strategy)\n",
        "    write_to_parquet(updated_dim_date, blob_service_client, container_name, folder_name, \"dim_date.parquet\")\n",
        "    write_to_parquet(updated_product_id_mapping, blob_service_client, container_name, folder_name, \"product_id_mapping.parquet\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "luVmIjSTp4kF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
