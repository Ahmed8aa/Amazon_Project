{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmed8aa/az_sc_tr/blob/main/amazon_sc_tr_uk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNjF_3I6HBnI",
        "outputId": "4b18cb77-e111-4677-eb59-599d85e2c8bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.25.0 trio-0.26.2 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Collecting webdriver_manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver_manager)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (24.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2024.8.30)\n",
            "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv, webdriver_manager\n",
            "Successfully installed python-dotenv-1.0.1 webdriver_manager-4.0.2\n",
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.1+cu121)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.10.0.84)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from easyocr) (10.4.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.24.0)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.6)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2024.6.1)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.35.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->easyocr) (1.3.0)\n",
            "Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (908 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, ninja, easyocr\n",
            "Successfully installed easyocr-1.7.2 ninja-1.11.1.1 pyclipper-1.3.0.post5 python-bidi-0.6.0\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp) (0.2.0)\n",
            "Collecting azure-storage-blob\n",
            "  Downloading azure_storage_blob-12.23.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting azure-core>=1.30.0 (from azure-storage-blob)\n",
            "  Downloading azure_core-1.31.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (43.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (4.12.2)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.30.0->azure-storage-blob) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.30.0->azure-storage-blob) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-storage-blob) (2024.8.30)\n",
            "Downloading azure_storage_blob-12.23.1-py3-none-any.whl (405 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.6/405.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.31.0-py3-none-any.whl (197 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.4/197.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate, azure-core, azure-storage-blob\n",
            "Successfully installed azure-core-1.31.0 azure-storage-blob-12.23.1 isodate-0.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium\n",
        "!pip install webdriver_manager\n",
        "!pip install easyocr\n",
        "!pip install aiohttp #asyncio\n",
        "# !pip install nest_asyncio\n",
        "!pip install azure-storage-blob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "xt3HyH6GHMEq"
      },
      "outputs": [],
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "import random\n",
        "import io\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementNotInteractableException, ElementClickInterceptedException\n",
        "# from google.colab import files\n",
        "from datetime import datetime\n",
        "import nest_asyncio\n",
        "import easyocr\n",
        "import re\n",
        "\n",
        "class AmazonScraper:\n",
        "    def __init__(self, search_term, num_pages, country=\"Egypt\"):\n",
        "        self.search_term = search_term\n",
        "        self.num_pages = num_pages\n",
        "        self.country = country\n",
        "        self.driver = self.web_driver()\n",
        "        self.product_data = []\n",
        "        self.user_agents = [\n",
        "            'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0',\n",
        "          'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36',\n",
        "          'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36 Edg/93.0.961.47',\n",
        "\n",
        "          # Less common browsers\n",
        "          'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36 OPR/79.0.4143.50',\n",
        "          'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36 Vivaldi/4.1',\n",
        "\n",
        "          # Older browser versions\n",
        "          'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0',\n",
        "          'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/601.7.7 (KHTML, like Gecko) Version/9.1.2 Safari/601.7.7',\n",
        "        ]\n",
        "        self.current_user_agent_index = 0\n",
        "        self.base_url = 'https://www.amazon.co.uk'\n",
        "\n",
        "    def get_next_user_agent(self):\n",
        "        user_agent = self.user_agents[self.current_user_agent_index]\n",
        "        self.current_user_agent_index = (self.current_user_agent_index + 1) % len(self.user_agents)\n",
        "        return user_agent\n",
        "\n",
        "    def update_user_agent(self):\n",
        "        new_user_agent = self.get_next_user_agent()\n",
        "        self.driver.execute_cdp_cmd('Network.setUserAgentOverride', {\"userAgent\": new_user_agent})\n",
        "        print(f\"Updated user agent to: {new_user_agent}\")\n",
        "\n",
        "    def web_driver(self):\n",
        "        options = webdriver.ChromeOptions()\n",
        "        options.add_argument(\"--verbose\")\n",
        "        options.add_argument('--no-sandbox')\n",
        "        options.add_argument('--headless')\n",
        "        options.add_argument('--disable-gpu')\n",
        "        options.add_argument(\"--window-size=1920,1200\")\n",
        "        options.add_argument('--disable-dev-shm-usage')\n",
        "        driver = webdriver.Chrome(options=options)\n",
        "        driver.set_window_size(1920, 1080)\n",
        "        return driver\n",
        "\n",
        "    def access_amazon(self):\n",
        "        print(\"Accessing Amazon...\")\n",
        "        max_attempts = 3\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                self.driver.get('https://www.amazon.co.uk')\n",
        "                self.handle_captcha()\n",
        "                return\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
        "                if attempt < max_attempts - 1:\n",
        "                    print(\"Retrying...\")\n",
        "                    time.sleep(random.uniform(2, 5))\n",
        "                else:\n",
        "                    print(\"Max attempts reached. Unable to access Amazon.\")\n",
        "                    raise\n",
        "\n",
        "    def handle_captcha(self):\n",
        "        try:\n",
        "            captcha_input = WebDriverWait(self.driver, 5).until(\n",
        "                EC.presence_of_element_located((By.ID, 'captchacharacters'))\n",
        "            )\n",
        "            print(\"CAPTCHA detected. Taking screenshot...\")\n",
        "            self.take_captcha_screenshot()\n",
        "            time.sleep(10)\n",
        "            captcha_code = self.extract_captcha_code('/content/after_login.png')\n",
        "            if captcha_code:\n",
        "                print(f\"Extracted CAPTCHA code: {captcha_code}\")\n",
        "                captcha_input.send_keys(captcha_code)\n",
        "                continue_button = self.driver.find_element(By.CSS_SELECTOR, 'button.a-button-text')\n",
        "                continue_button.click()\n",
        "            else:\n",
        "                print(\"No CAPTCHA code detected.\")\n",
        "            time.sleep(3)\n",
        "        except TimeoutException:\n",
        "            print(\"No CAPTCHA detected.\")\n",
        "\n",
        "    def take_captcha_screenshot(self):\n",
        "        screenshot = self.driver.get_screenshot_as_png()\n",
        "        image = Image.open(io.BytesIO(screenshot))\n",
        "        image.save('/content/after_login.png')\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    def extract_captcha_code(self, image_path):\n",
        "        # Initialize EasyOCR reader\n",
        "        reader = easyocr.Reader(['en'])\n",
        "        results = reader.readtext(image_path)\n",
        "        captcha_text = ' '.join(result[1] for result in results if result[1].isalnum())\n",
        "\n",
        "        # Simple heuristic: return the first alphanumeric code of appropriate length\n",
        "        captcha_code = re.findall(r'[A-Z0-9]{6}', captcha_text)\n",
        "        return captcha_code[0] if captcha_code else None\n",
        "\n",
        "    def change_amazon_location(self):\n",
        "        print(f\"Attempting to change location to {self.country}\")\n",
        "        max_attempts = 3\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                self.driver.get('https://www.amazon.co.uk')\n",
        "                time.sleep(5)\n",
        "                location_button = WebDriverWait(self.driver, 10).until(\n",
        "                    EC.element_to_be_clickable((By.ID, \"glow-ingress-line2\"))\n",
        "                )\n",
        "                location_button.click()\n",
        "                time.sleep(2)\n",
        "                try:\n",
        "                    deliver_to = WebDriverWait(self.driver, 5).until(\n",
        "                        EC.element_to_be_clickable((By.ID, \"GLUXCountryListDropdown\"))\n",
        "                    )\n",
        "                    deliver_to.click()\n",
        "                    time.sleep(2)\n",
        "                except:\n",
        "                    print(\"'Deliver to' dropdown not found, continuing...\")\n",
        "                country_option = WebDriverWait(self.driver, 10).until(\n",
        "                    EC.element_to_be_clickable((By.XPATH, f\"//a[contains(text(), '{self.country}')]\"))\n",
        "                )\n",
        "                country_option.click()\n",
        "                time.sleep(2)\n",
        "                done_button = WebDriverWait(self.driver, 10).until(\n",
        "                    EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'a-button-text') and contains(text(), 'Done')]\"))\n",
        "                )\n",
        "                done_button.click()\n",
        "                time.sleep(5)\n",
        "                location_text = WebDriverWait(self.driver, 10).until(\n",
        "                    EC.presence_of_element_located((By.ID, \"glow-ingress-line2\"))\n",
        "                ).text\n",
        "                if self.country.lower() in location_text.lower():\n",
        "                    print(f\"Successfully changed location to {self.country}\")\n",
        "                    return\n",
        "                else:\n",
        "                    print(f\"Location change unsuccessful. Current location: {location_text}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt + 1} to change location failed.\")\n",
        "                if attempt < max_attempts - 1:\n",
        "                    print(\"Retrying...\")\n",
        "                    time.sleep(random.uniform(2, 5))\n",
        "                else:\n",
        "                    print(f\"Failed to change location to {self.country} after {max_attempts} attempts.\")\n",
        "        print(\"Location change process completed, but may not have been successful.\")\n",
        "\n",
        "    def scroll_with_pauses(self):\n",
        "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        while True:\n",
        "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "            time.sleep(random.uniform(1, 2))\n",
        "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
        "            if new_height == last_height:\n",
        "                break\n",
        "            last_height = new_height\n",
        "\n",
        "    def extract_product_links_and_prices(self):\n",
        "        print(\"Extracting product links, prices, ratings, and image URLs...\")\n",
        "        self.driver.get(f\"https://www.amazon.co.uk/s?k={self.search_term.replace(' ', '+')}\")\n",
        "\n",
        "        while \"Something went wrong\" in self.driver.page_source:\n",
        "            print(f\"Error page detected for URL. Navigating to Amazon homepage.\")\n",
        "            self.driver.get('https://www.amazon.co.uk')\n",
        "            time.sleep(2)\n",
        "            self.driver.get(f\"https://www.amazon.co.uk/?k={self.search_term.replace(' ', '+')}\")\n",
        "            time.sleep(2)\n",
        "\n",
        "        page_num = 1\n",
        "        while page_num <= self.num_pages:\n",
        "            try:\n",
        "                WebDriverWait(self.driver, 10).until(\n",
        "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"div.s-result-item\"))\n",
        "                )\n",
        "                self.scroll_with_pauses()\n",
        "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "                products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
        "\n",
        "                for product in products:\n",
        "                    try:\n",
        "                        link_tag = product.find('a', class_='a-link-normal')\n",
        "                        link = urljoin(self.base_url, link_tag['href']) if link_tag else None\n",
        "\n",
        "                        price_span = product.select_one('span.a-price-whole')\n",
        "                        if not price_span:\n",
        "                            price_span = product.select_one('div[data-cy=\"secondary-offer-recipe\"] span.a-color-base')\n",
        "                        price = price_span.get_text(strip=True) if price_span else \"No Price\"\n",
        "\n",
        "                        rating_span = product.select_one('span.a-icon-alt')\n",
        "                        rating = rating_span.get_text(strip=True) if rating_span else \"No Rating\"\n",
        "\n",
        "                        img_tag = product.find('img', class_='s-image')\n",
        "                        image_url = img_tag['src'] if img_tag else \"No Image\"\n",
        "\n",
        "                        if link:\n",
        "                            self.product_data.append({\n",
        "                                \"link\": link,\n",
        "                                \"price\": price,\n",
        "                                \"currency\":\"GBP\",\n",
        "                                \"rating\": rating,\n",
        "                                \"image_url\": image_url\n",
        "                            })\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing product: {str(e)}\")\n",
        "\n",
        "                print(f\"Extracted {len(self.product_data)} products so far...\")\n",
        "\n",
        "                next_button = self.driver.find_elements(By.CSS_SELECTOR, \"a.s-pagination-next\")\n",
        "                if next_button and page_num < self.num_pages:\n",
        "                    self.driver.execute_script(\"arguments[0].click();\", next_button[0])\n",
        "                    time.sleep(random.uniform(3, 5))\n",
        "                    page_num += 1\n",
        "                else:\n",
        "                    print(f\"Reached the last page or hit the page limit. Stopping at page {page_num}\")\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error on page {page_num}: {str(e)}\")\n",
        "                print(\"Continuing with the data collected so far...\")\n",
        "                break\n",
        "\n",
        "        print(f\"Total products extracted: {len(self.product_data)}\")\n",
        "\n",
        "    async def fetch_page(self, session, url):\n",
        "        async with session.get(url, timeout=50) as response:\n",
        "            return await response.text()\n",
        "\n",
        "    async def extract_product_details_async(self, session, url, extended_sleep=False):\n",
        "        max_attempts = 3\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                await asyncio.sleep(random.uniform(2, 5) + extended_sleep)\n",
        "                html = await self.fetch_page(session, url)\n",
        "\n",
        "                soup = BeautifulSoup(html, 'lxml')\n",
        "\n",
        "                # Extract product title\n",
        "                product_title_element = soup.find('span', {'id': 'productTitle'})\n",
        "                product_title = product_title_element.get_text(strip=True) if product_title_element else 'No title found'\n",
        "\n",
        "                # Initialize details dictionary\n",
        "                details = {\n",
        "                    \"Product Title\": product_title,\n",
        "                    \"Category\": \"Mobile Phones\",\n",
        "                    \"Site\": \"Amazon\"\n",
        "                }\n",
        "\n",
        "                # Extract product details from the table\n",
        "                table = soup.find('table', class_='a-normal a-spacing-micro')\n",
        "                if table:\n",
        "                    rows = table.find_all('tr')\n",
        "                    for row in rows:\n",
        "                        key_element = row.find('td', class_='a-span3')\n",
        "                        value_element = row.find('td', class_='a-span9')\n",
        "\n",
        "                        if key_element and value_element:\n",
        "                            key = key_element.get_text(strip=True)\n",
        "                            value = value_element.get_text(strip=True)\n",
        "                            details[key] = value\n",
        "                else:\n",
        "                    rows = soup.select('#productDetails_detailBullets_sections1 tr')\n",
        "                    for row in rows:\n",
        "                        key_element = row.find('th')\n",
        "                        value_element = row.find('td')\n",
        "\n",
        "                        if key_element and value_element:\n",
        "                            key = key_element.get_text(strip=True)\n",
        "                            value = value_element.get_text(strip=True)\n",
        "                            details[key] = value\n",
        "\n",
        "                # Now extract reviews\n",
        "                all_reviews = []\n",
        "                if \"No customer reviews\" in html:\n",
        "                    print(f\"No customer reviews found for URL {url}\")\n",
        "                    reviews_info = {\n",
        "\n",
        "                        \"All Reviews\": []\n",
        "                    }\n",
        "                else:\n",
        "                    reviews = soup.find_all('div', {'data-hook': 'review'})\n",
        "                    for review in reviews:\n",
        "                        try:\n",
        "                            reviewer_name = review.find('span', {'class': 'a-profile-name'})\n",
        "                            rating = review.find('i', {'data-hook': 'review-star-rating'})\n",
        "                            review_title = review.find('a', {'data-hook': 'review-title'})\n",
        "                            review_date = review.find('span', {'data-hook': 'review-date'})\n",
        "                            review_body = review.find('span', {'data-hook': 'review-body'})\n",
        "\n",
        "                            review_dict = {\n",
        "                                \"Reviewer Name\": reviewer_name.text.strip() if reviewer_name else \"N/A\",\n",
        "                                \"Rating\": rating.text.strip() if rating else \"N/A\",\n",
        "                                \"Title\": review_title.find_all('span')[2].text.strip() if review_title and len(review_title.find_all('span')) > 2 else \"N/A\",\n",
        "                                \"Date\": review_date.text.strip() if review_date else \"N/A\",\n",
        "                                \"Review Body\": review_body.text.strip() if review_body else \"N/A\"\n",
        "                            }\n",
        "\n",
        "                            all_reviews.append(review_dict)\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error extracting review details: {e}\")\n",
        "\n",
        "\n",
        "                    reviews_info = {\n",
        "\n",
        "                        \"All Reviews\": all_reviews\n",
        "                    }\n",
        "\n",
        "                # Combine product details and reviews\n",
        "                details.update(reviews_info)\n",
        "\n",
        "                return details\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt + 1} failed for URL {url}.\")\n",
        "                if attempt < max_attempts - 1:\n",
        "                    print(\"Retrying...\")\n",
        "                    await asyncio.sleep(random.uniform(2, 5))\n",
        "                else:\n",
        "                    print(f\"Failed to extract product details for {url} after {max_attempts} attempts.\")\n",
        "                    return {\"Product Title\": \"Error\", \"Category\": \"Error\"}\n",
        "\n",
        "\n",
        "    async def run_async_product_details(self):\n",
        "        try:\n",
        "            self.access_amazon()\n",
        "            self.extract_product_links_and_prices()\n",
        "\n",
        "            async with aiohttp.ClientSession() as session:\n",
        "                tasks = []\n",
        "                for idx, product in enumerate(self.product_data, start=1):\n",
        "                    if idx % 50 == 0:\n",
        "                        self.update_user_agent()\n",
        "                    task = asyncio.ensure_future(self.process_product_details(session, product, idx))\n",
        "                    tasks.append(task)\n",
        "\n",
        "                self.product_data = await asyncio.gather(*tasks)\n",
        "\n",
        "            df = pd.DataFrame(self.product_data)\n",
        "\n",
        "            # Get today's date\n",
        "            today_date = datetime.now().date()\n",
        "\n",
        "            # Add a column with today's date\n",
        "            df['Today'] = today_date\n",
        "\n",
        "            # print(\"Saving product details to CSV file...\")\n",
        "            # df.to_csv('amazon_product_details.csv', index=False)\n",
        "            # print(\"Product details have been saved successfully.\")\n",
        "\n",
        "        finally:\n",
        "            pass\n",
        "\n",
        "\n",
        "\n",
        "    def run_product_details(self):\n",
        "        asyncio.get_event_loop().run_until_complete(self.run_async_product_details())\n",
        "        self.driver.quit()\n",
        "\n",
        "    async def process_product_details(self, session, product, idx):\n",
        "        extended_sleep = (idx // 250) * 1.5\n",
        "\n",
        "        details = await self.extract_product_details_async(session, product[\"link\"], extended_sleep)\n",
        "        product.update(details)\n",
        "\n",
        "        return product\n",
        "\n",
        "\n",
        "\n",
        "    async def process_product_reviews(self, session, product_url, idx):\n",
        "        extended_sleep = (idx // 250) * 1.5\n",
        "\n",
        "        reviews = await self.extract_reviews_async(session, product_url, extended_sleep)\n",
        "        return reviews\n",
        "\n",
        "def main():\n",
        "    search_term = 'mobile phone'\n",
        "    num_pages =20\n",
        "    country = \"Egypt\"\n",
        "\n",
        "    nest_asyncio.apply()\n",
        "    %time\n",
        "\n",
        "\n",
        "    scraper = AmazonScraper(search_term, num_pages, country)\n",
        "    scraper.run_product_details()\n",
        "    return scraper.product_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pNk9oXoWHsTB",
        "outputId": "88a2b638-e855-4b06-c4a3-d39aa1fcdf10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 8.34 µs\n",
            "Accessing Amazon...\n",
            "No CAPTCHA detected.\n",
            "Extracting product links, prices, ratings, and image URLs...\n",
            "Extracted 16 products so far...\n",
            "Extracted 32 products so far...\n",
            "Extracted 48 products so far...\n",
            "Extracted 64 products so far...\n",
            "Extracted 80 products so far...\n",
            "Extracted 96 products so far...\n",
            "Extracted 112 products so far...\n",
            "Extracted 128 products so far...\n",
            "Extracted 144 products so far...\n",
            "Extracted 160 products so far...\n",
            "Extracted 176 products so far...\n",
            "Extracted 192 products so far...\n",
            "Extracted 208 products so far...\n",
            "Extracted 224 products so far...\n",
            "Extracted 240 products so far...\n",
            "Extracted 262 products so far...\n",
            "Extracted 278 products so far...\n",
            "Extracted 294 products so far...\n",
            "Extracted 310 products so far...\n",
            "Extracted 312 products so far...\n",
            "Reached the last page or hit the page limit. Stopping at page 20\n",
            "Total products extracted: 312\n",
            "Updated user agent to: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0\n",
            "Updated user agent to: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\n",
            "Updated user agent to: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36 Edg/93.0.961.47\n",
            "Updated user agent to: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36 OPR/79.0.4143.50\n",
            "Updated user agent to: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36 Vivaldi/4.1\n",
            "Updated user agent to: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0\n",
            "No customer reviews found for URL https://www.amazon.co.uk/Nokia-Smartphone-Cameras-Toughened-Fingerprint/dp/B0DH56VDQ6/ref=sr_1_230?dib=eyJ2IjoiMSJ9.TGjTuUcSodtybFW1d5phlV6Q3fHIaKQPLN1isc5oCYThh8bVeRr93dmhWUf_-WVk2MoCHP5IgYREgNtGfPKcO6g0L3K7aF4CAp5-xOpmodFNOMHew8BitOK8mzY2fDFHXt5162CqStRs_-r1XyaCLFAZTHCIwlsVqIwQKGpgKTqYiW9xUpBF8HlSSWJBd7ECqOx6kvjfZEiRVipHTvevgwgHwvgdn1_ebpN1Tbw8DtY.joLPFM7Xn61IfVygGUplJlnD9HhRR5Qwwo6ML30httE&dib_tag=se&keywords=mobile+phone&nsdOptOutParam=true&qid=1728954881&sr=8-230\n",
            "Attempt 1 failed for URL https://www.amazon.co.uk/sspa/click?ie=UTF8&spc=MTo4ODk5ODYwMzQ1OTY2MjY4OjE3Mjg5NTQ4ODY6c3BfbXRmOjMwMDMzNTA1MzI2NzgzMjo6MDo6&url=%2FOUKITEL-C51-Sim-Free-Unlocked-Smartphone-Black%2Fdp%2FB0D2HFGV9Z%2Fref%3Dsr_1_248_sspa%3Fdib%3DeyJ2IjoiMSJ9.bdA_TgQ1q_rPf71gCvmjL2A1owOkqTfb6PcPvEVuag7t0lIFCYlXWT48ERP7qEZkKfmNJtqtqA2hYDCASld0RrEPQTXgf30VcLt7h7g1_YaUEpY-jyokX09q2sNigsPt95JmmmuAA7b_AQaqLSWIJzvtxCMNPhJmcwNrGNn4am40wR12DxIYUesMfHG51w4DDymmZzgv7W3_SHOgQNmoQHGD2huMP24kz6IcoN0Th2U.xxB-jug3iMRLlv5ZOIBRlaGKX66BauXyyq3ssw_9nMM%26dib_tag%3Dse%26keywords%3Dmobile%2Bphone%26nsdOptOutParam%3Dtrue%26qid%3D1728954886%26sr%3D8-248-spons%26sp_csd%3Dd2lkZ2V0TmFtZT1zcF9tdGY%26psc%3D1.\n",
            "Retrying...\n",
            "Attempt 1 failed for URL https://www.amazon.co.uk/Doro-Unlocked-Smartphone-Seniors-Localisation/dp/B08RRW3TQY/ref=sr_1_252?dib=eyJ2IjoiMSJ9.bdA_TgQ1q_rPf71gCvmjL2A1owOkqTfb6PcPvEVuag7t0lIFCYlXWT48ERP7qEZkKfmNJtqtqA2hYDCASld0RrEPQTXgf30VcLt7h7g1_YaUEpY-jyokX09q2sNigsPt95JmmmuAA7b_AQaqLSWIJzvtxCMNPhJmcwNrGNn4am40wR12DxIYUesMfHG51w4DDymmZzgv7W3_SHOgQNmoQHGD2huMP24kz6IcoN0Th2U.xxB-jug3iMRLlv5ZOIBRlaGKX66BauXyyq3ssw_9nMM&dib_tag=se&keywords=mobile+phone&nsdOptOutParam=true&qid=1728954886&sr=8-252.\n",
            "Retrying...\n",
            "Attempt 1 failed for URL https://www.amazon.co.uk/SnHey-Unlocked-Android-9-0%EF%BC%8C16GB-Support-A23Ultra-Purple-Pink/dp/B0CNSMRPKD/ref=sr_1_278?dib=eyJ2IjoiMSJ9.bsHLzjA0dueWBXCPYQqgGjk66_ZADKv-6cXVvQwmzjO8o8EAePg9voJ6fBUwIghFDoAdUf3M7LxED4EOKJPWjK1j4Hjps2U_s2RoHuKJA9GMl8zwYHeyYF9ap_kzwXCQR7fikbT0WKVOQuG9AZ6RgIO2i4Wzd3r2BFalAbXO8dINQDC4thjoPIM5pr5xckMIBmoPclN8BDlfgprH0oGotds7Jk9mEEALquisA6hhymw.RyjsUcfDFhj-izK7YebHWo2PRA9NRciB0qgwVE8Yz0w&dib_tag=se&keywords=mobile+phone&nsdOptOutParam=true&qid=1728954900&sr=8-278.\n",
            "Retrying...\n",
            "Attempt 1 failed for URL https://www.amazon.co.uk/PTHTECHUS-SmartPhone-Two-way-bluetooth-headphone/dp/B0B8ZBK74F/ref=sr_1_260?dib=eyJ2IjoiMSJ9.bdA_TgQ1q_rPf71gCvmjL2A1owOkqTfb6PcPvEVuag7t0lIFCYlXWT48ERP7qEZkKfmNJtqtqA2hYDCASld0RrEPQTXgf30VcLt7h7g1_YaUEpY-jyokX09q2sNigsPt95JmmmuAA7b_AQaqLSWIJzvtxCMNPhJmcwNrGNn4am40wR12DxIYUesMfHG51w4DDymmZzgv7W3_SHOgQNmoQHGD2huMP24kz6IcoN0Th2U.xxB-jug3iMRLlv5ZOIBRlaGKX66BauXyyq3ssw_9nMM&dib_tag=se&keywords=mobile+phone&nsdOptOutParam=true&qid=1728954886&sr=8-260.\n",
            "Retrying...\n",
            "Attempt 1 failed for URL https://www.amazon.co.uk/FOSSiBOT-F105-Rugged-Smartphone-10300mAh-Green/dp/B0D76CX2G7/ref=sr_1_290?dib=eyJ2IjoiMSJ9.7Efll9jRVgQQQ7wISU09u_fSv1r_W9sSCeTIAqlo4PQrBAC9r_HW2LAmgWRUPfLd2OEi8-Id3zAsDhO1qyL9XRm1z_e8ae7nZ9dHeQq_--UOS3nx9QOzuzyKR0JWABxOKlIYGGvbIU-bEauCzPXfQERXJ5Q7wzWXJsMhdTbMgHmqxGH31SR4evMZdDXJeCypAo-WMI4x8JU3PF9l_x2jKJC9ONlhGsoNoLti24oE2nY.97ZC5mvdPE4oBH1enN-kXwlBC_AvbSA6rrvTVKf5M_k&dib_tag=se&keywords=mobile+phone&nsdOptOutParam=true&qid=1728954906&sr=8-290.\n",
            "Retrying...\n",
            "Attempt 1 failed for URL https://www.amazon.co.uk/sspa/click?ie=UTF8&spc=MTo4ODk5ODYwMzQ1OTY2MjY4OjE3Mjg5NTQ4ODY6c3BfbXRmOjMwMDM2Mzc1NzI5MDUzMjo6MDo6&url=%2FUMIDIGI-C1-MAX-Smartphone-OTG-Silver-Silver%2Fdp%2FB0D3TH8YCY%2Fref%3Dsr_1_258_sspa%3Fdib%3DeyJ2IjoiMSJ9.bdA_TgQ1q_rPf71gCvmjL2A1owOkqTfb6PcPvEVuag7t0lIFCYlXWT48ERP7qEZkKfmNJtqtqA2hYDCASld0RrEPQTXgf30VcLt7h7g1_YaUEpY-jyokX09q2sNigsPt95JmmmuAA7b_AQaqLSWIJzvtxCMNPhJmcwNrGNn4am40wR12DxIYUesMfHG51w4DDymmZzgv7W3_SHOgQNmoQHGD2huMP24kz6IcoN0Th2U.xxB-jug3iMRLlv5ZOIBRlaGKX66BauXyyq3ssw_9nMM%26dib_tag%3Dse%26keywords%3Dmobile%2Bphone%26nsdOptOutParam%3Dtrue%26qid%3D1728954886%26sr%3D8-258-spons%26sp_csd%3Dd2lkZ2V0TmFtZT1zcF9tdGY%26psc%3D1.\n",
            "Retrying...\n",
            "Attempt 1 failed for URL https://www.amazon.co.uk/sspa/click?ie=UTF8&spc=MTo4ODk5ODYwMzQ1OTY2MjY4OjE3Mjg5NTQ4ODY6c3BfYXRmX25leHQ6MzAwMzQyODc3ODA2MzMyOjowOjo&url=%2FHeyxFome-SIM-free-Unlocked-Octa-core-Smartphone-Gold%2Fdp%2FB0D3DZ8716%2Fref%3Dsr_1_242_sspa%3Fdib%3DeyJ2IjoiMSJ9.bdA_TgQ1q_rPf71gCvmjL2A1owOkqTfb6PcPvEVuag7t0lIFCYlXWT48ERP7qEZkKfmNJtqtqA2hYDCASld0RrEPQTXgf30VcLt7h7g1_YaUEpY-jyokX09q2sNigsPt95JmmmuAA7b_AQaqLSWIJzvtxCMNPhJmcwNrGNn4am40wR12DxIYUesMfHG51w4DDymmZzgv7W3_SHOgQNmoQHGD2huMP24kz6IcoN0Th2U.xxB-jug3iMRLlv5ZOIBRlaGKX66BauXyyq3ssw_9nMM%26dib_tag%3Dse%26keywords%3Dmobile%2Bphone%26nsdOptOutParam%3Dtrue%26qid%3D1728954886%26sr%3D8-242-spons%26sp_csd%3Dd2lkZ2V0TmFtZT1zcF9hdGZfbmV4dA%26psc%3D1.\n",
            "Retrying...\n",
            "Attempt 1 failed for URL https://www.amazon.co.uk/Doro-Unlocked-Seniors-Display-Assistance-Red/dp/B0C1H6PVVT/ref=sr_1_164?dib=eyJ2IjoiMSJ9.3py4yx2SASB9Ba9iGoFoSinylvYcdx69ku7ltEPNgrmCttQavdzJ-q_frF13rIOj9MTVnRVmDJNPHr74mZY7guWEQ4X4VsX-rlM859p5kOjRZZ_PSkA95HcwYMDlM_JM0vC9ZYP49Y3vhTrwyNz59yaO6Ux-YkxRF2V9YfWGmBYTm9q9HSyUiWRmvq-fcRw4eMWE_L4mFjZ1QV1U6fyADnAEoQFNAMrex1ESyQ-ZMSg.9baNw39UCSYQM40Nfy-6Hy0ShVsyAljfMJ5cYadebog&dib_tag=se&keywords=mobile+phone&nsdOptOutParam=true&qid=1728954857&sr=8-164.\n",
            "Retrying...\n",
            "Attempt 1 failed for URL https://www.amazon.co.uk/Samsung-Smartphone-6-5-inch-Infinity-V-expandable/dp/B0987CXSZZ/ref=sr_1_265?dib=eyJ2IjoiMSJ9.LOhmesK3xGO561ySXL6nUViSM6j7w-hTfAMA01WjzwHrLXr6UZ-aid1kNXM7a9KgAFIMoflHoHFZqpu8mIQKEvy8YcCLaDNL21cfuQN5eyU16LcS3UPmG-2QMiatx6UpOztmXh6NLiUccKoRRdaKnJHtJjQNB6F6n2nXE_wSXcpko8g11uJL4JpochRQ6AYQpLi8pX1hwUn0OF118ak55Ng1vzRTQ0_-F3sqNIDiHxE.MrWzfWZcBISeJ_StOn5GFstvkNfX4qLHudPqNygs7pQ&dib_tag=se&keywords=mobile+phone&nsdOptOutParam=true&qid=1728954894&sr=8-265.\n",
            "Retrying...\n",
            "Attempt 1 failed for URL https://www.amazon.co.uk/OUKITEL-WP50-Rugged-Smartphone-5G-Black/dp/B0D8W6T2NY/ref=sr_1_258?dib=eyJ2IjoiMSJ9.LOhmesK3xGO561ySXL6nUViSM6j7w-hTfAMA01WjzwHrLXr6UZ-aid1kNXM7a9KgAFIMoflHoHFZqpu8mIQKEvy8YcCLaDNL21cfuQN5eyU16LcS3UPmG-2QMiatx6UpOztmXh6NLiUccKoRRdaKnJHtJjQNB6F6n2nXE_wSXcpko8g11uJL4JpochRQ6AYQpLi8pX1hwUn0OF118ak55Ng1vzRTQ0_-F3sqNIDiHxE.MrWzfWZcBISeJ_StOn5GFstvkNfX4qLHudPqNygs7pQ&dib_tag=se&keywords=mobile+phone&nsdOptOutParam=true&qid=1728954894&sr=8-258.\n",
            "Retrying...\n",
            "Attempt 1 failed for URL https://www.amazon.co.uk/Blackview-N6000SE-Rugged-Smartphone-Phone-Orange/dp/B0DB7XWVLC/ref=sr_1_263?dib=eyJ2IjoiMSJ9.LOhmesK3xGO561ySXL6nUViSM6j7w-hTfAMA01WjzwHrLXr6UZ-aid1kNXM7a9KgAFIMoflHoHFZqpu8mIQKEvy8YcCLaDNL21cfuQN5eyU16LcS3UPmG-2QMiatx6UpOztmXh6NLiUccKoRRdaKnJHtJjQNB6F6n2nXE_wSXcpko8g11uJL4JpochRQ6AYQpLi8pX1hwUn0OF118ak55Ng1vzRTQ0_-F3sqNIDiHxE.MrWzfWZcBISeJ_StOn5GFstvkNfX4qLHudPqNygs7pQ&dib_tag=se&keywords=mobile+phone&nsdOptOutParam=true&qid=1728954894&sr=8-263.\n",
            "Retrying...\n",
            "DataFrame successfully processed.\n",
            "Final DataFrame\n"
          ]
        }
      ],
      "source": [
        "def validate_df(df):\n",
        "    # Check if df is a list or DataFrame and validate accordingly\n",
        "    if isinstance(df, list) and df:  # Check if it's a non-empty list\n",
        "        return True\n",
        "    elif hasattr(df, 'empty'):  # Check if it's a DataFrame\n",
        "        return not df.empty\n",
        "    return False\n",
        "\n",
        "# Loop until a valid df_f is returned\n",
        "df_f = None\n",
        "max_attempts = 5  # Set a limit to avoid infinite loops\n",
        "attempt = 0\n",
        "\n",
        "while attempt < max_attempts:\n",
        "    df_f = main()\n",
        "    if validate_df(df_f):\n",
        "        print(\"DataFrame successfully processed.\")\n",
        "        break\n",
        "    else:\n",
        "        print(f\"Attempt {attempt + 1}: DataFrame is not valid. Reprocessing...\")\n",
        "        attempt += 1\n",
        "\n",
        "if df_f is None or not validate_df(df_f):\n",
        "    print(\"Failed to generate a valid DataFrame after multiple attempts.\")\n",
        "else:\n",
        "    print(\"Final DataFrame\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "vWxj6QkPIbJS"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame(df_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "t3BZDoG5MguB"
      },
      "outputs": [],
      "source": [
        "# Get today's date\n",
        "today_date = datetime.now().date()\n",
        "\n",
        "# Add a column with today's date\n",
        "df['date'] = today_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "-JcXnOhr0HbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "864b8445-78e8-4d9e-c399-7b9edf303d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Container already exists: The specified container already exists.\n",
            "RequestId:fc6605b3-a01e-0003-209f-1e18fd000000\n",
            "Time:2024-10-15T01:16:40.2799684Z\n",
            "ErrorCode:ContainerAlreadyExists\n",
            "Content: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>ContainerAlreadyExists</Code><Message>The specified container already exists.\n",
            "RequestId:fc6605b3-a01e-0003-209f-1e18fd000000\n",
            "Time:2024-10-15T01:16:40.2799684Z</Message></Error>\n",
            "CSV file raw/amazon_uk_raw2024-10-15.csv uploaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "import os\n",
        "\n",
        "# Azure Storage connection string\n",
        "connect_str = \"DefaultEndpointsProtocol=https;AccountName=ynwa;AccountKey=aTLtGZuymmaAirlsL1/39g3dDjaQvBFu3lHzQaUrY0o6LDdGduAW8Wbk8qI7fuilCKS8chuiklq7+AStBN3UdA==;EndpointSuffix=core.windows.net\"\n",
        "\n",
        "# Initialize BlobServiceClient\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "\n",
        "# Name of the container and blob\n",
        "container_name = \"huayra\"  # Replace with your container name\n",
        "folder_name = \"raw\"\n",
        "\n",
        "# Name of the blob (file) with today's date\n",
        "blob_name = f\"{folder_name}/amazon_uk_raw{today_date}.csv\"  # Example: scraped_data_2024-10-07.csv\n",
        "\n",
        "# Create a container if it doesn't exist\n",
        "container_client = blob_service_client.get_container_client(container_name)\n",
        "try:\n",
        "    container_client.create_container()\n",
        "except Exception as e:\n",
        "    print(f\"Container already exists: {e}\")\n",
        "\n",
        "\n",
        "# Save the CSV locally (you can skip this if you already have the file path)\n",
        "csv_file_path = f\"scraped_data_{today_date}.csv\"\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Upload the CSV file to Blob Storage with today's date in the name\n",
        "with open(csv_file_path, \"rb\") as data:\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "    blob_client.upload_blob(data, overwrite=True)\n",
        "    print(f\"CSV file {blob_name} uploaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDK8AZuTybmM"
      },
      "source": [
        "### Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "K5bUHvdXyIiE"
      },
      "outputs": [],
      "source": [
        "df_new=df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPfmWIksjvw9",
        "outputId": "24153525-63af-4526-faad-605f0c61df13"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 312 entries, 0 to 311\n",
            "Data columns (total 42 columns):\n",
            " #   Column                       Non-Null Count  Dtype \n",
            "---  ------                       --------------  ----- \n",
            " 0   link                         312 non-null    object\n",
            " 1   price                        312 non-null    object\n",
            " 2   currency                     312 non-null    object\n",
            " 3   rating                       312 non-null    object\n",
            " 4   image_url                    312 non-null    object\n",
            " 5   Product Title                312 non-null    object\n",
            " 6   Category                     312 non-null    object\n",
            " 7   Site                         312 non-null    object\n",
            " 8   Brand                        204 non-null    object\n",
            " 9   Operating system             197 non-null    object\n",
            " 10  RAM memory installed size    167 non-null    object\n",
            " 11  CPU model                    183 non-null    object\n",
            " 12  Memory storage capacity      191 non-null    object\n",
            " 13  Screen size                  195 non-null    object\n",
            " 14  Refresh rate                 56 non-null     object\n",
            " 15  Model name                   197 non-null    object\n",
            " 16  Wireless carrier             184 non-null    object\n",
            " 17  Cellular technology          131 non-null    object\n",
            " 18  All Reviews                  312 non-null    object\n",
            " 19  CPU speed                    53 non-null     object\n",
            " 20  Resolution                   115 non-null    object\n",
            " 21  Connectivity technology      79 non-null     object\n",
            " 22  Colour                       48 non-null     object\n",
            " 23  Wireless network technology  17 non-null     object\n",
            " 24  ASIN                         1 non-null      object\n",
            " 25  Customer Reviews             1 non-null      object\n",
            " 26  Best Sellers Rank            1 non-null      object\n",
            " 27  Date First Available         1 non-null      object\n",
            " 28  SIM card slot count          4 non-null      object\n",
            " 29  Special feature              1 non-null      object\n",
            " 30  Supported application        1 non-null      object\n",
            " 31  Specific uses for product    1 non-null      object\n",
            " 32  Connector type               1 non-null      object\n",
            " 33  Form factor                  2 non-null      object\n",
            " 34  Model year                   1 non-null      object\n",
            " 35  CPU manufacturer             1 non-null      object\n",
            " 36  CPU socket                   1 non-null      object\n",
            " 37  Secondary cache              1 non-null      object\n",
            " 38  Wattage                      1 non-null      object\n",
            " 39  Processor count              1 non-null      object\n",
            " 40  Manufacturer                 1 non-null      object\n",
            " 41  date                         312 non-null    object\n",
            "dtypes: object(42)\n",
            "memory usage: 102.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "0WrGjdvJyHbm"
      },
      "outputs": [],
      "source": [
        "ordered_columns_original = [ 'link','Site','Category','rating', 'image_url',\n",
        "    'Brand', 'Model name', 'Product Title', 'price','currency',\n",
        "    'Operating system', 'RAM memory installed size',\n",
        "    'Memory storage capacity', 'Screen size', 'Resolution', 'Refresh rate',\n",
        "    'CPU speed', 'Connectivity technology', 'CPU model', 'Colour',\n",
        "    'Wireless carrier', 'Cellular technology','date',\n",
        "     'All Reviews'\n",
        "]\n",
        "# Ensure all desired columns are present in the DataFrame\n",
        "missing_columns = set(ordered_columns_original) - set(df_new.columns)\n",
        "if missing_columns:\n",
        "    print(f\"Warning: Missing columns: {missing_columns}\")\n",
        "\n",
        "df_new = df_new[ordered_columns_original]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "3JhYaOKTyHYO"
      },
      "outputs": [],
      "source": [
        "def format_column_names(column_names):\n",
        "    return [name.replace(' ', '_').lower() for name in column_names]\n",
        "\n",
        "# Preprocess column names in the DataFrame\n",
        "df_new.columns = format_column_names(df_new.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "DgZlsskKyHVc"
      },
      "outputs": [],
      "source": [
        "df_new.rename(columns={'operating_system':'os','ram_memory_installed_size':'ram',\n",
        "                         'memory_storage_capacity':'storage','today':'date'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "8-pqwPRAyHSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24892b54-1caa-47e1-9808-c465ea69e568"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(197, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "#drop nulls in model name\n",
        "df_new.dropna(subset=['model_name','price'], inplace=True)\n",
        "df_new.reset_index(drop=True, inplace=True)\n",
        "df_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "-ra1eJP5yHPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2353d64b-d290-42c8-b44f-0acbfe335c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-f0591b57d383>:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df_new = df_new.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n"
          ]
        }
      ],
      "source": [
        "# Convert all values to lowercase\n",
        "df_new = df_new.applymap(lambda x: x.lower() if isinstance(x, str) else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "6KrjWknNyHMd"
      },
      "outputs": [],
      "source": [
        "df_last=df_new.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "xARRYfMPyHJl"
      },
      "outputs": [],
      "source": [
        "# Function definitions with updated names\n",
        "def format_samsung_model(model):\n",
        "    model = model.lower()  # Convert to lowercase for consistency\n",
        "\n",
        "    # Extract special terms like FE, Ultra, Plus, 5G, 4G\n",
        "    special_terms = re.findall(r'\\b(fe|ultra|plus)\\b', model, re.IGNORECASE)\n",
        "    special_terms = list(dict.fromkeys(special_terms))  # Remove duplicates while maintaining order\n",
        "    special_terms_str = ' '.join(special_terms).upper()\n",
        "\n",
        "    # Remove unwanted words and characters\n",
        "    model = re.sub(r'\\b(samsung|galaxy)\\b', '', model).strip()\n",
        "    model = re.sub(r'\\b(5g|4g)\\b', '', model).strip()  # Remove 5G/4G here to prevent duplication\n",
        "\n",
        "    # Extract core model name\n",
        "    if 'note' in model:\n",
        "        match = re.search(r'note\\s*(\\d*)\\s*(\\w*)', model)\n",
        "        if match:\n",
        "            note_num, note_suffix = match.groups()\n",
        "            core_model = f\"Note {note_num} {note_suffix}\".strip().title()\n",
        "    else:\n",
        "        match = re.search(r'([a-z]+\\s*\\d+(?:\\s*[a-z]+)?)', model)\n",
        "        if match:\n",
        "            core_model = match.group(1).strip().title()\n",
        "        else:\n",
        "            core_model = model.strip().title()\n",
        "\n",
        "    # Remove special terms from core_model if they're already present\n",
        "    for term in special_terms:\n",
        "        core_model = re.sub(rf'\\b{term}\\b', '', core_model, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    # Combine core model and special terms\n",
        "    result = f'{core_model} {special_terms_str}'.strip()\n",
        "\n",
        "    return result.lower()\n",
        "\n",
        "def clean_motorola_model(model):\n",
        "    if not isinstance(model, str):\n",
        "        return model\n",
        "\n",
        "    # Remove \"moto\" and \"motorola\"\n",
        "    model = re.sub(r'\\bmoto\\b|\\bmotorola\\b', '', model)\n",
        "\n",
        "    # Remove '+' characters\n",
        "    model = re.sub(r'\\+', '', model)\n",
        "\n",
        "    # Remove parentheses but preserve the content\n",
        "    model = re.sub(r'\\s*\\((\\d{4})\\)', r' \\1', model)  # Preserve year in parentheses\n",
        "\n",
        "    # Remove single quotes in years (e.g., '22)\n",
        "    model = re.sub(r'\\'(\\d{2})', r'\\1', model)\n",
        "\n",
        "    # Replace multiple spaces with a single space\n",
        "    model = re.sub(r'\\s+', ' ', model)\n",
        "\n",
        "    # Strip leading and trailing whitespace\n",
        "    model = model.strip()\n",
        "\n",
        "    return model\n",
        "def extract_google_model(model):\n",
        "    # Ensure model is a string\n",
        "    if not isinstance(model, str):\n",
        "        return model  # Return the original value if it's not a string\n",
        "\n",
        "    # Convert to lowercase for consistency\n",
        "    model = model.lower()\n",
        "\n",
        "    # Use regex to extract everything after \"pixel\"\n",
        "    match = re.search(r'\\bpixel\\s+(.*)', model)\n",
        "    if match:\n",
        "        return match.group(1).strip()  # Return the portion after \"pixel\"\n",
        "\n",
        "    return model\n",
        "\n",
        "def clean_oneplus_model(model):\n",
        "    if not isinstance(model, str):\n",
        "        return model\n",
        "\n",
        "    # Remove \"oneplus\" and leading whitespace\n",
        "    model = re.sub(r'\\boneplus\\b', '', model).strip()\n",
        "\n",
        "    # Remove parentheses but keep the content inside\n",
        "    model = re.sub(r'\\s*\\(', ' ', model)\n",
        "    model = re.sub(r'\\)', '', model)\n",
        "\n",
        "    # Remove any text associated with \"gb\"\n",
        "    model = re.sub(r'\\s*\\d+gb\\b', '', model, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    # Extract text up to and including \"5g\"\n",
        "    match = re.search(r'(.*?\\b5g\\b)', model)\n",
        "    if match:\n",
        "        model = match.group(1).strip()\n",
        "\n",
        "    return model\n",
        "\n",
        "def remove_xiaomi_brand(model):\n",
        "    return model.replace('xiaomi', '').strip()\n",
        "\n",
        "def extract_iphone_model(model):\n",
        "    parts = model.split('iphone ', 1)\n",
        "    if len(parts) > 1:\n",
        "        return parts[1].split('\\t', 1)[0].strip()\n",
        "    return model.strip()\n",
        "\n",
        "# Function to apply based on brand\n",
        "def process_model(row):\n",
        "    brand = row['brand'].lower()\n",
        "    model_name = row['model_name']\n",
        "\n",
        "    if brand == 'samsung':\n",
        "        return format_samsung_model(model_name)\n",
        "    elif brand == 'motorola':\n",
        "        return clean_motorola_model(model_name)\n",
        "    elif brand == 'google':\n",
        "        return extract_google_model(model_name)\n",
        "    elif brand == 'oneplus':\n",
        "        return clean_oneplus_model(model_name)\n",
        "    elif brand == 'xiaomi':\n",
        "        return remove_xiaomi_brand(model_name)\n",
        "    elif brand == 'apple':\n",
        "        return extract_iphone_model(model_name)\n",
        "    else:\n",
        "        return model_name\n",
        "\n",
        "\n",
        "# Apply function to the DataFrame\n",
        "df_last['model_name'] = df_last.apply(process_model, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "clViBDjyyHHU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the function to extract and format OS versions\n",
        "def extract_versions(text):\n",
        "    if not isinstance(text, str):  # Ensure the input is a string\n",
        "        return ''\n",
        "\n",
        "    text = text.lower()  # Convert to lowercase for consistency\n",
        "\n",
        "    results = set()  # Use a set to avoid duplicates\n",
        "\n",
        "    # Helper function to format version numbers\n",
        "    def format_version(version):\n",
        "        if '.' not in version:\n",
        "            return f\"{version}.0\"  # Add '.0' if there is no decimal point\n",
        "        return version\n",
        "\n",
        "    # Special case for 'google_android'\n",
        "    if 'google_android' in text:\n",
        "        text = 'android' + text.split('google_android')[1]\n",
        "\n",
        "    # Extract Android version numbers\n",
        "    android_match = re.search(r'\\bandroid\\s+(\\d+(\\.\\d+)?)\\b', text)\n",
        "    if android_match:\n",
        "        version = format_version(android_match.group(1))\n",
        "        results.add(f\"android {version}\")\n",
        "\n",
        "    # Extract iOS version numbers\n",
        "    ios_match = re.search(r'\\bios\\s+(\\d+(\\.\\d+)?)\\b', text)\n",
        "    if ios_match:\n",
        "        version = format_version(ios_match.group(1))\n",
        "        results.add(f\"ios {version}\")\n",
        "\n",
        "    # Extract any other OS version numbers\n",
        "    os_match = re.search(r'\\b(\\w+os)\\s+(\\d+(\\.\\d+)?)\\b', text)\n",
        "    if os_match:\n",
        "        os_name = os_match.group(1).lower()\n",
        "        version = format_version(os_match.group(2))\n",
        "        results.add(f\"{os_name} {version}\")\n",
        "\n",
        "    # Return the results as a comma-separated string\n",
        "    return ', '.join(results) if results else text\n",
        "df_last['os'] = df_last['os'].apply(extract_versions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "bwOA2UQTyHE0"
      },
      "outputs": [],
      "source": [
        "df_last['os']=df_last.os.map(lambda x: 'android' if 'oxygenos' in x else x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Smxt9Xcrylu7"
      },
      "outputs": [],
      "source": [
        "# Define the function to extract RAM size\n",
        "def extract_ram(text):\n",
        "    if not isinstance(text, str):  # Ensure the input is a string\n",
        "        return ''\n",
        "\n",
        "    # Use regex to find the RAM size\n",
        "    match = re.search(r'(\\d+)\\s*gb', text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1)  # Return only the numeric part of the RAM size\n",
        "\n",
        "    return ''  # Return empty string if no match is found\n",
        "df_last['ram']=df_last['ram'].apply(extract_ram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "zrENiOdhylsc"
      },
      "outputs": [],
      "source": [
        "# Function to extract storage size\n",
        "def extract_storage(text):\n",
        "    if not isinstance(text, str):  # Ensure the input is a string\n",
        "        return ''\n",
        "\n",
        "    match = re.search(r'(\\d+)\\s*gb', text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1)  # Return only the numeric part of the storage size\n",
        "\n",
        "    return ''  # Return empty string if no match is found\n",
        "\n",
        "# Function to extract screen size\n",
        "def extract_screen_size(text):\n",
        "    if not isinstance(text, str):  # Ensure the input is a string\n",
        "        return ''\n",
        "\n",
        "    match = re.search(r'(\\d+(\\.\\d+)?)\\s*inches?', text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1)  # Return the screen size\n",
        "\n",
        "    return ''  # Return empty string if no match is found\n",
        "\n",
        "# Function to extract resolution\n",
        "def extract_resolution(text):\n",
        "    if not isinstance(text, str):  # Ensure the input is a string\n",
        "        return ''\n",
        "\n",
        "    # Use regex to find resolution in formats like '1920 x 1080', '1920x1080', '1280 x 720 pixels', etc.\n",
        "    match = re.search(r'(\\d{3,4})\\s*x\\s*(\\d{3,4})\\b', text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return f\"{match.group(1)} x {match.group(2)}\"  # Return resolution in 'width x height' format\n",
        "\n",
        "    # Handle cases where resolution might be written as '1920x1080' without spaces\n",
        "    match_no_space = re.search(r'(\\d{3,4})\\s*x\\s*(\\d{3,4})', text, re.IGNORECASE)\n",
        "    if match_no_space:\n",
        "        return f\"{match_no_space.group(1)} x {match_no_space.group(2)}\"  # Return resolution in 'width x height' format\n",
        "\n",
        "    return ''  # Return empty string if no match is found\n",
        "\n",
        "# Function to extract refresh rate\n",
        "def extract_refresh_rate(text):\n",
        "    if not isinstance(text, str):  # Ensure the input is a string\n",
        "        return ''\n",
        "\n",
        "    match = re.search(r'(\\d+)\\s*hz', text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1)  # Return the refresh rate\n",
        "\n",
        "    return ''  # Return empty string if no match is found\n",
        "\n",
        "# Function to extract CPU speed\n",
        "def extract_cpu_speed(text):\n",
        "    if not isinstance(text, str):  # Ensure the input is a string\n",
        "        return ''\n",
        "\n",
        "    match = re.search(r'(\\d+(\\.\\d+)?)\\s*ghz', text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1)  # Return the CPU speed\n",
        "\n",
        "    return ''  # Return empty string if no match is found\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "kEJmZpsqylp8"
      },
      "outputs": [],
      "source": [
        "df_last['storage']=df_last['storage'].apply(extract_storage)\n",
        "df_last['screen_size']=df_last['screen_size'].apply(extract_screen_size)\n",
        "df_last['resolution']=df_last['resolution'].apply(extract_resolution)\n",
        "df_last['refresh_rate']=df_last['refresh_rate'].apply(extract_refresh_rate)\n",
        "df_last['cpu_speed']=df_last['cpu_speed'].apply(extract_cpu_speed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "D7_Np1zNylnl"
      },
      "outputs": [],
      "source": [
        "df_last.rename(columns={'ram':'ram_gb','screen_size':'screen_size_in','refresh_rate':'refresh_rate_hz','cpu_speed':'cpu_speed_ghz','colour':'color'},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "GNHcjh7JyllQ"
      },
      "outputs": [],
      "source": [
        "df_last['wireless_carrier']=df_last['wireless_carrier'].map(lambda x: 'unlocked' if x=='unlocked for all carriers' or x==' unlocked' else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "uznrBIsfylir"
      },
      "outputs": [],
      "source": [
        "# Define the mapping dictionary\n",
        "carrier_mapping = {\n",
        "    'unlocked': 'unlocked',\n",
        "    'tracfone': 'tracfone',\n",
        "    'verizon': 'verizon',\n",
        "    'verizon wireless': 'verizon',\n",
        "    't-mobile': 't-mobile',\n",
        "    'simple mobile': 'simple mobile',\n",
        "    '3': '3',\n",
        "    'mvno': 'mvno',\n",
        "    't-mobile, at&t': 't-mobile',\n",
        "    'at&t': 'at&t',\n",
        "    'straight talk': 'straight talk',\n",
        "    'boost mobile': 'boost mobile',\n",
        "    'vodafone': 'vodafone',\n",
        "    't-mobile, unlocked': 't-mobile',\n",
        "    't-mobile, unlocked, verizon, sprint': 't-mobile',\n",
        "    't-mobile, unlocked, sprint': 't-mobile',\n",
        "    'sprint': 'sprint',\n",
        "    'total wireless': 'total wireless'\n",
        "}\n",
        "\n",
        "# Handle NaN values separately\n",
        "df_last['wireless_carrier'] = df_last['wireless_carrier'].fillna('')\n",
        "\n",
        "# Apply the mapping dictionary\n",
        "df_last['wireless_carrier'] = df_last['wireless_carrier'].map(carrier_mapping).fillna(df_last['wireless_carrier'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "ZpHnpdBIylgd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Replace all zeroes with NaNs\n",
        "df_last.replace(0, np.nan, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "MJahX59KyleD"
      },
      "outputs": [],
      "source": [
        "# Replace empty strings and spaces with NaNs\n",
        "df_last.replace(r'^\\s*$', np.nan, regex=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "UVc38f9sylTT"
      },
      "outputs": [],
      "source": [
        "# Convert `price_usd` to numeric\n",
        "df_last['price'] = pd.to_numeric(df_last['price'], errors='coerce')\n",
        "\n",
        "# Convert `ram_gb`, `storage`, `screen_size_in`, `refresh_rate_hz`, `cpu_speed_ghz` to numeric\n",
        "df_last['ram_gb'] = pd.to_numeric(df_last['ram_gb'], errors='coerce')\n",
        "df_last['storage'] = pd.to_numeric(df_last['storage'], errors='coerce')\n",
        "df_last['screen_size_in'] = pd.to_numeric(df_last['screen_size_in'], errors='coerce')\n",
        "df_last['refresh_rate_hz'] = pd.to_numeric(df_last['refresh_rate_hz'], errors='coerce')\n",
        "df_last['cpu_speed_ghz'] = pd.to_numeric(df_last['cpu_speed_ghz'], errors='coerce')\n",
        "df_last['site']='amazon_uk'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "JftXT-K8yHCM"
      },
      "outputs": [],
      "source": [
        "def extract_product_id(url):\n",
        "    # Regular expression pattern to capture product ID after '/dp/'\n",
        "    pattern = r'/dp/([A-Za-z0-9]+)'\n",
        "    match = re.search(pattern, url)\n",
        "\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_review_url( product_url):\n",
        "        asin =extract_product_id(product_url)\n",
        "        if asin:\n",
        "            return f\"https://www.amazon.co.jp/product-reviews/{asin}\"\n",
        "        return None\n",
        "def extract_rating(text):\n",
        "    # Regular expression to match a floating point number at the start of the string\n",
        "    pattern = r'(\\d+\\.\\d+)'\n",
        "    match = re.search(pattern, text)\n",
        "\n",
        "    if match:\n",
        "        return float(match.group(1))\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "pu451_Gaznw0"
      },
      "outputs": [],
      "source": [
        "df_last['ASIN']=df_last['link'].apply(extract_product_id)\n",
        "df_last['review_url']=df_last['link'].apply(get_review_url)\n",
        "df_last['rating']=df_last['rating'].apply(extract_rating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "xaTQyvQ5yG_u"
      },
      "outputs": [],
      "source": [
        "df=df_last.copy()\n",
        "# Function to extract and remove network type\n",
        "def extract_network(text):\n",
        "    # Check if text is a string, otherwise return it unchanged\n",
        "    if isinstance(text, str):\n",
        "        match = re.search(r'(5g|4g)', text)\n",
        "        if match:\n",
        "            network = match.group(0)\n",
        "            # Remove 5G/4G from the original text\n",
        "            text = re.sub(r'\\s?(5g|4g)\\s?', '', text).strip()\n",
        "            return text, network\n",
        "    return text, None\n",
        "\n",
        "# Initialize 'network' column as None\n",
        "df['network'] = None\n",
        "\n",
        "# Apply the extraction logic to each column and update 'network' column\n",
        "for col in ['model_name', 'product_title', 'cellular_technology']:\n",
        "    # Apply the function to extract network and updated column text\n",
        "    df[col], extracted_networks = zip(*df[col].apply(extract_network))\n",
        "\n",
        "    # Fill the 'network' column where it is None with the extracted network\n",
        "    df['network'] = df['network'].combine_first(pd.Series(extracted_networks))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "ZmJkwbaFyG88"
      },
      "outputs": [],
      "source": [
        "# Fill any remaining None values in 'network' using 'cellular_technology'\n",
        "df['network'] = df['network'].fillna(df['cellular_technology'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "HO3KL4IGyG6T"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=['cellular_technology'],inplace=True)\n",
        "df.dropna(subset=['price'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "9txlLO_HyGut"
      },
      "outputs": [],
      "source": [
        "df.date=df.date.astype('datetime64[ns]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "se7C5DAiOH3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e3879f6-6020-4e63-dd0a-d9b20062b752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Container already exists: The specified container already exists.\n",
            "RequestId:fa2d6a63-801e-002b-2f9f-1e7955000000\n",
            "Time:2024-10-15T01:16:43.1481257Z\n",
            "ErrorCode:ContainerAlreadyExists\n",
            "Content: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>ContainerAlreadyExists</Code><Message>The specified container already exists.\n",
            "RequestId:fa2d6a63-801e-002b-2f9f-1e7955000000\n",
            "Time:2024-10-15T01:16:43.1481257Z</Message></Error>\n",
            "CSV file transformed/amazon_uk_raw2024-10-15.csv uploaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "import os\n",
        "\n",
        "# Azure Storage connection string\n",
        "connect_str = \"DefaultEndpointsProtocol=https;AccountName=ynwa;AccountKey=aTLtGZuymmaAirlsL1/39g3dDjaQvBFu3lHzQaUrY0o6LDdGduAW8Wbk8qI7fuilCKS8chuiklq7+AStBN3UdA==;EndpointSuffix=core.windows.net\"\n",
        "\n",
        "# Initialize BlobServiceClient\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "\n",
        "# Name of the container and blob\n",
        "container_name = \"huayra\"  # Replace with your container name\n",
        "folder_name = \"transformed\"\n",
        "\n",
        "# Name of the blob (file) with today's date\n",
        "blob_name = f\"{folder_name}/amazon_uk_raw{today_date}.csv\"  # Example: scraped_data_2024-10-07.csv\n",
        "\n",
        "# Create a container if it doesn't exist\n",
        "container_client = blob_service_client.get_container_client(container_name)\n",
        "try:\n",
        "    container_client.create_container()\n",
        "except Exception as e:\n",
        "    print(f\"Container already exists: {e}\")\n",
        "\n",
        "\n",
        "# Save the CSV locally (you can skip this if you already have the file path)\n",
        "csv_file_path = f\"scraped_data_{today_date}.csv\"\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Upload the CSV file to Blob Storage with today's date in the name\n",
        "with open(csv_file_path, \"rb\") as data:\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "    blob_client.upload_blob(data, overwrite=True)\n",
        "    print(f\"CSV file {blob_name} uploaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NEXT HASHED CELL IS THE MAIN DATA MODEL CREATION CELL"
      ],
      "metadata": {
        "id": "s4jJ5CJ5zRfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pyarrow as pa\n",
        "# import pyarrow.parquet as pq\n",
        "# import io\n",
        "# import json\n",
        "# import hashlib\n",
        "\n",
        "# def generate_unique_product_id(row):\n",
        "#     unique_string = f\"{row['brand']}_{row['model_name']}_{row['storage']}_{row['ram_gb']}_{row['network']}\"\n",
        "#     return hashlib.md5(unique_string.encode()).hexdigest()\n",
        "\n",
        "# def assign_partition(row, id_column, num_partitions=7):\n",
        "#     return int(hashlib.md5(str(row[id_column]).encode()).hexdigest(), 16) % num_partitions\n",
        "\n",
        "# def write_to_parquet(data, blob_service_client, container_name, folder_name, file_name, partition_strategy=None):\n",
        "#     if partition_strategy is None:\n",
        "#         table = pa.Table.from_pandas(data)\n",
        "#         buf = io.BytesIO()\n",
        "#         pq.write_table(table, buf)\n",
        "#         buf.seek(0)\n",
        "\n",
        "#         blob_client = blob_service_client.get_blob_client(container=container_name, blob=f\"{folder_name}/{file_name}\")\n",
        "#         blob_client.upload_blob(buf.getvalue(), overwrite=True)\n",
        "#     else:\n",
        "#         data['partition'] = data.apply(partition_strategy, axis=1)\n",
        "#         for partition, partition_data in data.groupby('partition'):\n",
        "#             partition_data = partition_data.drop('partition', axis=1)\n",
        "#             table = pa.Table.from_pandas(partition_data)\n",
        "#             buf = io.BytesIO()\n",
        "#             pq.write_table(table, buf)\n",
        "#             buf.seek(0)\n",
        "\n",
        "#             # blob_client = blob_service_client.get_blob_client(container=container_name,\n",
        "#             #                                                   blob=f\"{folder_name}/partition={partition}/{file_name}\")\n",
        "#             blob_client = blob_service_client.get_blob_client(container=container_name,\n",
        "#                                                               blob=f\"{folder_name}/{file_name}/{partition}_{file_name}\")\n",
        "#             blob_client.upload_blob(buf.getvalue(), overwrite=True)\n",
        "\n",
        "# def process_fact_table(df):\n",
        "#     fact_table = df[['product_id', 'site', 'date', 'price', 'currency', 'rating','link']].copy()\n",
        "#     fact_table['site_id'] = pd.factorize(fact_table['site'])[0]\n",
        "#     fact_table['discount'] = 1\n",
        "#     fact_table['price_without_discount'] = fact_table['price'] / fact_table['discount']\n",
        "#     fact_table['rating_avg'] = fact_table['rating']\n",
        "#     fact_table['url']=fact_table['link']\n",
        "#     return fact_table[['product_id', 'url','site_id', 'date',  'price', 'currency', 'discount', 'price_without_discount', 'rating_avg']]\n",
        "\n",
        "# def process_dim_device_specification(df):\n",
        "#     return df[['product_id', 'product_title', 'image_url', 'os', 'screen_size_in', 'resolution', 'refresh_rate_hz', 'cpu_speed_ghz', 'cpu_model', 'color', 'wireless_carrier', 'category']].copy()\n",
        "\n",
        "# def process_product_mapping(df):\n",
        "#     return df[['product_id', 'model_name', 'brand', 'network', 'ram_gb', 'storage']].copy()\n",
        "\n",
        "# def process_dim_site(df):\n",
        "#     site_df = pd.DataFrame({'site_name': df['site'].unique()})\n",
        "#     site_df['site_id'] = range(len(site_df))\n",
        "#     return site_df\n",
        "# def process_dim_review(df):\n",
        "#     reviews_list = []\n",
        "\n",
        "#     for idx, row in df.iterrows():\n",
        "#         # Extract the list of reviews (list of dicts) for the current row\n",
        "#         if row['all_reviews']:\n",
        "#             for review in row['all_reviews']:\n",
        "#                 reviews_list.append({\n",
        "#                     'review_id': hashlib.md5(f\"{row['product_id']}_{review['Date']}_{row['site']}\".encode()).hexdigest(),  # Unique review ID\n",
        "#                     'product_id': row['product_id'],\n",
        "#                     'product_reviews_url': row['review_url'],\n",
        "#                     'review_text': review.get('Review Body', None),\n",
        "#                     'review_rating': review.get('Rating', None).split()[0] if review.get('Rating') else None,\n",
        "#                     'review_date': pd.to_datetime(review.get('Date').split('on ')[-1], format='%d %B %Y', errors='coerce'),\n",
        "#                     'site': row['site']  # Ensure site column is properly handled\n",
        "#                 })\n",
        "\n",
        "#     # Convert the list of review dictionaries into a DataFrame\n",
        "#     review_df = pd.DataFrame(reviews_list)\n",
        "\n",
        "#     # Convert the 'site' column to string explicitly to avoid ArrowInvalid\n",
        "#     review_df['site'] = review_df['site'].astype(str)\n",
        "\n",
        "#     return review_df\n",
        "\n",
        "# def process_dim_date(df):\n",
        "#     dates = pd.to_datetime(df['date'].unique())\n",
        "#     date_df = pd.DataFrame({\n",
        "#         'date_ID': dates,\n",
        "#         'day': dates.day,\n",
        "#         'month': dates.month,\n",
        "#         'year': dates.year\n",
        "#     })\n",
        "#     return date_df\n",
        "\n",
        "# def main():\n",
        "#     # Azure Storage connection string\n",
        "#     connect_str = \"DefaultEndpointsProtocol=https;AccountName=ynwa;AccountKey=aTLtGZuymmaAirlsL1/39g3dDjaQvBFu3lHzQaUrY0o6LDdGduAW8Wbk8qI7fuilCKS8chuiklq7+AStBN3UdA==;EndpointSuffix=core.windows.net\"\n",
        "\n",
        "#     # Initialize BlobServiceClient\n",
        "#     blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "\n",
        "#     # Name of the container and folder\n",
        "#     container_name = \"huayra\"\n",
        "#     folder_name = \"model\"\n",
        "\n",
        "#     # Generate unique product IDs\n",
        "#     df['product_id'] = df.apply(generate_unique_product_id, axis=1)\n",
        "\n",
        "#     # Process data and write to parquet files\n",
        "#     fact_table = process_fact_table(df)\n",
        "#     fact_partition_strategy = lambda row: assign_partition(row, 'product_id')\n",
        "#     write_to_parquet(fact_table, blob_service_client, container_name, folder_name, \"fact_table.parquet\", partition_strategy=fact_partition_strategy)\n",
        "\n",
        "#     dim_device_spec = process_dim_device_specification(df)\n",
        "#     write_to_parquet(dim_device_spec, blob_service_client, container_name, folder_name, \"dim_device_specification.parquet\")\n",
        "\n",
        "#     product_mapping = process_product_mapping(df)\n",
        "#     write_to_parquet(product_mapping, blob_service_client, container_name, folder_name, \"product_mapping.parquet\")\n",
        "\n",
        "#     dim_site = process_dim_site(df)\n",
        "#     write_to_parquet(dim_site, blob_service_client, container_name, folder_name, \"dim_site.parquet\")\n",
        "\n",
        "#     dim_review = process_dim_review(df)\n",
        "#     review_partition_strategy = lambda row: assign_partition(row, 'review_id')\n",
        "#     write_to_parquet(dim_review, blob_service_client, container_name, folder_name, \"dim_review.parquet\", partition_strategy=review_partition_strategy)\n",
        "\n",
        "#     dim_date = process_dim_date(df)\n",
        "#     write_to_parquet(dim_date, blob_service_client, container_name, folder_name, \"dim_date.parquet\")\n",
        "\n",
        "#     # Save the product ID mapping separately\n",
        "#     product_id_mapping = df[['brand', 'model_name', 'storage', 'ram_gb', 'network', 'product_id']]\n",
        "#     write_to_parquet(product_id_mapping, blob_service_client, container_name, folder_name, \"product_id_mapping.parquet\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "id": "EPo9S4kxjG4h"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import io\n",
        "import json\n",
        "import hashlib\n",
        "\n",
        "def generate_unique_product_id(row):\n",
        "    unique_string = f\"{row['brand']}_{row['model_name']}_{row['storage']}_{row['ram_gb']}_{row['network']}\"\n",
        "    return hashlib.md5(unique_string.encode()).hexdigest()\n",
        "\n",
        "def assign_partition(row, id_column, num_partitions=7):\n",
        "    return int(hashlib.md5(str(row[id_column]).encode()).hexdigest(), 16) % num_partitions\n",
        "\n",
        "def read_parquet_from_blob(blob_service_client, container_name, folder_name, file_name):\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=f\"{folder_name}/{file_name}\")\n",
        "    stream = blob_client.download_blob()\n",
        "    bytes_data = stream.readall()\n",
        "    return pd.read_parquet(io.BytesIO(bytes_data))\n",
        "\n",
        "def read_partitioned_parquet_from_blob(blob_service_client, container_name, folder_name, file_name):\n",
        "    all_data = []\n",
        "    for i in range(7):  # Assuming 7 partitions\n",
        "        try:\n",
        "            blob_client = blob_service_client.get_blob_client(container=container_name, blob=f\"{folder_name}/{file_name}/{i}_{file_name}\")\n",
        "            stream = blob_client.download_blob()\n",
        "            bytes_data = stream.readall()\n",
        "            partition_data = pd.read_parquet(io.BytesIO(bytes_data))\n",
        "            all_data.append(partition_data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading partition {i}: {e}\")\n",
        "    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
        "\n",
        "def write_to_parquet(data, blob_service_client, container_name, folder_name, file_name, partition_strategy=None):\n",
        "    if partition_strategy is None:\n",
        "        table = pa.Table.from_pandas(data)\n",
        "        buf = io.BytesIO()\n",
        "        pq.write_table(table, buf)\n",
        "        buf.seek(0)\n",
        "\n",
        "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=f\"{folder_name}/{file_name}\")\n",
        "        blob_client.upload_blob(buf.getvalue(), overwrite=True)\n",
        "    else:\n",
        "        data['partition'] = data.apply(partition_strategy, axis=1)\n",
        "        for partition, partition_data in data.groupby('partition'):\n",
        "            partition_data = partition_data.drop('partition', axis=1)\n",
        "            table = pa.Table.from_pandas(partition_data)\n",
        "            buf = io.BytesIO()\n",
        "            pq.write_table(table, buf)\n",
        "            buf.seek(0)\n",
        "\n",
        "            blob_client = blob_service_client.get_blob_client(container=container_name,\n",
        "                                                              blob=f\"{folder_name}/{file_name}/{partition}_{file_name}\")\n",
        "            blob_client.upload_blob(buf.getvalue(), overwrite=True)\n",
        "\n",
        "def update_fact_table(existing_fact, new_data):\n",
        "    new_fact = process_fact_table(new_data)\n",
        "    updated_fact = pd.concat([existing_fact, new_fact]).drop_duplicates(subset=['product_id','site_id','date','url'], keep='last')\n",
        "    return updated_fact\n",
        "\n",
        "def update_dim_device_specification(existing_dim, new_data):\n",
        "    new_dim = process_dim_device_specification(new_data)\n",
        "    updated_dim = pd.concat([existing_dim, new_dim]).drop_duplicates(subset=['product_id'], keep='last')\n",
        "    return updated_dim\n",
        "\n",
        "def update_product_mapping(existing_mapping, new_data):\n",
        "    new_mapping = process_product_mapping(new_data)\n",
        "    updated_mapping = pd.concat([existing_mapping, new_mapping]).drop_duplicates(subset=['product_id'], keep='last')\n",
        "    return updated_mapping\n",
        "\n",
        "def update_dim_site(existing_site, new_data):\n",
        "    new_site = process_dim_site(new_data)\n",
        "    updated_site = pd.concat([existing_site, new_site]).drop_duplicates(subset=['site_name'], keep='last')\n",
        "    updated_site['site_id'] = range(len(updated_site))  # Reassign site_ids\n",
        "    return updated_site\n",
        "\n",
        "def update_dim_review(existing_review, new_data):\n",
        "    new_review = process_dim_review(new_data)\n",
        "    updated_review = pd.concat([existing_review, new_review]).drop_duplicates(subset=['review_id'], keep='last')\n",
        "    return updated_review\n",
        "\n",
        "def update_dim_date(existing_date, new_data):\n",
        "    new_date = process_dim_date(new_data)\n",
        "    updated_date = pd.concat([existing_date, new_date]).drop_duplicates(subset=['date_ID'], keep='last')\n",
        "    return updated_date\n",
        "\n",
        "def process_fact_table(df):\n",
        "    fact_table = df[['product_id', 'site', 'date', 'price', 'currency', 'rating','link']].copy()\n",
        "    fact_table['site_id'] = 1\n",
        "    fact_table['discount'] = 1\n",
        "    fact_table['price_without_discount'] = fact_table['price'] / fact_table['discount']\n",
        "    fact_table['rating_avg'] = fact_table['rating']\n",
        "    fact_table['url']=fact_table['link']\n",
        "    return fact_table[['product_id', 'url','site_id', 'date',  'price', 'currency', 'discount', 'price_without_discount', 'rating_avg']]\n",
        "\n",
        "def process_dim_device_specification(df):\n",
        "    return df[['product_id', 'product_title', 'image_url', 'os', 'screen_size_in', 'resolution', 'refresh_rate_hz', 'cpu_speed_ghz', 'cpu_model', 'color', 'wireless_carrier', 'category']].copy()\n",
        "\n",
        "def process_product_mapping(df):\n",
        "    return df[['product_id', 'model_name', 'brand', 'network', 'ram_gb', 'storage']].copy()\n",
        "\n",
        "def process_dim_site(df):\n",
        "    site_df = pd.DataFrame({'site_name': df['site'].unique()})\n",
        "    site_df['site_id'] = range(len(site_df))\n",
        "    return site_df\n",
        "\n",
        "def process_dim_review(df):\n",
        "    reviews_list = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Extract the list of reviews (list of dicts) for the current row\n",
        "        if row['all_reviews']:\n",
        "            for review in row['all_reviews']:\n",
        "                reviews_list.append({\n",
        "                    'review_id': hashlib.md5(f\"{row['product_id']}_{review['Date']}_{row['site']}\".encode()).hexdigest(),  # Unique review ID\n",
        "                    'product_id': row['product_id'],\n",
        "                    'product_reviews_url': row['review_url'],\n",
        "                    'review_text': review.get('Review Body', None),\n",
        "                    'review_rating': review.get('Rating', None).split()[0] if review.get('Rating') else None,\n",
        "                    'review_date': pd.to_datetime(review.get('Date').split('on ')[-1], format='%d %B %Y', errors='coerce'),\n",
        "                    'site': row['site']  # Ensure site column is properly handled\n",
        "                })\n",
        "\n",
        "    # Convert the list of review dictionaries into a DataFrame\n",
        "    review_df = pd.DataFrame(reviews_list)\n",
        "\n",
        "    # Convert the 'site' column to string explicitly to avoid ArrowInvalid\n",
        "    review_df['site'] = review_df['site'].astype(str)\n",
        "\n",
        "    return review_df\n",
        "\n",
        "def process_dim_date(df):\n",
        "    dates = pd.to_datetime(df['date'].unique())\n",
        "    date_df = pd.DataFrame({\n",
        "        'date_ID': dates,\n",
        "        'day': dates.day,\n",
        "        'month': dates.month,\n",
        "        'year': dates.year\n",
        "    })\n",
        "    return date_df\n",
        "\n",
        "def main():\n",
        "    # Azure Storage connection string\n",
        "    connect_str = \"DefaultEndpointsProtocol=https;AccountName=ynwa;AccountKey=aTLtGZuymmaAirlsL1/39g3dDjaQvBFu3lHzQaUrY0o6LDdGduAW8Wbk8qI7fuilCKS8chuiklq7+AStBN3UdA==;EndpointSuffix=core.windows.net\"\n",
        "\n",
        "    # Initialize BlobServiceClient\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "\n",
        "    # Name of the container and folder\n",
        "    container_name = \"huayra\"\n",
        "    folder_name = \"model\"\n",
        "\n",
        "    # Read new data (replace this with your actual data loading method)\n",
        "    new_df = df.copy()\n",
        "\n",
        "    # Read existing product ID mapping\n",
        "    existing_product_id_mapping = read_parquet_from_blob(blob_service_client, container_name, folder_name, \"product_id_mapping.parquet\")\n",
        "\n",
        "    # Generate unique product IDs for new data, using existing IDs where possible\n",
        "    new_df['temp_id'] = new_df.apply(generate_unique_product_id, axis=1)\n",
        "    new_df = pd.merge(new_df, existing_product_id_mapping[['brand', 'model_name', 'storage', 'ram_gb', 'network', 'product_id']],\n",
        "                      on=['brand', 'model_name', 'storage', 'ram_gb', 'network'], how='left')\n",
        "    new_df['product_id'] = new_df['product_id'].fillna(new_df['temp_id'])\n",
        "    new_df = new_df.drop('temp_id', axis=1)\n",
        "\n",
        "    # Update product ID mapping\n",
        "    updated_product_id_mapping = pd.concat([existing_product_id_mapping, new_df[['brand', 'model_name', 'storage', 'ram_gb', 'network', 'product_id']]])\n",
        "    updated_product_id_mapping = updated_product_id_mapping.drop_duplicates(subset=['brand', 'model_name', 'storage', 'ram_gb', 'network'], keep='last')\n",
        "\n",
        "    # Read existing data from Azure Blob Storage\n",
        "    existing_fact = read_partitioned_parquet_from_blob(blob_service_client, container_name, folder_name, \"fact_table.parquet\")\n",
        "    existing_dim_device = read_parquet_from_blob(blob_service_client, container_name, folder_name, \"dim_device_specification.parquet\")\n",
        "    existing_product_mapping = read_parquet_from_blob(blob_service_client, container_name, folder_name, \"product_mapping.parquet\")\n",
        "    existing_dim_site = read_parquet_from_blob(blob_service_client, container_name, folder_name, \"dim_site.parquet\")\n",
        "    existing_dim_review = read_partitioned_parquet_from_blob(blob_service_client, container_name, folder_name, \"dim_review.parquet\")\n",
        "    existing_dim_date = read_parquet_from_blob(blob_service_client, container_name, folder_name, \"dim_date.parquet\")\n",
        "\n",
        "    # Update tables\n",
        "    updated_fact = update_fact_table(existing_fact, new_df)\n",
        "    updated_dim_device = update_dim_device_specification(existing_dim_device, new_df)\n",
        "    updated_product_mapping = update_product_mapping(existing_product_mapping, new_df)\n",
        "    updated_dim_site = update_dim_site(existing_dim_site, new_df)\n",
        "    updated_dim_review = update_dim_review(existing_dim_review, new_df)\n",
        "    updated_dim_date = update_dim_date(existing_dim_date, new_df)\n",
        "\n",
        "    # Define partition strategies\n",
        "    fact_partition_strategy = lambda row: assign_partition(row, 'product_id')\n",
        "    review_partition_strategy = lambda row: assign_partition(row, 'review_id')\n",
        "\n",
        "    # Write updated tables back to Azure Blob Storage\n",
        "    write_to_parquet(updated_fact, blob_service_client, container_name, folder_name, \"fact_table.parquet\", partition_strategy=fact_partition_strategy)\n",
        "    write_to_parquet(updated_dim_device, blob_service_client, container_name, folder_name, \"dim_device_specification.parquet\")\n",
        "    write_to_parquet(updated_product_mapping, blob_service_client, container_name, folder_name, \"product_mapping.parquet\")\n",
        "    write_to_parquet(updated_dim_site, blob_service_client, container_name, folder_name, \"dim_site.parquet\")\n",
        "    write_to_parquet(updated_dim_review, blob_service_client, container_name, folder_name, \"dim_review.parquet\", partition_strategy=review_partition_strategy)\n",
        "    write_to_parquet(updated_dim_date, blob_service_client, container_name, folder_name, \"dim_date.parquet\")\n",
        "    write_to_parquet(updated_product_id_mapping, blob_service_client, container_name, folder_name, \"product_id_mapping.parquet\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7NC1g7xouYJ",
        "outputId": "034ed971-554a-46b6-e655-5f1941d25b2c"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(163, 27)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Done For UK')"
      ],
      "metadata": {
        "id": "0M-px56WxKtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3q6j14BRz9QE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONO7cXEUbhj30iFxqzulqz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}